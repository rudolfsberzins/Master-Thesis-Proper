{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from parse_and_prepare import ProteinProteinInteractionClassifier as ppi\n",
    "import file_readers as fr\n",
    "import prediction as pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dros_strict_real = pickle.load(open('Results/drosophila/strict_real.pkl', 'rb'))\n",
    "dros_gen_real = pickle.load(open('Results/drosophila/gen_real.pkl', 'rb'))\n",
    "dros_be_real = pickle.load(open('Results/drosophila/be_real.pkl', 'rb'))\n",
    "random_seeds = [144, 235, 905, 2895, 3462, 4225, 5056, 5192, 7751, 7813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    real_tr_te_name = 'dros_real_tr_te_split_' + str(seed)\n",
    "    train_data, b, c, d = pred.manual_train_test_split(dros_strict_real, real_tr_te_name, random_state=seed ,test_set_prop=0.1)\n",
    "    tr_val_name = 'dros_tr_val_split_' + str(seed)\n",
    "    real_train_data, b, c, d = pred.manual_train_test_split(train_data, tr_val_name, random_state=seed, test_set_prop=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:34:39,562 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:34:39,563 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:34:39,599 : INFO : collected 13177 word types from a corpus of 185681 raw words and 7271 sentences\n",
      "2017-05-06 21:34:39,600 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:34:39,614 : INFO : min_count=5 retains 3537 unique words (26% of original 13177, drops 9640)\n",
      "2017-05-06 21:34:39,615 : INFO : min_count=5 leaves 170112 word corpus (91% of original 185681, drops 15569)\n",
      "2017-05-06 21:34:39,624 : INFO : deleting the raw counts dictionary of 13177 items\n",
      "2017-05-06 21:34:39,626 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-06 21:34:39,626 : INFO : downsampling leaves estimated 123095 word corpus (72.4% of prior 170112)\n",
      "2017-05-06 21:34:39,627 : INFO : estimated required memory for 3537 words and 100 dimensions: 4598100 bytes\n",
      "2017-05-06 21:34:39,638 : INFO : resetting layer weights\n",
      "2017-05-06 21:34:39,674 : INFO : training model with 4 workers on 3537 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:34:39,675 : INFO : expecting 7271 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:34:40,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:34:40,098 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:34:40,103 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:34:40,106 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:34:40,107 : INFO : training on 928405 raw words (615514 effective words) took 0.4s, 1441981 effective words/s\n",
      "2017-05-06 21:34:40,107 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:34:40,131 : INFO : saving Word2Vec object under Results/dros_strict_dim_parameter_100_model, separately None\n",
      "2017-05-06 21:34:40,132 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:34:40,132 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:34:40,166 : INFO : saved Results/dros_strict_dim_parameter_100_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:34:40,549 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:34:40,550 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:34:40,592 : INFO : PROGRESS: at sentence #10000, processed 244981 words, keeping 13594 word types\n",
      "2017-05-06 21:34:40,635 : INFO : PROGRESS: at sentence #20000, processed 488356 words, keeping 18196 word types\n",
      "2017-05-06 21:34:40,677 : INFO : PROGRESS: at sentence #30000, processed 731064 words, keeping 21332 word types\n",
      "2017-05-06 21:34:40,709 : INFO : collected 23335 word types from a corpus of 901524 raw words and 37017 sentences\n",
      "2017-05-06 21:34:40,710 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:34:40,737 : INFO : min_count=5 retains 8246 unique words (35% of original 23335, drops 15089)\n",
      "2017-05-06 21:34:40,738 : INFO : min_count=5 leaves 876113 word corpus (97% of original 901524, drops 25411)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:34:40,757 : INFO : deleting the raw counts dictionary of 23335 items\n",
      "2017-05-06 21:34:40,759 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-06 21:34:40,760 : INFO : downsampling leaves estimated 646591 word corpus (73.8% of prior 876113)\n",
      "2017-05-06 21:34:40,760 : INFO : estimated required memory for 8246 words and 100 dimensions: 10719800 bytes\n",
      "2017-05-06 21:34:40,788 : INFO : resetting layer weights\n",
      "2017-05-06 21:34:40,867 : INFO : training model with 4 workers on 8246 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:34:40,868 : INFO : expecting 37017 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:34:41,877 : INFO : PROGRESS: at 43.41% examples, 1397916 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:42,880 : INFO : PROGRESS: at 87.50% examples, 1409421 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:34:43,142 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:34:43,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:34:43,146 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:34:43,148 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:34:43,149 : INFO : training on 4507620 raw words (3232841 effective words) took 2.3s, 1420408 effective words/s\n",
      "2017-05-06 21:34:43,150 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:34:43,190 : INFO : saving Word2Vec object under Results/dros_gen_dim_parameter_100_model, separately None\n",
      "2017-05-06 21:34:43,191 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:34:43,192 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:34:43,271 : INFO : saved Results/dros_gen_dim_parameter_100_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:34:45,447 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:34:45,448 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:34:45,494 : INFO : PROGRESS: at sentence #10000, processed 231388 words, keeping 14641 word types\n",
      "2017-05-06 21:34:45,542 : INFO : PROGRESS: at sentence #20000, processed 463186 words, keeping 20111 word types\n",
      "2017-05-06 21:34:45,594 : INFO : PROGRESS: at sentence #30000, processed 694962 words, keeping 23963 word types\n",
      "2017-05-06 21:34:45,642 : INFO : PROGRESS: at sentence #40000, processed 925654 words, keeping 27029 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:34:45,692 : INFO : PROGRESS: at sentence #50000, processed 1157257 words, keeping 29704 word types\n",
      "2017-05-06 21:34:45,745 : INFO : PROGRESS: at sentence #60000, processed 1387247 words, keeping 32225 word types\n",
      "2017-05-06 21:34:45,793 : INFO : PROGRESS: at sentence #70000, processed 1618178 words, keeping 34360 word types\n",
      "2017-05-06 21:34:45,839 : INFO : PROGRESS: at sentence #80000, processed 1848004 words, keeping 36287 word types\n",
      "2017-05-06 21:34:45,883 : INFO : PROGRESS: at sentence #90000, processed 2077948 words, keeping 38042 word types\n",
      "2017-05-06 21:34:45,928 : INFO : PROGRESS: at sentence #100000, processed 2309347 words, keeping 39767 word types\n",
      "2017-05-06 21:34:45,976 : INFO : PROGRESS: at sentence #110000, processed 2541111 words, keeping 41317 word types\n",
      "2017-05-06 21:34:46,020 : INFO : PROGRESS: at sentence #120000, processed 2772131 words, keeping 42710 word types\n",
      "2017-05-06 21:34:46,066 : INFO : PROGRESS: at sentence #130000, processed 3002172 words, keeping 44010 word types\n",
      "2017-05-06 21:34:46,111 : INFO : PROGRESS: at sentence #140000, processed 3232261 words, keeping 45286 word types\n",
      "2017-05-06 21:34:46,159 : INFO : PROGRESS: at sentence #150000, processed 3461626 words, keeping 46520 word types\n",
      "2017-05-06 21:34:46,208 : INFO : PROGRESS: at sentence #160000, processed 3691038 words, keeping 47732 word types\n",
      "2017-05-06 21:34:46,261 : INFO : PROGRESS: at sentence #170000, processed 3921777 words, keeping 48878 word types\n",
      "2017-05-06 21:34:46,312 : INFO : collected 49890 word types from a corpus of 4141788 raw words and 179573 sentences\n",
      "2017-05-06 21:34:46,313 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:34:46,366 : INFO : min_count=5 retains 18427 unique words (36% of original 49890, drops 31463)\n",
      "2017-05-06 21:34:46,367 : INFO : min_count=5 leaves 4088708 word corpus (98% of original 4141788, drops 53080)\n",
      "2017-05-06 21:34:46,416 : INFO : deleting the raw counts dictionary of 49890 items\n",
      "2017-05-06 21:34:46,418 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-06 21:34:46,419 : INFO : downsampling leaves estimated 3077219 word corpus (75.3% of prior 4088708)\n",
      "2017-05-06 21:34:46,420 : INFO : estimated required memory for 18427 words and 100 dimensions: 23955100 bytes\n",
      "2017-05-06 21:34:46,485 : INFO : resetting layer weights\n",
      "2017-05-06 21:34:46,669 : INFO : training model with 4 workers on 18427 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:34:46,669 : INFO : expecting 179573 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:34:47,680 : INFO : PROGRESS: at 8.81% examples, 1353879 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:48,688 : INFO : PROGRESS: at 17.65% examples, 1350079 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:49,690 : INFO : PROGRESS: at 26.60% examples, 1359472 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:34:50,693 : INFO : PROGRESS: at 35.24% examples, 1350528 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:51,694 : INFO : PROGRESS: at 44.21% examples, 1355965 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:52,700 : INFO : PROGRESS: at 54.04% examples, 1381057 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:53,700 : INFO : PROGRESS: at 63.21% examples, 1385059 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:54,706 : INFO : PROGRESS: at 72.18% examples, 1383484 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:34:55,706 : INFO : PROGRESS: at 78.07% examples, 1330453 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:56,707 : INFO : PROGRESS: at 84.77% examples, 1300580 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:57,711 : INFO : PROGRESS: at 90.55% examples, 1262957 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:34:58,717 : INFO : PROGRESS: at 96.83% examples, 1237508 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:34:59,097 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:34:59,102 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:34:59,102 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:34:59,106 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:34:59,106 : INFO : training on 20708940 raw words (15385853 effective words) took 12.4s, 1237856 effective words/s\n",
      "2017-05-06 21:34:59,107 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:34:59,193 : INFO : saving Word2Vec object under Results/dros_be_dim_parameter_100_model, separately None\n",
      "2017-05-06 21:34:59,194 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:34:59,194 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:34:59,345 : INFO : saved Results/dros_be_dim_parameter_100_model\n",
      "2017-05-06 21:34:59,508 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:34:59,508 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:34:59,542 : INFO : collected 13177 word types from a corpus of 185681 raw words and 7271 sentences\n",
      "2017-05-06 21:34:59,543 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:34:59,555 : INFO : min_count=5 retains 3537 unique words (26% of original 13177, drops 9640)\n",
      "2017-05-06 21:34:59,556 : INFO : min_count=5 leaves 170112 word corpus (91% of original 185681, drops 15569)\n",
      "2017-05-06 21:34:59,564 : INFO : deleting the raw counts dictionary of 13177 items\n",
      "2017-05-06 21:34:59,565 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-06 21:34:59,566 : INFO : downsampling leaves estimated 123095 word corpus (72.4% of prior 170112)\n",
      "2017-05-06 21:34:59,566 : INFO : estimated required memory for 3537 words and 200 dimensions: 7427700 bytes\n",
      "2017-05-06 21:34:59,576 : INFO : resetting layer weights\n",
      "2017-05-06 21:34:59,615 : INFO : training model with 4 workers on 3537 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:34:59,616 : INFO : expecting 7271 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:00,150 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:35:00,156 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:35:00,157 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:35:00,163 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:35:00,164 : INFO : training on 928405 raw words (615127 effective words) took 0.5s, 1133565 effective words/s\n",
      "2017-05-06 21:35:00,165 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:35:00,184 : INFO : saving Word2Vec object under Results/dros_strict_dim_parameter_200_model, separately None\n",
      "2017-05-06 21:35:00,184 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:35:00,185 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:35:00,233 : INFO : saved Results/dros_strict_dim_parameter_200_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:00,618 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:35:00,619 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:35:00,662 : INFO : PROGRESS: at sentence #10000, processed 244981 words, keeping 13594 word types\n",
      "2017-05-06 21:35:00,702 : INFO : PROGRESS: at sentence #20000, processed 488356 words, keeping 18196 word types\n",
      "2017-05-06 21:35:00,746 : INFO : PROGRESS: at sentence #30000, processed 731064 words, keeping 21332 word types\n",
      "2017-05-06 21:35:00,777 : INFO : collected 23335 word types from a corpus of 901524 raw words and 37017 sentences\n",
      "2017-05-06 21:35:00,778 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:35:00,802 : INFO : min_count=5 retains 8246 unique words (35% of original 23335, drops 15089)\n",
      "2017-05-06 21:35:00,802 : INFO : min_count=5 leaves 876113 word corpus (97% of original 901524, drops 25411)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:00,823 : INFO : deleting the raw counts dictionary of 23335 items\n",
      "2017-05-06 21:35:00,825 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-06 21:35:00,826 : INFO : downsampling leaves estimated 646591 word corpus (73.8% of prior 876113)\n",
      "2017-05-06 21:35:00,827 : INFO : estimated required memory for 8246 words and 200 dimensions: 17316600 bytes\n",
      "2017-05-06 21:35:00,861 : INFO : resetting layer weights\n",
      "2017-05-06 21:35:00,957 : INFO : training model with 4 workers on 8246 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:35:00,958 : INFO : expecting 37017 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:35:01,966 : INFO : PROGRESS: at 31.89% examples, 1028777 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:35:02,971 : INFO : PROGRESS: at 62.24% examples, 1002658 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:03,977 : INFO : PROGRESS: at 93.92% examples, 1007550 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:04,157 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:35:04,159 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:35:04,164 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:35:04,165 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:35:04,167 : INFO : training on 4507620 raw words (3232968 effective words) took 3.2s, 1009260 effective words/s\n",
      "2017-05-06 21:35:04,168 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:35:04,212 : INFO : saving Word2Vec object under Results/dros_gen_dim_parameter_200_model, separately None\n",
      "2017-05-06 21:35:04,213 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:35:04,213 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:35:04,386 : INFO : saved Results/dros_gen_dim_parameter_200_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:06,646 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:35:06,647 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:35:06,696 : INFO : PROGRESS: at sentence #10000, processed 231388 words, keeping 14641 word types\n",
      "2017-05-06 21:35:06,739 : INFO : PROGRESS: at sentence #20000, processed 463186 words, keeping 20111 word types\n",
      "2017-05-06 21:35:06,787 : INFO : PROGRESS: at sentence #30000, processed 694962 words, keeping 23963 word types\n",
      "2017-05-06 21:35:06,834 : INFO : PROGRESS: at sentence #40000, processed 925654 words, keeping 27029 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:06,878 : INFO : PROGRESS: at sentence #50000, processed 1157257 words, keeping 29704 word types\n",
      "2017-05-06 21:35:06,933 : INFO : PROGRESS: at sentence #60000, processed 1387247 words, keeping 32225 word types\n",
      "2017-05-06 21:35:06,991 : INFO : PROGRESS: at sentence #70000, processed 1618178 words, keeping 34360 word types\n",
      "2017-05-06 21:35:07,050 : INFO : PROGRESS: at sentence #80000, processed 1848004 words, keeping 36287 word types\n",
      "2017-05-06 21:35:07,106 : INFO : PROGRESS: at sentence #90000, processed 2077948 words, keeping 38042 word types\n",
      "2017-05-06 21:35:07,154 : INFO : PROGRESS: at sentence #100000, processed 2309347 words, keeping 39767 word types\n",
      "2017-05-06 21:35:07,201 : INFO : PROGRESS: at sentence #110000, processed 2541111 words, keeping 41317 word types\n",
      "2017-05-06 21:35:07,246 : INFO : PROGRESS: at sentence #120000, processed 2772131 words, keeping 42710 word types\n",
      "2017-05-06 21:35:07,296 : INFO : PROGRESS: at sentence #130000, processed 3002172 words, keeping 44010 word types\n",
      "2017-05-06 21:35:07,342 : INFO : PROGRESS: at sentence #140000, processed 3232261 words, keeping 45286 word types\n",
      "2017-05-06 21:35:07,392 : INFO : PROGRESS: at sentence #150000, processed 3461626 words, keeping 46520 word types\n",
      "2017-05-06 21:35:07,440 : INFO : PROGRESS: at sentence #160000, processed 3691038 words, keeping 47732 word types\n",
      "2017-05-06 21:35:07,488 : INFO : PROGRESS: at sentence #170000, processed 3921777 words, keeping 48878 word types\n",
      "2017-05-06 21:35:07,537 : INFO : collected 49890 word types from a corpus of 4141788 raw words and 179573 sentences\n",
      "2017-05-06 21:35:07,537 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:35:07,590 : INFO : min_count=5 retains 18427 unique words (36% of original 49890, drops 31463)\n",
      "2017-05-06 21:35:07,591 : INFO : min_count=5 leaves 4088708 word corpus (98% of original 4141788, drops 53080)\n",
      "2017-05-06 21:35:07,639 : INFO : deleting the raw counts dictionary of 49890 items\n",
      "2017-05-06 21:35:07,641 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-06 21:35:07,642 : INFO : downsampling leaves estimated 3077219 word corpus (75.3% of prior 4088708)\n",
      "2017-05-06 21:35:07,642 : INFO : estimated required memory for 18427 words and 200 dimensions: 38696700 bytes\n",
      "2017-05-06 21:35:07,710 : INFO : resetting layer weights\n",
      "2017-05-06 21:35:07,919 : INFO : training model with 4 workers on 18427 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:35:07,919 : INFO : expecting 179573 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:35:08,930 : INFO : PROGRESS: at 6.16% examples, 945133 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:09,934 : INFO : PROGRESS: at 12.37% examples, 949224 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:10,943 : INFO : PROGRESS: at 18.66% examples, 951359 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:11,952 : INFO : PROGRESS: at 24.82% examples, 948697 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:35:12,953 : INFO : PROGRESS: at 31.14% examples, 953323 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:13,955 : INFO : PROGRESS: at 37.57% examples, 958392 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:35:14,961 : INFO : PROGRESS: at 43.15% examples, 943691 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:15,967 : INFO : PROGRESS: at 48.02% examples, 918842 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:16,994 : INFO : PROGRESS: at 52.69% examples, 894151 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:17,994 : INFO : PROGRESS: at 57.44% examples, 877523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:18,995 : INFO : PROGRESS: at 62.40% examples, 867299 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:20,001 : INFO : PROGRESS: at 68.18% examples, 868792 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:21,012 : INFO : PROGRESS: at 74.54% examples, 876514 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:22,022 : INFO : PROGRESS: at 80.39% examples, 877387 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:23,032 : INFO : PROGRESS: at 85.30% examples, 868856 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:24,039 : INFO : PROGRESS: at 89.59% examples, 855547 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:25,048 : INFO : PROGRESS: at 93.97% examples, 844551 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:26,071 : INFO : PROGRESS: at 98.62% examples, 836186 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:35:26,340 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:35:26,351 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:35:26,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:35:26,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:35:26,358 : INFO : training on 20708940 raw words (15385532 effective words) took 18.4s, 834623 effective words/s\n",
      "2017-05-06 21:35:26,359 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:35:26,524 : INFO : saving Word2Vec object under Results/dros_be_dim_parameter_200_model, separately None\n",
      "2017-05-06 21:35:26,525 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:35:26,526 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:35:26,876 : INFO : saved Results/dros_be_dim_parameter_200_model\n",
      "2017-05-06 21:35:27,087 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:35:27,088 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:35:27,127 : INFO : collected 13177 word types from a corpus of 185681 raw words and 7271 sentences\n",
      "2017-05-06 21:35:27,128 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:35:27,146 : INFO : min_count=5 retains 3537 unique words (26% of original 13177, drops 9640)\n",
      "2017-05-06 21:35:27,147 : INFO : min_count=5 leaves 170112 word corpus (91% of original 185681, drops 15569)\n",
      "2017-05-06 21:35:27,156 : INFO : deleting the raw counts dictionary of 13177 items\n",
      "2017-05-06 21:35:27,157 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-06 21:35:27,158 : INFO : downsampling leaves estimated 123095 word corpus (72.4% of prior 170112)\n",
      "2017-05-06 21:35:27,158 : INFO : estimated required memory for 3537 words and 300 dimensions: 10257300 bytes\n",
      "2017-05-06 21:35:27,171 : INFO : resetting layer weights\n",
      "2017-05-06 21:35:27,216 : INFO : training model with 4 workers on 3537 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:35:27,217 : INFO : expecting 7271 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:27,860 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:35:27,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:35:27,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:35:27,877 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:35:27,878 : INFO : training on 928405 raw words (615725 effective words) took 0.7s, 939518 effective words/s\n",
      "2017-05-06 21:35:27,878 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:35:27,897 : INFO : saving Word2Vec object under Results/dros_strict_dim_parameter_300_model, separately None\n",
      "2017-05-06 21:35:27,899 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:35:27,900 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:35:27,972 : INFO : saved Results/dros_strict_dim_parameter_300_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:28,387 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:35:28,387 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:35:28,441 : INFO : PROGRESS: at sentence #10000, processed 244981 words, keeping 13594 word types\n",
      "2017-05-06 21:35:28,483 : INFO : PROGRESS: at sentence #20000, processed 488356 words, keeping 18196 word types\n",
      "2017-05-06 21:35:28,532 : INFO : PROGRESS: at sentence #30000, processed 731064 words, keeping 21332 word types\n",
      "2017-05-06 21:35:28,574 : INFO : collected 23335 word types from a corpus of 901524 raw words and 37017 sentences\n",
      "2017-05-06 21:35:28,574 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:28,601 : INFO : min_count=5 retains 8246 unique words (35% of original 23335, drops 15089)\n",
      "2017-05-06 21:35:28,601 : INFO : min_count=5 leaves 876113 word corpus (97% of original 901524, drops 25411)\n",
      "2017-05-06 21:35:28,621 : INFO : deleting the raw counts dictionary of 23335 items\n",
      "2017-05-06 21:35:28,623 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-06 21:35:28,624 : INFO : downsampling leaves estimated 646591 word corpus (73.8% of prior 876113)\n",
      "2017-05-06 21:35:28,624 : INFO : estimated required memory for 8246 words and 300 dimensions: 23913400 bytes\n",
      "2017-05-06 21:35:28,650 : INFO : resetting layer weights\n",
      "2017-05-06 21:35:28,752 : INFO : training model with 4 workers on 8246 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:35:28,753 : INFO : expecting 37017 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:35:29,764 : INFO : PROGRESS: at 26.13% examples, 840606 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:30,771 : INFO : PROGRESS: at 52.49% examples, 843126 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:35:31,775 : INFO : PROGRESS: at 78.87% examples, 844967 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:32,590 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:35:32,591 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:35:32,592 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:35:32,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:35:32,604 : INFO : training on 4507620 raw words (3232459 effective words) took 3.8s, 840529 effective words/s\n",
      "2017-05-06 21:35:32,604 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:35:32,651 : INFO : saving Word2Vec object under Results/dros_gen_dim_parameter_300_model, separately None\n",
      "2017-05-06 21:35:32,652 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:35:32,652 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:35:32,832 : INFO : saved Results/dros_gen_dim_parameter_300_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:35,017 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:35:35,018 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:35:35,068 : INFO : PROGRESS: at sentence #10000, processed 231388 words, keeping 14641 word types\n",
      "2017-05-06 21:35:35,114 : INFO : PROGRESS: at sentence #20000, processed 463186 words, keeping 20111 word types\n",
      "2017-05-06 21:35:35,162 : INFO : PROGRESS: at sentence #30000, processed 694962 words, keeping 23963 word types\n",
      "2017-05-06 21:35:35,208 : INFO : PROGRESS: at sentence #40000, processed 925654 words, keeping 27029 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:35:35,252 : INFO : PROGRESS: at sentence #50000, processed 1157257 words, keeping 29704 word types\n",
      "2017-05-06 21:35:35,300 : INFO : PROGRESS: at sentence #60000, processed 1387247 words, keeping 32225 word types\n",
      "2017-05-06 21:35:35,345 : INFO : PROGRESS: at sentence #70000, processed 1618178 words, keeping 34360 word types\n",
      "2017-05-06 21:35:35,389 : INFO : PROGRESS: at sentence #80000, processed 1848004 words, keeping 36287 word types\n",
      "2017-05-06 21:35:35,438 : INFO : PROGRESS: at sentence #90000, processed 2077948 words, keeping 38042 word types\n",
      "2017-05-06 21:35:35,483 : INFO : PROGRESS: at sentence #100000, processed 2309347 words, keeping 39767 word types\n",
      "2017-05-06 21:35:35,529 : INFO : PROGRESS: at sentence #110000, processed 2541111 words, keeping 41317 word types\n",
      "2017-05-06 21:35:35,577 : INFO : PROGRESS: at sentence #120000, processed 2772131 words, keeping 42710 word types\n",
      "2017-05-06 21:35:35,624 : INFO : PROGRESS: at sentence #130000, processed 3002172 words, keeping 44010 word types\n",
      "2017-05-06 21:35:35,673 : INFO : PROGRESS: at sentence #140000, processed 3232261 words, keeping 45286 word types\n",
      "2017-05-06 21:35:35,720 : INFO : PROGRESS: at sentence #150000, processed 3461626 words, keeping 46520 word types\n",
      "2017-05-06 21:35:35,768 : INFO : PROGRESS: at sentence #160000, processed 3691038 words, keeping 47732 word types\n",
      "2017-05-06 21:35:35,815 : INFO : PROGRESS: at sentence #170000, processed 3921777 words, keeping 48878 word types\n",
      "2017-05-06 21:35:35,859 : INFO : collected 49890 word types from a corpus of 4141788 raw words and 179573 sentences\n",
      "2017-05-06 21:35:35,860 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:35:35,915 : INFO : min_count=5 retains 18427 unique words (36% of original 49890, drops 31463)\n",
      "2017-05-06 21:35:35,916 : INFO : min_count=5 leaves 4088708 word corpus (98% of original 4141788, drops 53080)\n",
      "2017-05-06 21:35:35,963 : INFO : deleting the raw counts dictionary of 49890 items\n",
      "2017-05-06 21:35:35,968 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-06 21:35:35,969 : INFO : downsampling leaves estimated 3077219 word corpus (75.3% of prior 4088708)\n",
      "2017-05-06 21:35:35,969 : INFO : estimated required memory for 18427 words and 300 dimensions: 53438300 bytes\n",
      "2017-05-06 21:35:36,035 : INFO : resetting layer weights\n",
      "2017-05-06 21:35:36,270 : INFO : training model with 4 workers on 18427 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:35:36,271 : INFO : expecting 179573 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:35:37,285 : INFO : PROGRESS: at 4.90% examples, 752148 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:38,291 : INFO : PROGRESS: at 9.68% examples, 741261 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:39,294 : INFO : PROGRESS: at 13.58% examples, 693963 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:40,298 : INFO : PROGRESS: at 18.03% examples, 690292 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:35:41,306 : INFO : PROGRESS: at 22.95% examples, 702465 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:42,314 : INFO : PROGRESS: at 27.81% examples, 709416 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:43,314 : INFO : PROGRESS: at 31.86% examples, 697117 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:44,315 : INFO : PROGRESS: at 35.48% examples, 679488 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:45,320 : INFO : PROGRESS: at 39.21% examples, 667156 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:46,325 : INFO : PROGRESS: at 42.82% examples, 655871 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:47,325 : INFO : PROGRESS: at 47.01% examples, 654942 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:35:48,344 : INFO : PROGRESS: at 51.68% examples, 659287 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:35:49,354 : INFO : PROGRESS: at 56.42% examples, 663985 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:35:50,361 : INFO : PROGRESS: at 61.20% examples, 668660 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:51,368 : INFO : PROGRESS: at 66.05% examples, 673713 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:35:52,375 : INFO : PROGRESS: at 70.98% examples, 678627 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:53,387 : INFO : PROGRESS: at 75.94% examples, 683117 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:54,396 : INFO : PROGRESS: at 80.78% examples, 686034 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:35:55,406 : INFO : PROGRESS: at 85.34% examples, 686704 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:56,408 : INFO : PROGRESS: at 89.93% examples, 687528 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:57,411 : INFO : PROGRESS: at 94.80% examples, 690351 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:58,417 : INFO : PROGRESS: at 99.59% examples, 692173 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:35:58,473 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:35:58,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:35:58,486 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:35:58,493 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:35:58,493 : INFO : training on 20708940 raw words (15386689 effective words) took 22.2s, 692653 effective words/s\n",
      "2017-05-06 21:35:58,494 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:35:58,609 : INFO : saving Word2Vec object under Results/dros_be_dim_parameter_300_model, separately None\n",
      "2017-05-06 21:35:58,610 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:35:58,610 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:36:00,299 : INFO : saved Results/dros_be_dim_parameter_300_model\n",
      "2017-05-06 21:36:00,489 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:36:00,490 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:36:00,528 : INFO : collected 13177 word types from a corpus of 185681 raw words and 7271 sentences\n",
      "2017-05-06 21:36:00,529 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:36:00,546 : INFO : min_count=5 retains 3537 unique words (26% of original 13177, drops 9640)\n",
      "2017-05-06 21:36:00,546 : INFO : min_count=5 leaves 170112 word corpus (91% of original 185681, drops 15569)\n",
      "2017-05-06 21:36:00,556 : INFO : deleting the raw counts dictionary of 13177 items\n",
      "2017-05-06 21:36:00,557 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-06 21:36:00,557 : INFO : downsampling leaves estimated 123095 word corpus (72.4% of prior 170112)\n",
      "2017-05-06 21:36:00,558 : INFO : estimated required memory for 3537 words and 400 dimensions: 13086900 bytes\n",
      "2017-05-06 21:36:00,570 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:00,634 : INFO : training model with 4 workers on 3537 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:36:00,635 : INFO : expecting 7271 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:36:01,389 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:36:01,400 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:36:01,407 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:36:01,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:36:01,411 : INFO : training on 928405 raw words (615812 effective words) took 0.8s, 798311 effective words/s\n",
      "2017-05-06 21:36:01,412 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:36:01,433 : INFO : saving Word2Vec object under Results/dros_strict_dim_parameter_400_model, separately None\n",
      "2017-05-06 21:36:01,434 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:36:01,435 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:36:01,562 : INFO : saved Results/dros_strict_dim_parameter_400_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:01,999 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:36:02,000 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:36:02,048 : INFO : PROGRESS: at sentence #10000, processed 244981 words, keeping 13594 word types\n",
      "2017-05-06 21:36:02,097 : INFO : PROGRESS: at sentence #20000, processed 488356 words, keeping 18196 word types\n",
      "2017-05-06 21:36:02,147 : INFO : PROGRESS: at sentence #30000, processed 731064 words, keeping 21332 word types\n",
      "2017-05-06 21:36:02,183 : INFO : collected 23335 word types from a corpus of 901524 raw words and 37017 sentences\n",
      "2017-05-06 21:36:02,184 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:02,214 : INFO : min_count=5 retains 8246 unique words (35% of original 23335, drops 15089)\n",
      "2017-05-06 21:36:02,215 : INFO : min_count=5 leaves 876113 word corpus (97% of original 901524, drops 25411)\n",
      "2017-05-06 21:36:02,239 : INFO : deleting the raw counts dictionary of 23335 items\n",
      "2017-05-06 21:36:02,241 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-06 21:36:02,242 : INFO : downsampling leaves estimated 646591 word corpus (73.8% of prior 876113)\n",
      "2017-05-06 21:36:02,242 : INFO : estimated required memory for 8246 words and 400 dimensions: 30510200 bytes\n",
      "2017-05-06 21:36:02,272 : INFO : resetting layer weights\n",
      "2017-05-06 21:36:02,397 : INFO : training model with 4 workers on 8246 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:36:02,398 : INFO : expecting 37017 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:36:03,428 : INFO : PROGRESS: at 20.60% examples, 653120 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:04,437 : INFO : PROGRESS: at 42.10% examples, 670864 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:05,441 : INFO : PROGRESS: at 61.14% examples, 651745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:06,444 : INFO : PROGRESS: at 83.06% examples, 665556 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:07,449 : INFO : PROGRESS: at 97.92% examples, 628067 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:07,547 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:36:07,563 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:36:07,595 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:36:07,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:36:07,605 : INFO : training on 4507620 raw words (3232644 effective words) took 5.2s, 622103 effective words/s\n",
      "2017-05-06 21:36:07,606 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:36:07,710 : INFO : saving Word2Vec object under Results/dros_gen_dim_parameter_400_model, separately None\n",
      "2017-05-06 21:36:07,711 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:36:07,713 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:36:08,523 : INFO : saved Results/dros_gen_dim_parameter_400_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:11,806 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:36:11,807 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:36:11,859 : INFO : PROGRESS: at sentence #10000, processed 231388 words, keeping 14641 word types\n",
      "2017-05-06 21:36:11,911 : INFO : PROGRESS: at sentence #20000, processed 463186 words, keeping 20111 word types\n",
      "2017-05-06 21:36:11,968 : INFO : PROGRESS: at sentence #30000, processed 694962 words, keeping 23963 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:12,024 : INFO : PROGRESS: at sentence #40000, processed 925654 words, keeping 27029 word types\n",
      "2017-05-06 21:36:12,079 : INFO : PROGRESS: at sentence #50000, processed 1157257 words, keeping 29704 word types\n",
      "2017-05-06 21:36:12,138 : INFO : PROGRESS: at sentence #60000, processed 1387247 words, keeping 32225 word types\n",
      "2017-05-06 21:36:12,195 : INFO : PROGRESS: at sentence #70000, processed 1618178 words, keeping 34360 word types\n",
      "2017-05-06 21:36:12,253 : INFO : PROGRESS: at sentence #80000, processed 1848004 words, keeping 36287 word types\n",
      "2017-05-06 21:36:12,308 : INFO : PROGRESS: at sentence #90000, processed 2077948 words, keeping 38042 word types\n",
      "2017-05-06 21:36:12,370 : INFO : PROGRESS: at sentence #100000, processed 2309347 words, keeping 39767 word types\n",
      "2017-05-06 21:36:12,433 : INFO : PROGRESS: at sentence #110000, processed 2541111 words, keeping 41317 word types\n",
      "2017-05-06 21:36:12,492 : INFO : PROGRESS: at sentence #120000, processed 2772131 words, keeping 42710 word types\n",
      "2017-05-06 21:36:12,542 : INFO : PROGRESS: at sentence #130000, processed 3002172 words, keeping 44010 word types\n",
      "2017-05-06 21:36:12,591 : INFO : PROGRESS: at sentence #140000, processed 3232261 words, keeping 45286 word types\n",
      "2017-05-06 21:36:12,638 : INFO : PROGRESS: at sentence #150000, processed 3461626 words, keeping 46520 word types\n",
      "2017-05-06 21:36:12,684 : INFO : PROGRESS: at sentence #160000, processed 3691038 words, keeping 47732 word types\n",
      "2017-05-06 21:36:12,732 : INFO : PROGRESS: at sentence #170000, processed 3921777 words, keeping 48878 word types\n",
      "2017-05-06 21:36:12,777 : INFO : collected 49890 word types from a corpus of 4141788 raw words and 179573 sentences\n",
      "2017-05-06 21:36:12,778 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:36:12,830 : INFO : min_count=5 retains 18427 unique words (36% of original 49890, drops 31463)\n",
      "2017-05-06 21:36:12,831 : INFO : min_count=5 leaves 4088708 word corpus (98% of original 4141788, drops 53080)\n",
      "2017-05-06 21:36:12,877 : INFO : deleting the raw counts dictionary of 49890 items\n",
      "2017-05-06 21:36:12,881 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-06 21:36:12,882 : INFO : downsampling leaves estimated 3077219 word corpus (75.3% of prior 4088708)\n",
      "2017-05-06 21:36:12,883 : INFO : estimated required memory for 18427 words and 400 dimensions: 68179900 bytes\n",
      "2017-05-06 21:36:12,954 : INFO : resetting layer weights\n",
      "2017-05-06 21:36:13,222 : INFO : training model with 4 workers on 18427 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:36:13,223 : INFO : expecting 179573 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:36:14,232 : INFO : PROGRESS: at 3.74% examples, 575895 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:15,243 : INFO : PROGRESS: at 7.70% examples, 589067 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:16,253 : INFO : PROGRESS: at 11.84% examples, 603479 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:17,256 : INFO : PROGRESS: at 15.75% examples, 602058 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:18,258 : INFO : PROGRESS: at 19.73% examples, 603121 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:19,266 : INFO : PROGRESS: at 23.09% examples, 588496 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:20,293 : INFO : PROGRESS: at 26.37% examples, 574393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:21,301 : INFO : PROGRESS: at 29.65% examples, 565273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:22,309 : INFO : PROGRESS: at 32.87% examples, 557155 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:23,321 : INFO : PROGRESS: at 36.50% examples, 556408 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:24,321 : INFO : PROGRESS: at 40.13% examples, 556457 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:25,324 : INFO : PROGRESS: at 43.92% examples, 558789 words/s, in_qsize 6, out_qsize 2\n",
      "2017-05-06 21:36:26,352 : INFO : PROGRESS: at 47.58% examples, 558029 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:27,364 : INFO : PROGRESS: at 51.01% examples, 555376 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:28,376 : INFO : PROGRESS: at 54.28% examples, 551567 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:29,385 : INFO : PROGRESS: at 57.34% examples, 546050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:30,423 : INFO : PROGRESS: at 60.43% examples, 540700 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:31,424 : INFO : PROGRESS: at 63.55% examples, 537505 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:32,441 : INFO : PROGRESS: at 67.50% examples, 540738 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:36:33,445 : INFO : PROGRESS: at 70.78% examples, 538866 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:36:34,451 : INFO : PROGRESS: at 74.20% examples, 538141 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:35,477 : INFO : PROGRESS: at 77.88% examples, 538667 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:36:36,478 : INFO : PROGRESS: at 81.35% examples, 538459 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:37,479 : INFO : PROGRESS: at 84.81% examples, 538268 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:38,511 : INFO : PROGRESS: at 88.39% examples, 538029 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:39,517 : INFO : PROGRESS: at 91.66% examples, 536633 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:36:40,539 : INFO : PROGRESS: at 95.23% examples, 536627 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:41,542 : INFO : PROGRESS: at 98.33% examples, 534381 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-06 21:36:41,955 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:36:41,956 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:36:41,967 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:36:41,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:36:41,977 : INFO : training on 20708940 raw words (15387262 effective words) took 28.7s, 535225 effective words/s\n",
      "2017-05-06 21:36:41,978 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:36:42,104 : INFO : saving Word2Vec object under Results/dros_be_dim_parameter_400_model, separately None\n",
      "2017-05-06 21:36:42,105 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:36:42,106 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:36:44,119 : INFO : saved Results/dros_be_dim_parameter_400_model\n",
      "2017-05-06 21:36:44,327 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:36:44,328 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:36:44,367 : INFO : collected 13177 word types from a corpus of 185681 raw words and 7271 sentences\n",
      "2017-05-06 21:36:44,368 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:36:44,384 : INFO : min_count=5 retains 3537 unique words (26% of original 13177, drops 9640)\n",
      "2017-05-06 21:36:44,385 : INFO : min_count=5 leaves 170112 word corpus (91% of original 185681, drops 15569)\n",
      "2017-05-06 21:36:44,394 : INFO : deleting the raw counts dictionary of 13177 items\n",
      "2017-05-06 21:36:44,396 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-06 21:36:44,396 : INFO : downsampling leaves estimated 123095 word corpus (72.4% of prior 170112)\n",
      "2017-05-06 21:36:44,397 : INFO : estimated required memory for 3537 words and 500 dimensions: 15916500 bytes\n",
      "2017-05-06 21:36:44,410 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:44,471 : INFO : training model with 4 workers on 3537 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:36:44,472 : INFO : expecting 7271 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:36:45,484 : INFO : PROGRESS: at 80.68% examples, 492468 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:36:45,672 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:36:45,676 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:36:45,689 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:36:45,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:36:45,697 : INFO : training on 928405 raw words (615137 effective words) took 1.2s, 504102 effective words/s\n",
      "2017-05-06 21:36:45,698 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:36:45,719 : INFO : saving Word2Vec object under Results/dros_strict_dim_parameter_500_model, separately None\n",
      "2017-05-06 21:36:45,720 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:36:45,721 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:36:45,845 : INFO : saved Results/dros_strict_dim_parameter_500_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:46,299 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:36:46,299 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:36:46,352 : INFO : PROGRESS: at sentence #10000, processed 244981 words, keeping 13594 word types\n",
      "2017-05-06 21:36:46,404 : INFO : PROGRESS: at sentence #20000, processed 488356 words, keeping 18196 word types\n",
      "2017-05-06 21:36:46,460 : INFO : PROGRESS: at sentence #30000, processed 731064 words, keeping 21332 word types\n",
      "2017-05-06 21:36:46,497 : INFO : collected 23335 word types from a corpus of 901524 raw words and 37017 sentences\n",
      "2017-05-06 21:36:46,498 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:46,526 : INFO : min_count=5 retains 8246 unique words (35% of original 23335, drops 15089)\n",
      "2017-05-06 21:36:46,527 : INFO : min_count=5 leaves 876113 word corpus (97% of original 901524, drops 25411)\n",
      "2017-05-06 21:36:46,552 : INFO : deleting the raw counts dictionary of 23335 items\n",
      "2017-05-06 21:36:46,553 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-06 21:36:46,554 : INFO : downsampling leaves estimated 646591 word corpus (73.8% of prior 876113)\n",
      "2017-05-06 21:36:46,555 : INFO : estimated required memory for 8246 words and 500 dimensions: 37107000 bytes\n",
      "2017-05-06 21:36:46,589 : INFO : resetting layer weights\n",
      "2017-05-06 21:36:46,726 : INFO : training model with 4 workers on 8246 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:36:46,727 : INFO : expecting 37017 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:36:47,764 : INFO : PROGRESS: at 13.73% examples, 430884 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:48,766 : INFO : PROGRESS: at 27.67% examples, 440074 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:36:49,767 : INFO : PROGRESS: at 41.87% examples, 446158 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:50,768 : INFO : PROGRESS: at 55.15% examples, 442077 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:36:51,795 : INFO : PROGRESS: at 69.54% examples, 444296 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:52,798 : INFO : PROGRESS: at 83.72% examples, 446402 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:53,803 : INFO : PROGRESS: at 97.03% examples, 443745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:36:53,954 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:36:53,964 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:36:53,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:36:53,982 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:36:53,984 : INFO : training on 4507620 raw words (3232757 effective words) took 7.3s, 445843 effective words/s\n",
      "2017-05-06 21:36:53,985 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:36:54,035 : INFO : saving Word2Vec object under Results/dros_gen_dim_parameter_500_model, separately None\n",
      "2017-05-06 21:36:54,035 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:36:54,036 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:36:54,315 : INFO : saved Results/dros_gen_dim_parameter_500_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:57,698 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:36:57,699 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:36:57,776 : INFO : PROGRESS: at sentence #10000, processed 231388 words, keeping 14641 word types\n",
      "2017-05-06 21:36:57,853 : INFO : PROGRESS: at sentence #20000, processed 463186 words, keeping 20111 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:36:57,931 : INFO : PROGRESS: at sentence #30000, processed 694962 words, keeping 23963 word types\n",
      "2017-05-06 21:36:58,010 : INFO : PROGRESS: at sentence #40000, processed 925654 words, keeping 27029 word types\n",
      "2017-05-06 21:36:58,089 : INFO : PROGRESS: at sentence #50000, processed 1157257 words, keeping 29704 word types\n",
      "2017-05-06 21:36:58,167 : INFO : PROGRESS: at sentence #60000, processed 1387247 words, keeping 32225 word types\n",
      "2017-05-06 21:36:58,244 : INFO : PROGRESS: at sentence #70000, processed 1618178 words, keeping 34360 word types\n",
      "2017-05-06 21:36:58,323 : INFO : PROGRESS: at sentence #80000, processed 1848004 words, keeping 36287 word types\n",
      "2017-05-06 21:36:58,411 : INFO : PROGRESS: at sentence #90000, processed 2077948 words, keeping 38042 word types\n",
      "2017-05-06 21:36:58,493 : INFO : PROGRESS: at sentence #100000, processed 2309347 words, keeping 39767 word types\n",
      "2017-05-06 21:36:58,575 : INFO : PROGRESS: at sentence #110000, processed 2541111 words, keeping 41317 word types\n",
      "2017-05-06 21:36:58,654 : INFO : PROGRESS: at sentence #120000, processed 2772131 words, keeping 42710 word types\n",
      "2017-05-06 21:36:58,732 : INFO : PROGRESS: at sentence #130000, processed 3002172 words, keeping 44010 word types\n",
      "2017-05-06 21:36:58,789 : INFO : PROGRESS: at sentence #140000, processed 3232261 words, keeping 45286 word types\n",
      "2017-05-06 21:36:58,839 : INFO : PROGRESS: at sentence #150000, processed 3461626 words, keeping 46520 word types\n",
      "2017-05-06 21:36:58,887 : INFO : PROGRESS: at sentence #160000, processed 3691038 words, keeping 47732 word types\n",
      "2017-05-06 21:36:58,935 : INFO : PROGRESS: at sentence #170000, processed 3921777 words, keeping 48878 word types\n",
      "2017-05-06 21:36:58,980 : INFO : collected 49890 word types from a corpus of 4141788 raw words and 179573 sentences\n",
      "2017-05-06 21:36:58,981 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:36:59,037 : INFO : min_count=5 retains 18427 unique words (36% of original 49890, drops 31463)\n",
      "2017-05-06 21:36:59,038 : INFO : min_count=5 leaves 4088708 word corpus (98% of original 4141788, drops 53080)\n",
      "2017-05-06 21:36:59,084 : INFO : deleting the raw counts dictionary of 49890 items\n",
      "2017-05-06 21:36:59,087 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-06 21:36:59,088 : INFO : downsampling leaves estimated 3077219 word corpus (75.3% of prior 4088708)\n",
      "2017-05-06 21:36:59,088 : INFO : estimated required memory for 18427 words and 500 dimensions: 82921500 bytes\n",
      "2017-05-06 21:36:59,154 : INFO : resetting layer weights\n",
      "2017-05-06 21:36:59,433 : INFO : training model with 4 workers on 18427 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:36:59,433 : INFO : expecting 179573 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:37:00,442 : INFO : PROGRESS: at 2.98% examples, 458614 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:01,468 : INFO : PROGRESS: at 6.21% examples, 471994 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:02,473 : INFO : PROGRESS: at 9.44% examples, 479658 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:03,474 : INFO : PROGRESS: at 12.61% examples, 481895 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:04,480 : INFO : PROGRESS: at 15.90% examples, 485754 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:05,486 : INFO : PROGRESS: at 19.00% examples, 483492 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:06,503 : INFO : PROGRESS: at 22.12% examples, 482228 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:07,521 : INFO : PROGRESS: at 24.77% examples, 471996 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:08,524 : INFO : PROGRESS: at 27.47% examples, 465642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:09,555 : INFO : PROGRESS: at 29.99% examples, 456355 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:10,573 : INFO : PROGRESS: at 32.63% examples, 451236 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:11,577 : INFO : PROGRESS: at 35.67% examples, 452406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:12,585 : INFO : PROGRESS: at 38.87% examples, 454991 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:37:13,586 : INFO : PROGRESS: at 41.99% examples, 456896 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:14,594 : INFO : PROGRESS: at 45.03% examples, 457364 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:15,629 : INFO : PROGRESS: at 47.73% examples, 453791 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:16,670 : INFO : PROGRESS: at 50.29% examples, 449197 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:17,684 : INFO : PROGRESS: at 52.93% examples, 446606 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:18,705 : INFO : PROGRESS: at 55.69% examples, 444875 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:19,738 : INFO : PROGRESS: at 58.83% examples, 446006 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-06 21:37:20,739 : INFO : PROGRESS: at 61.77% examples, 446282 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:21,757 : INFO : PROGRESS: at 64.71% examples, 446208 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:22,788 : INFO : PROGRESS: at 67.79% examples, 446838 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:23,822 : INFO : PROGRESS: at 70.44% examples, 444660 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:24,829 : INFO : PROGRESS: at 73.04% examples, 442793 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:25,830 : INFO : PROGRESS: at 75.70% examples, 441446 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:26,833 : INFO : PROGRESS: at 78.41% examples, 440462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:27,837 : INFO : PROGRESS: at 81.50% examples, 441600 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:28,862 : INFO : PROGRESS: at 84.43% examples, 441604 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:29,862 : INFO : PROGRESS: at 87.56% examples, 442952 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:30,868 : INFO : PROGRESS: at 90.55% examples, 443403 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:31,878 : INFO : PROGRESS: at 93.06% examples, 441506 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:37:32,895 : INFO : PROGRESS: at 95.76% examples, 440493 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:33,918 : INFO : PROGRESS: at 98.47% examples, 439481 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:34,406 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:37:34,423 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:37:34,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:37:34,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:37:34,435 : INFO : training on 20708940 raw words (15387686 effective words) took 35.0s, 439693 effective words/s\n",
      "2017-05-06 21:37:34,436 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:37:34,646 : INFO : saving Word2Vec object under Results/dros_be_dim_parameter_500_model, separately None\n",
      "2017-05-06 21:37:34,647 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:37:34,648 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:37:35,419 : INFO : saved Results/dros_be_dim_parameter_500_model\n",
      "2017-05-06 21:37:35,623 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:37:35,624 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:37:35,667 : INFO : collected 13177 word types from a corpus of 185681 raw words and 7271 sentences\n",
      "2017-05-06 21:37:35,668 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:37:35,686 : INFO : min_count=5 retains 3537 unique words (26% of original 13177, drops 9640)\n",
      "2017-05-06 21:37:35,687 : INFO : min_count=5 leaves 170112 word corpus (91% of original 185681, drops 15569)\n",
      "2017-05-06 21:37:35,698 : INFO : deleting the raw counts dictionary of 13177 items\n",
      "2017-05-06 21:37:35,701 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-06 21:37:35,702 : INFO : downsampling leaves estimated 123095 word corpus (72.4% of prior 170112)\n",
      "2017-05-06 21:37:35,702 : INFO : estimated required memory for 3537 words and 600 dimensions: 18746100 bytes\n",
      "2017-05-06 21:37:35,713 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:37:35,788 : INFO : training model with 4 workers on 3537 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:37:35,789 : INFO : expecting 7271 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:37:36,812 : INFO : PROGRESS: at 76.41% examples, 462005 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:37,075 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:37:37,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:37:37,111 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:37:37,119 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:37:37,120 : INFO : training on 928405 raw words (615674 effective words) took 1.3s, 464533 effective words/s\n",
      "2017-05-06 21:37:37,121 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:37:37,153 : INFO : saving Word2Vec object under Results/dros_strict_dim_parameter_600_model, separately None\n",
      "2017-05-06 21:37:37,154 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:37:37,154 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:37:37,648 : INFO : saved Results/dros_strict_dim_parameter_600_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:37:38,148 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:37:38,149 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:37:38,210 : INFO : PROGRESS: at sentence #10000, processed 244981 words, keeping 13594 word types\n",
      "2017-05-06 21:37:38,267 : INFO : PROGRESS: at sentence #20000, processed 488356 words, keeping 18196 word types\n",
      "2017-05-06 21:37:38,325 : INFO : PROGRESS: at sentence #30000, processed 731064 words, keeping 21332 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:37:38,366 : INFO : collected 23335 word types from a corpus of 901524 raw words and 37017 sentences\n",
      "2017-05-06 21:37:38,367 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:37:38,396 : INFO : min_count=5 retains 8246 unique words (35% of original 23335, drops 15089)\n",
      "2017-05-06 21:37:38,397 : INFO : min_count=5 leaves 876113 word corpus (97% of original 901524, drops 25411)\n",
      "2017-05-06 21:37:38,422 : INFO : deleting the raw counts dictionary of 23335 items\n",
      "2017-05-06 21:37:38,424 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-06 21:37:38,425 : INFO : downsampling leaves estimated 646591 word corpus (73.8% of prior 876113)\n",
      "2017-05-06 21:37:38,425 : INFO : estimated required memory for 8246 words and 600 dimensions: 43703800 bytes\n",
      "2017-05-06 21:37:38,455 : INFO : resetting layer weights\n",
      "2017-05-06 21:37:38,622 : INFO : training model with 4 workers on 8246 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:37:38,623 : INFO : expecting 37017 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:37:39,631 : INFO : PROGRESS: at 13.51% examples, 435925 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-06 21:37:40,647 : INFO : PROGRESS: at 27.89% examples, 447077 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:37:41,650 : INFO : PROGRESS: at 42.10% examples, 450494 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:42,666 : INFO : PROGRESS: at 56.26% examples, 450712 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:43,679 : INFO : PROGRESS: at 69.98% examples, 448151 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:44,688 : INFO : PROGRESS: at 84.39% examples, 450376 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:45,699 : INFO : PROGRESS: at 98.14% examples, 448788 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:45,787 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:37:45,796 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:37:45,806 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:37:45,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:37:45,817 : INFO : training on 4507620 raw words (3232313 effective words) took 7.2s, 449740 effective words/s\n",
      "2017-05-06 21:37:45,819 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:37:45,884 : INFO : saving Word2Vec object under Results/dros_gen_dim_parameter_600_model, separately None\n",
      "2017-05-06 21:37:45,885 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:37:45,885 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:37:46,260 : INFO : saved Results/dros_gen_dim_parameter_600_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:37:48,510 : INFO : collecting all words and their counts\n",
      "2017-05-06 21:37:48,511 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-06 21:37:48,558 : INFO : PROGRESS: at sentence #10000, processed 231388 words, keeping 14641 word types\n",
      "2017-05-06 21:37:48,606 : INFO : PROGRESS: at sentence #20000, processed 463186 words, keeping 20111 word types\n",
      "2017-05-06 21:37:48,653 : INFO : PROGRESS: at sentence #30000, processed 694962 words, keeping 23963 word types\n",
      "2017-05-06 21:37:48,702 : INFO : PROGRESS: at sentence #40000, processed 925654 words, keeping 27029 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-06 21:37:48,749 : INFO : PROGRESS: at sentence #50000, processed 1157257 words, keeping 29704 word types\n",
      "2017-05-06 21:37:48,799 : INFO : PROGRESS: at sentence #60000, processed 1387247 words, keeping 32225 word types\n",
      "2017-05-06 21:37:48,848 : INFO : PROGRESS: at sentence #70000, processed 1618178 words, keeping 34360 word types\n",
      "2017-05-06 21:37:48,893 : INFO : PROGRESS: at sentence #80000, processed 1848004 words, keeping 36287 word types\n",
      "2017-05-06 21:37:48,950 : INFO : PROGRESS: at sentence #90000, processed 2077948 words, keeping 38042 word types\n",
      "2017-05-06 21:37:49,003 : INFO : PROGRESS: at sentence #100000, processed 2309347 words, keeping 39767 word types\n",
      "2017-05-06 21:37:49,057 : INFO : PROGRESS: at sentence #110000, processed 2541111 words, keeping 41317 word types\n",
      "2017-05-06 21:37:49,110 : INFO : PROGRESS: at sentence #120000, processed 2772131 words, keeping 42710 word types\n",
      "2017-05-06 21:37:49,169 : INFO : PROGRESS: at sentence #130000, processed 3002172 words, keeping 44010 word types\n",
      "2017-05-06 21:37:49,217 : INFO : PROGRESS: at sentence #140000, processed 3232261 words, keeping 45286 word types\n",
      "2017-05-06 21:37:49,268 : INFO : PROGRESS: at sentence #150000, processed 3461626 words, keeping 46520 word types\n",
      "2017-05-06 21:37:49,317 : INFO : PROGRESS: at sentence #160000, processed 3691038 words, keeping 47732 word types\n",
      "2017-05-06 21:37:49,371 : INFO : PROGRESS: at sentence #170000, processed 3921777 words, keeping 48878 word types\n",
      "2017-05-06 21:37:49,421 : INFO : collected 49890 word types from a corpus of 4141788 raw words and 179573 sentences\n",
      "2017-05-06 21:37:49,421 : INFO : Loading a fresh vocabulary\n",
      "2017-05-06 21:37:49,478 : INFO : min_count=5 retains 18427 unique words (36% of original 49890, drops 31463)\n",
      "2017-05-06 21:37:49,479 : INFO : min_count=5 leaves 4088708 word corpus (98% of original 4141788, drops 53080)\n",
      "2017-05-06 21:37:49,529 : INFO : deleting the raw counts dictionary of 49890 items\n",
      "2017-05-06 21:37:49,533 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-06 21:37:49,534 : INFO : downsampling leaves estimated 3077219 word corpus (75.3% of prior 4088708)\n",
      "2017-05-06 21:37:49,535 : INFO : estimated required memory for 18427 words and 600 dimensions: 97663100 bytes\n",
      "2017-05-06 21:37:49,604 : INFO : resetting layer weights\n",
      "2017-05-06 21:37:49,926 : INFO : training model with 4 workers on 18427 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-06 21:37:49,927 : INFO : expecting 179573 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-06 21:37:50,952 : INFO : PROGRESS: at 2.41% examples, 365670 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:51,960 : INFO : PROGRESS: at 4.95% examples, 377592 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:52,965 : INFO : PROGRESS: at 7.46% examples, 379706 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:54,007 : INFO : PROGRESS: at 10.07% examples, 381024 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:37:55,070 : INFO : PROGRESS: at 12.47% examples, 374431 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:56,078 : INFO : PROGRESS: at 14.45% examples, 362453 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:37:57,084 : INFO : PROGRESS: at 16.44% examples, 353937 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:58,085 : INFO : PROGRESS: at 18.42% examples, 347797 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:37:59,095 : INFO : PROGRESS: at 20.40% examples, 342641 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:00,102 : INFO : PROGRESS: at 22.66% examples, 343015 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:01,122 : INFO : PROGRESS: at 24.96% examples, 343597 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:02,149 : INFO : PROGRESS: at 27.28% examples, 343886 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:03,163 : INFO : PROGRESS: at 29.70% examples, 345599 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:04,187 : INFO : PROGRESS: at 32.24% examples, 348334 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:38:05,194 : INFO : PROGRESS: at 34.81% examples, 351122 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:06,207 : INFO : PROGRESS: at 37.37% examples, 353421 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:07,238 : INFO : PROGRESS: at 39.93% examples, 355101 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:08,245 : INFO : PROGRESS: at 42.48% examples, 357052 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:09,251 : INFO : PROGRESS: at 44.98% examples, 358431 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:38:10,281 : INFO : PROGRESS: at 47.58% examples, 359979 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:11,305 : INFO : PROGRESS: at 50.15% examples, 361133 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:12,315 : INFO : PROGRESS: at 52.74% examples, 362752 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:13,315 : INFO : PROGRESS: at 55.25% examples, 363724 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:14,359 : INFO : PROGRESS: at 57.91% examples, 364882 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-06 21:38:15,377 : INFO : PROGRESS: at 60.23% examples, 364282 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:16,391 : INFO : PROGRESS: at 62.35% examples, 362675 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:17,394 : INFO : PROGRESS: at 64.27% examples, 360230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:18,413 : INFO : PROGRESS: at 66.30% examples, 358298 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:19,418 : INFO : PROGRESS: at 68.42% examples, 357169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:20,450 : INFO : PROGRESS: at 70.49% examples, 355536 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:21,459 : INFO : PROGRESS: at 72.70% examples, 354972 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:22,464 : INFO : PROGRESS: at 74.93% examples, 354504 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:23,496 : INFO : PROGRESS: at 77.45% examples, 355094 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:24,513 : INFO : PROGRESS: at 79.72% examples, 354742 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:25,513 : INFO : PROGRESS: at 81.83% examples, 353935 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:26,516 : INFO : PROGRESS: at 84.28% examples, 354582 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-06 21:38:27,531 : INFO : PROGRESS: at 86.41% examples, 353701 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:28,547 : INFO : PROGRESS: at 88.58% examples, 353058 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:29,564 : INFO : PROGRESS: at 90.70% examples, 352236 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:30,575 : INFO : PROGRESS: at 93.30% examples, 353335 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:31,583 : INFO : PROGRESS: at 96.06% examples, 354932 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-06 21:38:32,598 : INFO : PROGRESS: at 98.91% examples, 356757 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-06 21:38:32,917 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-06 21:38:32,922 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-06 21:38:32,926 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-06 21:38:32,939 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-06 21:38:32,940 : INFO : training on 20708940 raw words (15386617 effective words) took 43.0s, 357808 effective words/s\n",
      "2017-05-06 21:38:32,941 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-06 21:38:33,077 : INFO : saving Word2Vec object under Results/dros_be_dim_parameter_600_model, separately None\n",
      "2017-05-06 21:38:33,077 : INFO : storing np array 'syn0' to Results/dros_be_dim_parameter_600_model.wv.syn0.npy\n",
      "2017-05-06 21:38:33,100 : INFO : not storing attribute syn0norm\n",
      "2017-05-06 21:38:33,101 : INFO : storing np array 'syn1neg' to Results/dros_be_dim_parameter_600_model.syn1neg.npy\n",
      "2017-05-06 21:38:33,127 : INFO : not storing attribute cum_table\n",
      "2017-05-06 21:38:33,174 : INFO : saved Results/dros_be_dim_parameter_600_model\n"
     ]
    }
   ],
   "source": [
    "dimensions = [100, 200, 300, 400, 500, 600]\n",
    "for i in dimensions:\n",
    "    w2v_parameters = [i, 5, 4, 6, 0.001]\n",
    "    strict = pred.make_w2v_model(dros_strict_real, 'dros_strict_dim_parameter_' + str(i), w2v_parameters)\n",
    "    gen = pred.make_w2v_model(dros_gen_real, 'dros_gen_dim_parameter_' + str(i), w2v_parameters)\n",
    "    be = pred.make_w2v_model(dros_be_real, 'dros_be_dim_parameter_' + str(i), w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    }
   ],
   "source": [
    "for i in dimensions:\n",
    "    w2w_model_strict = word2vec.Word2Vec.load('Results/dros_strict_dim_parameter_'+str(i)+'_model')\n",
    "    w2w_model_gen = word2vec.Word2Vec.load('Results/dros_gen_dim_parameter_'+str(i)+'_model')\n",
    "    w2w_model_be = word2vec.Word2Vec.load('Results/dros_be_dim_parameter_'+str(i)+'_model')\n",
    "    \n",
    "    for seed in random_seeds:\n",
    "        data_name = 'Results/dros_tr_val_split_' + str(seed)\n",
    "        train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "        train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "        validation_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "        validation_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_strict, feature_count=i)\n",
    "        \n",
    "        strict_list_SR_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_gen, feature_count=i)\n",
    "        \n",
    "        strict_list_GEN_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                      train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_be, feature_count=i)\n",
    "        \n",
    "        strict_list_BE_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        pickle.dump(strict_list_SR_dims_param, open('Results/dros_strict_list_SR_dims_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_GEN_dims_param, open('Results/dros_strict_list_GEN_dims_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_BE_dims_param, open('Results/dros_strict_list_BE_dims_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        \n",
    "        strict_final_list = [strict_list_SR_dims_param, \n",
    "                             strict_list_GEN_dims_param, \n",
    "                             strict_list_BE_dims_param]\n",
    "        print ('\\nPredicting\\n')\n",
    "        errors = []\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "\n",
    "        for entry in strict_final_list:\n",
    "            error_w2v_norm, probs_w2v_norm = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                                 entry[2], entry[3])\n",
    "            fpr_w2v_norm, tpr_w2v_norm, _ = roc_curve(entry[3], probs_w2v_norm)\n",
    "            error_w2v_fs, probs_w2v_fs = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                             entry[2], entry[3],\n",
    "                                                             feature_selection=True)\n",
    "            fpr_w2v_fs, tpr_w2v_fs, _ = roc_curve(entry[3], probs_w2v_fs)\n",
    "\n",
    "            errors.append([error_w2v_norm, error_w2v_fs])\n",
    "            fpr.append([fpr_w2v_norm, fpr_w2v_fs])\n",
    "            tpr.append([tpr_w2v_norm, tpr_w2v_fs])\n",
    "            \n",
    "        pickle.dump(errors, open('Results/dros_dim_param'+str(i)+'_errors_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(fpr, open('Results/dros_dim_param'+str(i)+'_fpr_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(tpr, open('Results/dros_dim_param'+str(i)+'_tpr_pickle_'+str(seed)+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mult_open(direct, pattern):\n",
    "    pickle_list = []\n",
    "    file_list = []\n",
    "    for f in os.listdir(direct):\n",
    "        if re.search(pattern, f):\n",
    "            f = f.split('_')\n",
    "            f[-1] = f[-1][:-4]\n",
    "            file_list.append(f)\n",
    "    file_list.sort(key = lambda x: int(x[-1]))\n",
    "    for file in file_list:\n",
    "        file = '_'.join(file)\n",
    "        file = file + '.pkl'\n",
    "        pkl = pickle.load(open(os.path.join(direct, file), 'rb'))\n",
    "        pickle_list.append(pkl)\n",
    "    return pickle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims = 100 Drosophila mean strict error original -  0.351453359042\n",
      "Dims = 100 Drosophila mean strict error feature selection -  0.349877380232\n",
      "Dims = 100 Drosophila mean strict AUC original -  0.48823523874\n",
      "Dims = 100 Drosophila mean strict AUC feature selection -  0.493827666301\n",
      "\n",
      "\n",
      "Dims = 100 Drosophila mean gen error original -  0.352627205145\n",
      "Dims = 100 Drosophila mean gen error feature selection -  0.352746845854\n",
      "Dims = 100 Drosophila mean gen AUC original -  0.498002964969\n",
      "Dims = 100 Drosophila mean gen AUC feature selection -  0.504285094557\n",
      "\n",
      "\n",
      "Dims = 100 Drosophila mean BE error original -  0.34692404945\n",
      "Dims = 100 Drosophila mean BE error feature selection -  0.345171406626\n",
      "Dims = 100 Drosophila mean BE AUC original -  0.495656337464\n",
      "Dims = 100 Drosophila mean BE AUC feature selection -  0.492639575514\n",
      "\n",
      "\n",
      "Dims = 200 Drosophila mean strict error original -  0.34775566577\n",
      "Dims = 200 Drosophila mean strict error feature selection -  0.347167134976\n",
      "Dims = 200 Drosophila mean strict AUC original -  0.518242149813\n",
      "Dims = 200 Drosophila mean strict AUC feature selection -  0.519622695748\n",
      "\n",
      "\n",
      "Dims = 200 Drosophila mean gen error original -  0.347834724189\n",
      "Dims = 200 Drosophila mean gen error feature selection -  0.349838895957\n",
      "Dims = 200 Drosophila mean gen AUC original -  0.506520367903\n",
      "Dims = 200 Drosophila mean gen AUC feature selection -  0.520416510331\n",
      "\n",
      "\n",
      "Dims = 200 Drosophila mean BE error original -  0.349703050248\n",
      "Dims = 200 Drosophila mean BE error feature selection -  0.355382920282\n",
      "Dims = 200 Drosophila mean BE AUC original -  0.49458909605\n",
      "Dims = 200 Drosophila mean BE AUC feature selection -  0.485498559817\n",
      "\n",
      "\n",
      "Dims = 300 Drosophila mean strict error original -  0.346860365652\n",
      "Dims = 300 Drosophila mean strict error feature selection -  0.352595066359\n",
      "Dims = 300 Drosophila mean strict AUC original -  0.497662611289\n",
      "Dims = 300 Drosophila mean strict AUC feature selection -  0.495140712247\n",
      "\n",
      "\n",
      "Dims = 300 Drosophila mean gen error original -  0.350425201981\n",
      "Dims = 300 Drosophila mean gen error feature selection -  0.353376719427\n",
      "Dims = 300 Drosophila mean gen AUC original -  0.497951234897\n",
      "Dims = 300 Drosophila mean gen AUC feature selection -  0.488438900647\n",
      "\n",
      "\n",
      "Dims = 300 Drosophila mean BE error original -  0.351804972649\n",
      "Dims = 300 Drosophila mean BE error feature selection -  0.348815279253\n",
      "Dims = 300 Drosophila mean BE AUC original -  0.498330663255\n",
      "Dims = 300 Drosophila mean BE AUC feature selection -  0.483447994672\n",
      "\n",
      "\n",
      "Dims = 400 Drosophila mean strict error original -  0.341673787024\n",
      "Dims = 400 Drosophila mean strict error feature selection -  0.344693965805\n",
      "Dims = 400 Drosophila mean strict AUC original -  0.530355182715\n",
      "Dims = 400 Drosophila mean strict AUC feature selection -  0.522848523164\n",
      "\n",
      "\n",
      "Dims = 400 Drosophila mean gen error original -  0.350242101114\n",
      "Dims = 400 Drosophila mean gen error feature selection -  0.35751318816\n",
      "Dims = 400 Drosophila mean gen AUC original -  0.504174192718\n",
      "Dims = 400 Drosophila mean gen AUC feature selection -  0.501883436351\n",
      "\n",
      "\n",
      "Dims = 400 Drosophila mean BE error original -  0.358574968713\n",
      "Dims = 400 Drosophila mean BE error feature selection -  0.353390043549\n",
      "Dims = 400 Drosophila mean BE AUC original -  0.491053228877\n",
      "Dims = 400 Drosophila mean BE AUC feature selection -  0.495965522422\n",
      "\n",
      "\n",
      "Dims = 500 Drosophila mean strict error original -  0.35403530142\n",
      "Dims = 500 Drosophila mean strict error feature selection -  0.351593650127\n",
      "Dims = 500 Drosophila mean strict AUC original -  0.51306193792\n",
      "Dims = 500 Drosophila mean strict AUC feature selection -  0.498997252443\n",
      "\n",
      "\n",
      "Dims = 500 Drosophila mean gen error original -  0.355430289006\n",
      "Dims = 500 Drosophila mean gen error feature selection -  0.3554041633\n",
      "Dims = 500 Drosophila mean gen AUC original -  0.495890341956\n",
      "Dims = 500 Drosophila mean gen AUC feature selection -  0.496831193417\n",
      "\n",
      "\n",
      "Dims = 500 Drosophila mean BE error original -  0.352619447336\n",
      "Dims = 500 Drosophila mean BE error feature selection -  0.362518650701\n",
      "Dims = 500 Drosophila mean BE AUC original -  0.498901271933\n",
      "Dims = 500 Drosophila mean BE AUC feature selection -  0.49154261637\n",
      "\n",
      "\n",
      "Dims = 600 Drosophila mean strict error original -  0.352898991368\n",
      "Dims = 600 Drosophila mean strict error feature selection -  0.350095353397\n",
      "Dims = 600 Drosophila mean strict AUC original -  0.531561847862\n",
      "Dims = 600 Drosophila mean strict AUC feature selection -  0.51873939355\n",
      "\n",
      "\n",
      "Dims = 600 Drosophila mean gen error original -  0.350698335497\n",
      "Dims = 600 Drosophila mean gen error feature selection -  0.352663438809\n",
      "Dims = 600 Drosophila mean gen AUC original -  0.507628510361\n",
      "Dims = 600 Drosophila mean gen AUC feature selection -  0.503696115449\n",
      "\n",
      "\n",
      "Dims = 600 Drosophila mean BE error original -  0.350723470394\n",
      "Dims = 600 Drosophila mean BE error feature selection -  0.35228778626\n",
      "Dims = 600 Drosophila mean BE AUC original -  0.486860508996\n",
      "Dims = 600 Drosophila mean BE AUC feature selection -  0.48583941366\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in dimensions:\n",
    "    drct = 'Results/W2V_Parameter_Search/Dims/'\n",
    "    errors_dros = mult_open(drct, str(i)+'_errors_')\n",
    "    fpr_dros = mult_open(drct, str(i)+'_fpr_')\n",
    "    tpr_dros = mult_open(drct, str(i)+'_tpr_')\n",
    "    \n",
    "    mean_err_dros_strict = []\n",
    "    mean_auc_dros_strict = []\n",
    "    mean_err_dros_gen = []\n",
    "    mean_auc_dros_gen = []\n",
    "    mean_err_dros_be = []\n",
    "    mean_auc_dros_be = []\n",
    "    for e, f, t in zip(errors_dros, fpr_dros, tpr_dros):\n",
    "        input_list = [[e, f, t]]\n",
    "        name_list = ['drosophila']\n",
    "        for idx in range(3):\n",
    "            for item, name in zip(input_list, name_list):\n",
    "                for fpr_item, tpr_item, error_item in zip(item[1][idx], item[2][idx], item[0][idx]):\n",
    "                    roc_auc = auc(fpr_item, tpr_item)\n",
    "                    auc_val = '%.3f' % roc_auc\n",
    "                    error = '%.3f' % error_item\n",
    "                    if idx == 0:\n",
    "                        mean_err_dros_strict.append(error_item)\n",
    "                        mean_auc_dros_strict.append(roc_auc)\n",
    "#                         legend_label = 'Strict error ' + name + ' auc=' + str(auc_val) + ' error=' + str(error)\n",
    "#                         print(legend_label)\n",
    "                    elif idx == 1:\n",
    "                        mean_err_dros_gen.append(error_item)\n",
    "                        mean_auc_dros_gen.append(roc_auc)\n",
    "#                         legend_label = 'Gen error ' + name + ' auc=' + str(auc_val) + ' error=' + str(error)\n",
    "#                         print(legend_label)\n",
    "                    elif idx == 2:\n",
    "                        mean_err_dros_be.append(error_item)\n",
    "                        mean_auc_dros_be.append(roc_auc)\n",
    "#                         legend_label = 'BE error ' + name + ' auc=' + str(auc_val) + ' error=' + str(error)\n",
    "#                         print(legend_label)\n",
    "#                 print('\\n')\n",
    "\n",
    "    mean_err_dros_org_strict = mean_err_dros_strict[0::2]\n",
    "    mean_err_dros_fs_strict = mean_err_dros_strict[1::2]\n",
    "    mean_auc_dros_org_strict = mean_auc_dros_strict[0::2]\n",
    "    mean_auc_dros_fs_strict = mean_auc_dros_strict[1::2]\n",
    "\n",
    "    mean_err_dros_org_gen = mean_err_dros_gen[0::2]\n",
    "    mean_err_dros_fs_gen = mean_err_dros_gen[1::2]\n",
    "    mean_auc_dros_org_gen = mean_auc_dros_gen[0::2]\n",
    "    mean_auc_dros_fs_gen = mean_auc_dros_gen[1::2]\n",
    "\n",
    "    mean_err_dros_org_be = mean_err_dros_be[0::2]\n",
    "    mean_err_dros_fs_be = mean_err_dros_be[1::2]\n",
    "    mean_auc_dros_org_be = mean_auc_dros_be[0::2]\n",
    "    mean_auc_dros_fs_be = mean_auc_dros_be[1::2]\n",
    "    \n",
    "    print('Dims = '+str(i)+' Drosophila mean strict error original - ', np.mean(mean_err_dros_org_strict))\n",
    "    print('Dims = '+str(i)+' Drosophila mean strict error feature selection - ', np.mean(mean_err_dros_fs_strict))\n",
    "    print('Dims = '+str(i)+' Drosophila mean strict AUC original - ', np.mean(mean_auc_dros_org_strict))\n",
    "    print('Dims = '+str(i)+' Drosophila mean strict AUC feature selection - ', np.mean(mean_auc_dros_fs_strict))\n",
    "    print('\\n')\n",
    "    print('Dims = '+str(i)+' Drosophila mean gen error original - ', np.mean(mean_err_dros_org_gen))\n",
    "    print('Dims = '+str(i)+' Drosophila mean gen error feature selection - ', np.mean(mean_err_dros_fs_gen))\n",
    "    print('Dims = '+str(i)+' Drosophila mean gen AUC original - ', np.mean(mean_auc_dros_org_gen))\n",
    "    print('Dims = '+str(i)+' Drosophila mean gen AUC feature selection - ', np.mean(mean_auc_dros_fs_gen))\n",
    "    print('\\n')\n",
    "    print('Dims = '+str(i)+' Drosophila mean BE error original - ', np.mean(mean_err_dros_org_be))\n",
    "    print('Dims = '+str(i)+' Drosophila mean BE error feature selection - ', np.mean(mean_err_dros_fs_be))\n",
    "    print('Dims = '+str(i)+' Drosophila mean BE AUC original - ', np.mean(mean_auc_dros_org_be))\n",
    "    print('Dims = '+str(i)+' Drosophila mean BE AUC feature selection - ', np.mean(mean_auc_dros_fs_be))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "org_100 = 0.48823523874\n",
    "fs_100 = 0.493827666301\n",
    "\n",
    "org_200 = 0.518242149813\n",
    "fs_200 = 0.519622695748\n",
    "\n",
    "org_300 = 0.49458909605\n",
    "fs_300 = 0.495140712247\n",
    "\n",
    "org_400 = 0.530355182715\n",
    "fs_400 = 0.522848523164\n",
    "\n",
    "org_500 = 0.51306193792\n",
    "fs_500 = 0.498997252443\n",
    "\n",
    "org_600 = 0.531561847862\n",
    "fs_600 = 0.51873939355\n",
    "\n",
    "res_list = [org_100, fs_100, org_200, fs_200, org_300, fs_300, org_400, fs_400, org_500, fs_500, org_600, fs_600]\n",
    " \n",
    "print(res_list.index(max(res_list))) #STRICT - ORG 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "org_100 = 0.498002964969\n",
    "fs_100 = 0.504285094557\n",
    "\n",
    "org_200 = 0.506520367903\n",
    "fs_200 = 0.520416510331\n",
    "\n",
    "org_300 = 0.497951234897\n",
    "fs_300 = 0.488438900647\n",
    "\n",
    "org_400 = 0.504174192718\n",
    "fs_400 = 0.501883436351\n",
    "\n",
    "org_500 = 0.495890341956\n",
    "fs_500 = 0.496831193417\n",
    "\n",
    "org_600 = 0.507628510361\n",
    "fs_600 = 0.503696115449\n",
    "\n",
    "res_list = [org_100, fs_100, org_200, fs_200, org_300, fs_300, org_400, fs_400, org_500, fs_500, org_600, fs_600]\n",
    " \n",
    "print(res_list.index(max(res_list))) #GEN - FS 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "org_100 = 0.495656337464\n",
    "fs_100 = 0.492639575514\n",
    "\n",
    "org_200 = 0.49458909605\n",
    "fs_200 = 0.485498559817\n",
    "\n",
    "org_300 = 0.498330663255\n",
    "fs_300 = 0.483447994672\n",
    "\n",
    "org_400 = 0.491053228877\n",
    "fs_400 = 0.495965522422\n",
    "\n",
    "org_500 = 0.498901271933\n",
    "fs_500 = 0.49154261637\n",
    "\n",
    "org_600 = 0.486860508996\n",
    "fs_600 = 0.48583941366\n",
    "\n",
    "res_list = [org_100, fs_100, org_200, fs_200, org_300, fs_300, org_400, fs_400, org_500, fs_500, org_600, fs_600]\n",
    " \n",
    "print(res_list.index(max(res_list))) #BE - ORG 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_word_count = [1, 2, 3, 4, 5, 6]\n",
    "for i in min_word_count:\n",
    "    w2v_parameters = [500, i, 4, 6, 0.001]\n",
    "    strict = pred.make_w2v_model(dros_strict_real, 'dros_strict_min_word_count_' + str(i), w2v_parameters)\n",
    "    gen = pred.make_w2v_model(dros_gen_real, 'dros_gen_min_word_count_' + str(i), w2v_parameters)\n",
    "    be = pred.make_w2v_model(dros_be_real, 'dros_be_min_word_count_' + str(i), w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_name = 'Results/dros_tr_val_split_' + str(144)\n",
    "train_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dros_strict_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
