{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Core-scripts/')\n",
    "\n",
    "from parse_and_prepare import ProteinProteinInteractionClassifier as ppi\n",
    "import file_readers as fr\n",
    "import prediction as pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dros_strict_real = pickle.load(open('../../Results/Drosophila/drosophila_mentions_strict_real.pkl', 'rb'))\n",
    "dros_gen_real = pickle.load(open('../../Results/Drosophila/drosophila_mentions_gen_real.pkl', 'rb'))\n",
    "dros_be_real = pickle.load(open('../../Results/Drosophila/drosophila_mentions_be_real.pkl', 'rb'))\n",
    "random_seeds = [144, 235, 905, 2895, 3462, 4225, 5056, 5192, 7751, 7813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    real_tr_te_name = 'Drosophila/train_test/dros_tr_te_split_' + str(seed)\n",
    "    train_data, b, c, d = pred.manual_train_test_split(dros_strict_real, real_tr_te_name, random_state=seed ,test_set_prop=0.1)\n",
    "    tr_val_name = 'Drosophila/train_val/dros_tr_val_split_' + str(seed)\n",
    "    real_train_data, b, c, d = pred.manual_train_test_split(train_data, tr_val_name, random_state=seed, test_set_prop=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = [100, 200, 300, 400, 500, 600]\n",
    "min_word_count = [1, 2, 3, 4, 5, 6]\n",
    "context_window = [2, 3, 4, 5, 6, 7]\n",
    "downsample = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:01:36,501 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:01:36,502 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:01:36,544 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 18:01:36,545 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:01:36,557 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 18:01:36,559 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n",
      "2017-05-12 18:01:36,567 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 18:01:36,570 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 18:01:36,571 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 18:01:36,572 : INFO : estimated required memory for 3553 words and 100 dimensions: 4618900 bytes\n",
      "2017-05-12 18:01:36,586 : INFO : resetting layer weights\n",
      "2017-05-12 18:01:36,621 : INFO : training model with 4 workers on 3553 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:01:36,624 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:01:37,244 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:01:37,249 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:01:37,250 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:01:37,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:01:37,259 : INFO : training on 943135 raw words (623249 effective words) took 0.6s, 994418 effective words/s\n",
      "2017-05-12 18:01:37,260 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:01:37,283 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_100_model, separately None\n",
      "2017-05-12 18:01:37,284 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:01:37,285 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:01:37,322 : INFO : saved ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_100_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:01:37,836 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:01:37,837 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:01:37,898 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 18:01:37,961 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 18:01:38,028 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:01:38,080 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 18:01:38,080 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:01:38,114 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 18:01:38,115 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 18:01:38,141 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 18:01:38,145 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 18:01:38,146 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 18:01:38,147 : INFO : estimated required memory for 8096 words and 100 dimensions: 10524800 bytes\n",
      "2017-05-12 18:01:38,187 : INFO : resetting layer weights\n",
      "2017-05-12 18:01:38,275 : INFO : training model with 4 workers on 8096 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:01:38,277 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:01:39,288 : INFO : PROGRESS: at 25.61% examples, 820963 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:40,293 : INFO : PROGRESS: at 50.78% examples, 814269 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:41,302 : INFO : PROGRESS: at 76.83% examples, 820663 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:42,211 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:01:42,218 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:01:42,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:01:42,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:01:42,229 : INFO : training on 4523435 raw words (3225235 effective words) took 3.9s, 817176 effective words/s\n",
      "2017-05-12 18:01:42,229 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:01:42,277 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_100_model, separately None\n",
      "2017-05-12 18:01:42,278 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:01:42,278 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:01:42,378 : INFO : saved ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_100_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:01:45,234 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:01:45,237 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:01:45,305 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 18:01:45,376 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:01:45,447 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 18:01:45,516 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 18:01:45,580 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 18:01:45,643 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 18:01:45,705 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 18:01:45,764 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 18:01:45,829 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 18:01:45,890 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 18:01:45,947 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 18:01:46,014 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 18:01:46,081 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 18:01:46,159 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 18:01:46,227 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 18:01:46,291 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 18:01:46,356 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 18:01:46,416 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 18:01:46,480 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 18:01:46,513 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 18:01:46,514 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:01:46,580 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 18:01:46,581 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 18:01:46,632 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 18:01:46,641 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 18:01:46,641 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 18:01:46,642 : INFO : estimated required memory for 18056 words and 100 dimensions: 23472800 bytes\n",
      "2017-05-12 18:01:46,730 : INFO : resetting layer weights\n",
      "2017-05-12 18:01:46,919 : INFO : training model with 4 workers on 18056 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:01:46,922 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:01:47,934 : INFO : PROGRESS: at 4.89% examples, 785368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:48,951 : INFO : PROGRESS: at 9.69% examples, 773557 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:01:49,958 : INFO : PROGRESS: at 14.59% examples, 777157 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:01:50,959 : INFO : PROGRESS: at 19.41% examples, 776423 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:51,965 : INFO : PROGRESS: at 24.34% examples, 779674 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:52,968 : INFO : PROGRESS: at 29.38% examples, 784581 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:53,971 : INFO : PROGRESS: at 34.18% examples, 782797 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:01:54,981 : INFO : PROGRESS: at 39.14% examples, 783677 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:55,988 : INFO : PROGRESS: at 44.21% examples, 786933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:56,997 : INFO : PROGRESS: at 49.19% examples, 788006 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:57,999 : INFO : PROGRESS: at 53.95% examples, 786038 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:01:59,013 : INFO : PROGRESS: at 58.72% examples, 783537 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:00,024 : INFO : PROGRESS: at 63.66% examples, 783922 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:01,026 : INFO : PROGRESS: at 68.51% examples, 783679 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:02,036 : INFO : PROGRESS: at 73.54% examples, 785002 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:02:03,045 : INFO : PROGRESS: at 78.50% examples, 785295 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:02:04,057 : INFO : PROGRESS: at 82.39% examples, 775511 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:05,074 : INFO : PROGRESS: at 85.95% examples, 763763 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:06,085 : INFO : PROGRESS: at 89.61% examples, 754267 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:07,088 : INFO : PROGRESS: at 93.27% examples, 746018 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:08,101 : INFO : PROGRESS: at 98.04% examples, 746560 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:08,489 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:02:08,490 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:02:08,492 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:02:08,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:02:08,498 : INFO : training on 21819630 raw words (16121760 effective words) took 21.6s, 747435 effective words/s\n",
      "2017-05-12 18:02:08,498 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:02:08,608 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_be_dim_parameter_100_model, separately None\n",
      "2017-05-12 18:02:08,609 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:02:08,611 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:02:08,808 : INFO : saved ../../Results/Drosophila/models/dims/dros_be_dim_parameter_100_model\n",
      "2017-05-12 18:02:09,046 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:02:09,048 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:02:09,092 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 18:02:09,093 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:02:09,114 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 18:02:09,114 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n",
      "2017-05-12 18:02:09,125 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 18:02:09,126 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 18:02:09,127 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 18:02:09,128 : INFO : estimated required memory for 3553 words and 200 dimensions: 7461300 bytes\n",
      "2017-05-12 18:02:09,144 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:09,191 : INFO : training model with 4 workers on 3553 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:02:09,192 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:02:09,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:02:09,930 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:02:09,935 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:02:09,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:02:09,939 : INFO : training on 943135 raw words (623300 effective words) took 0.7s, 840949 effective words/s\n",
      "2017-05-12 18:02:09,939 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:02:09,962 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_200_model, separately None\n",
      "2017-05-12 18:02:09,963 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:02:09,965 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:02:10,021 : INFO : saved ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_200_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:10,541 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:02:10,542 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:02:10,601 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 18:02:10,659 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 18:02:10,717 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:10,765 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 18:02:10,767 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:02:10,803 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 18:02:10,805 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 18:02:10,828 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 18:02:10,833 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 18:02:10,834 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 18:02:10,835 : INFO : estimated required memory for 8096 words and 200 dimensions: 17001600 bytes\n",
      "2017-05-12 18:02:10,875 : INFO : resetting layer weights\n",
      "2017-05-12 18:02:10,983 : INFO : training model with 4 workers on 8096 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:02:10,985 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:02:12,005 : INFO : PROGRESS: at 21.19% examples, 673915 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:13,006 : INFO : PROGRESS: at 42.16% examples, 674534 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:14,009 : INFO : PROGRESS: at 63.14% examples, 674466 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:15,028 : INFO : PROGRESS: at 85.21% examples, 680386 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:15,814 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:02:15,820 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:02:15,838 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:02:15,847 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:02:15,848 : INFO : training on 4523435 raw words (3224844 effective words) took 4.9s, 663923 effective words/s\n",
      "2017-05-12 18:02:15,849 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:02:15,930 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_200_model, separately None\n",
      "2017-05-12 18:02:15,932 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:02:15,933 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:02:16,135 : INFO : saved ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_200_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:20,336 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:02:20,338 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:02:20,399 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 18:02:20,463 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 18:02:20,527 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:20,580 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 18:02:20,636 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 18:02:20,693 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 18:02:20,749 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 18:02:20,803 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 18:02:20,861 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 18:02:20,919 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 18:02:20,981 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 18:02:21,044 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 18:02:21,104 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 18:02:21,166 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 18:02:21,232 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 18:02:21,301 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 18:02:21,372 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 18:02:21,440 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 18:02:21,506 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 18:02:21,545 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 18:02:21,546 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:02:21,618 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 18:02:21,619 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 18:02:21,670 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 18:02:21,675 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 18:02:21,676 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 18:02:21,677 : INFO : estimated required memory for 18056 words and 200 dimensions: 37917600 bytes\n",
      "2017-05-12 18:02:21,767 : INFO : resetting layer weights\n",
      "2017-05-12 18:02:21,983 : INFO : training model with 4 workers on 18056 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:02:21,983 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:02:23,014 : INFO : PROGRESS: at 3.38% examples, 532808 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:02:24,041 : INFO : PROGRESS: at 6.90% examples, 542878 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 18:02:25,055 : INFO : PROGRESS: at 10.47% examples, 551033 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:26,059 : INFO : PROGRESS: at 14.04% examples, 556604 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:27,060 : INFO : PROGRESS: at 17.66% examples, 561717 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:28,062 : INFO : PROGRESS: at 21.24% examples, 563843 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:29,065 : INFO : PROGRESS: at 24.62% examples, 561126 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:30,072 : INFO : PROGRESS: at 27.41% examples, 546905 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:31,086 : INFO : PROGRESS: at 30.25% examples, 536210 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:32,098 : INFO : PROGRESS: at 33.08% examples, 527807 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:33,115 : INFO : PROGRESS: at 35.87% examples, 520045 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:34,118 : INFO : PROGRESS: at 39.41% examples, 523904 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:35,121 : INFO : PROGRESS: at 42.75% examples, 524924 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 18:02:36,143 : INFO : PROGRESS: at 46.08% examples, 525048 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:37,146 : INFO : PROGRESS: at 49.33% examples, 524901 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:38,151 : INFO : PROGRESS: at 52.81% examples, 526946 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:39,159 : INFO : PROGRESS: at 56.01% examples, 526107 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:02:40,164 : INFO : PROGRESS: at 59.28% examples, 525846 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:02:41,212 : INFO : PROGRESS: at 62.34% examples, 522892 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:42,239 : INFO : PROGRESS: at 65.67% examples, 522985 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:43,239 : INFO : PROGRESS: at 69.11% examples, 524406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:44,249 : INFO : PROGRESS: at 71.85% examples, 520509 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:45,250 : INFO : PROGRESS: at 74.45% examples, 516170 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 18:02:46,258 : INFO : PROGRESS: at 77.16% examples, 512681 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:47,261 : INFO : PROGRESS: at 79.78% examples, 508982 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:48,282 : INFO : PROGRESS: at 82.29% examples, 504662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:49,283 : INFO : PROGRESS: at 84.85% examples, 501293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:50,308 : INFO : PROGRESS: at 87.32% examples, 497202 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:02:51,326 : INFO : PROGRESS: at 90.35% examples, 496551 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:02:52,339 : INFO : PROGRESS: at 93.82% examples, 498463 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:02:53,357 : INFO : PROGRESS: at 96.57% examples, 496414 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 18:02:54,363 : INFO : PROGRESS: at 99.69% examples, 496484 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 18:02:54,395 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:02:54,412 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:02:54,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:02:54,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:02:54,423 : INFO : training on 21819630 raw words (16122628 effective words) took 32.4s, 497088 effective words/s\n",
      "2017-05-12 18:02:54,424 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:02:54,524 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_be_dim_parameter_200_model, separately None\n",
      "2017-05-12 18:02:54,525 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:02:54,526 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:02:54,841 : INFO : saved ../../Results/Drosophila/models/dims/dros_be_dim_parameter_200_model\n",
      "2017-05-12 18:02:55,098 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:02:55,100 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:02:55,145 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 18:02:55,146 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:02:55,171 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 18:02:55,173 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n",
      "2017-05-12 18:02:55,183 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 18:02:55,186 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 18:02:55,187 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 18:02:55,188 : INFO : estimated required memory for 3553 words and 300 dimensions: 10303700 bytes\n",
      "2017-05-12 18:02:55,202 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:55,253 : INFO : training model with 4 workers on 3553 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:02:55,254 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:02:56,262 : INFO : PROGRESS: at 87.88% examples, 546453 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:02:56,346 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:02:56,361 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:02:56,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:02:56,364 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:02:56,366 : INFO : training on 943135 raw words (623620 effective words) took 1.1s, 563638 effective words/s\n",
      "2017-05-12 18:02:56,368 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:02:56,396 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_300_model, separately None\n",
      "2017-05-12 18:02:56,397 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:02:56,398 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:02:56,487 : INFO : saved ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_300_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:57,052 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:02:57,055 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:02:57,128 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 18:02:57,191 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 18:02:57,249 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:02:57,300 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 18:02:57,303 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:02:57,340 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 18:02:57,341 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 18:02:57,367 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 18:02:57,371 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 18:02:57,374 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 18:02:57,375 : INFO : estimated required memory for 8096 words and 300 dimensions: 23478400 bytes\n",
      "2017-05-12 18:02:57,415 : INFO : resetting layer weights\n",
      "2017-05-12 18:02:57,536 : INFO : training model with 4 workers on 8096 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:02:57,539 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:02:58,591 : INFO : PROGRESS: at 14.81% examples, 456288 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:02:59,604 : INFO : PROGRESS: at 30.26% examples, 473875 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:00,646 : INFO : PROGRESS: at 42.39% examples, 440704 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:01,647 : INFO : PROGRESS: at 54.32% examples, 426982 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:03:02,666 : INFO : PROGRESS: at 66.23% examples, 417003 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:03,681 : INFO : PROGRESS: at 78.61% examples, 413040 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:04,709 : INFO : PROGRESS: at 93.61% examples, 421408 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 18:03:05,086 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:03:05,088 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:03:05,106 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:03:05,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:03:05,117 : INFO : training on 4523435 raw words (3224724 effective words) took 7.6s, 425926 effective words/s\n",
      "2017-05-12 18:03:05,118 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:03:05,165 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_300_model, separately None\n",
      "2017-05-12 18:03:05,165 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:03:05,166 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:03:05,364 : INFO : saved ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_300_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:03:08,011 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:03:08,012 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:03:08,073 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 18:03:08,141 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 18:03:08,210 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:03:08,275 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 18:03:08,348 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 18:03:08,412 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 18:03:08,494 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 18:03:08,562 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 18:03:08,640 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 18:03:08,710 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 18:03:08,782 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 18:03:08,847 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 18:03:08,920 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 18:03:08,987 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 18:03:09,060 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 18:03:09,144 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 18:03:09,217 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 18:03:09,287 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 18:03:09,356 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 18:03:09,394 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 18:03:09,394 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:03:09,465 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 18:03:09,468 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 18:03:09,524 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 18:03:09,527 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 18:03:09,528 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 18:03:09,529 : INFO : estimated required memory for 18056 words and 300 dimensions: 52362400 bytes\n",
      "2017-05-12 18:03:09,629 : INFO : resetting layer weights\n",
      "2017-05-12 18:03:09,890 : INFO : training model with 4 workers on 18056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:03:09,891 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:03:10,933 : INFO : PROGRESS: at 2.79% examples, 434751 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:11,947 : INFO : PROGRESS: at 5.48% examples, 431873 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:12,951 : INFO : PROGRESS: at 8.18% examples, 432463 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:13,969 : INFO : PROGRESS: at 10.98% examples, 434824 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:14,997 : INFO : PROGRESS: at 13.58% examples, 429620 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:03:16,035 : INFO : PROGRESS: at 16.00% examples, 420706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:17,044 : INFO : PROGRESS: at 18.40% examples, 415139 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:18,045 : INFO : PROGRESS: at 20.65% examples, 408492 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:03:19,063 : INFO : PROGRESS: at 23.11% examples, 406642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:20,070 : INFO : PROGRESS: at 25.76% examples, 408426 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:21,071 : INFO : PROGRESS: at 28.46% examples, 410761 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:22,080 : INFO : PROGRESS: at 31.12% examples, 411903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:23,092 : INFO : PROGRESS: at 33.72% examples, 412201 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:24,101 : INFO : PROGRESS: at 36.47% examples, 414114 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:25,129 : INFO : PROGRESS: at 38.95% examples, 412346 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:26,131 : INFO : PROGRESS: at 41.70% examples, 414185 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:27,134 : INFO : PROGRESS: at 44.07% examples, 412326 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:28,146 : INFO : PROGRESS: at 46.77% examples, 413308 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:29,168 : INFO : PROGRESS: at 49.52% examples, 414370 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:30,182 : INFO : PROGRESS: at 52.31% examples, 415853 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:31,185 : INFO : PROGRESS: at 54.96% examples, 416388 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:32,202 : INFO : PROGRESS: at 57.71% examples, 417242 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:33,208 : INFO : PROGRESS: at 60.51% examples, 418532 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:34,215 : INFO : PROGRESS: at 63.20% examples, 419134 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:35,217 : INFO : PROGRESS: at 65.86% examples, 419436 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:36,222 : INFO : PROGRESS: at 68.65% examples, 420522 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:37,228 : INFO : PROGRESS: at 71.39% examples, 421256 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:38,229 : INFO : PROGRESS: at 74.04% examples, 421482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:39,243 : INFO : PROGRESS: at 76.84% examples, 422268 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:03:40,248 : INFO : PROGRESS: at 79.64% examples, 423110 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:41,274 : INFO : PROGRESS: at 82.29% examples, 422913 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:42,296 : INFO : PROGRESS: at 85.26% examples, 424380 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:43,312 : INFO : PROGRESS: at 88.01% examples, 424718 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:44,335 : INFO : PROGRESS: at 90.85% examples, 425396 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:03:45,343 : INFO : PROGRESS: at 93.73% examples, 426411 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:03:46,348 : INFO : PROGRESS: at 96.38% examples, 426403 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:47,348 : INFO : PROGRESS: at 99.01% examples, 426250 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:47,676 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:03:47,692 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:03:47,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:03:47,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:03:47,704 : INFO : training on 21819630 raw words (16123450 effective words) took 37.8s, 426464 effective words/s\n",
      "2017-05-12 18:03:47,705 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:03:47,831 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_be_dim_parameter_300_model, separately None\n",
      "2017-05-12 18:03:47,833 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:03:47,833 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:03:48,327 : INFO : saved ../../Results/Drosophila/models/dims/dros_be_dim_parameter_300_model\n",
      "2017-05-12 18:03:48,627 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:03:48,629 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:03:48,682 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 18:03:48,683 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:03:48,703 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 18:03:48,703 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n",
      "2017-05-12 18:03:48,716 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 18:03:48,717 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 18:03:48,718 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 18:03:48,719 : INFO : estimated required memory for 3553 words and 400 dimensions: 13146100 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:03:48,734 : INFO : resetting layer weights\n",
      "2017-05-12 18:03:48,797 : INFO : training model with 4 workers on 3553 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:03:48,799 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:03:49,817 : INFO : PROGRESS: at 67.77% examples, 417777 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:03:50,235 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:03:50,244 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:03:50,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:03:50,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:03:50,268 : INFO : training on 943135 raw words (623835 effective words) took 1.5s, 426426 effective words/s\n",
      "2017-05-12 18:03:50,269 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:03:50,297 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_400_model, separately None\n",
      "2017-05-12 18:03:50,298 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:03:50,299 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:03:50,426 : INFO : saved ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_400_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:03:51,281 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:03:51,282 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:03:51,345 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 18:03:51,413 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 18:03:51,478 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:03:51,536 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 18:03:51,536 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:03:51,577 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 18:03:51,577 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 18:03:51,606 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 18:03:51,608 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 18:03:51,609 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 18:03:51,610 : INFO : estimated required memory for 8096 words and 400 dimensions: 29955200 bytes\n",
      "2017-05-12 18:03:51,654 : INFO : resetting layer weights\n",
      "2017-05-12 18:03:51,804 : INFO : training model with 4 workers on 8096 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:03:51,805 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:03:52,846 : INFO : PROGRESS: at 10.61% examples, 330298 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:03:53,861 : INFO : PROGRESS: at 21.84% examples, 343878 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:54,894 : INFO : PROGRESS: at 33.12% examples, 346343 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:55,898 : INFO : PROGRESS: at 44.15% examples, 348343 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:56,912 : INFO : PROGRESS: at 54.54% examples, 344785 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:03:57,915 : INFO : PROGRESS: at 65.56% examples, 346434 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:58,928 : INFO : PROGRESS: at 75.96% examples, 344225 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:03:59,933 : INFO : PROGRESS: at 86.55% examples, 343647 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:00,951 : INFO : PROGRESS: at 96.93% examples, 342009 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:01,190 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:04:01,206 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:04:01,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:04:01,247 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:04:01,251 : INFO : training on 4523435 raw words (3225337 effective words) took 9.4s, 341681 effective words/s\n",
      "2017-05-12 18:04:01,254 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:04:01,320 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_400_model, separately None\n",
      "2017-05-12 18:04:01,323 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:04:01,324 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:04:01,619 : INFO : saved ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_400_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:04:04,695 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:04:04,697 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:04:04,759 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 18:04:04,823 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:04:04,898 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 18:04:04,965 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 18:04:05,034 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 18:04:05,102 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 18:04:05,163 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 18:04:05,225 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 18:04:05,284 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 18:04:05,343 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 18:04:05,403 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 18:04:05,461 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 18:04:05,524 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 18:04:05,585 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 18:04:05,647 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 18:04:05,707 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 18:04:05,770 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 18:04:05,833 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 18:04:05,901 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 18:04:05,934 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 18:04:05,935 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:04:06,004 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 18:04:06,005 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 18:04:06,064 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 18:04:06,069 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 18:04:06,070 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 18:04:06,071 : INFO : estimated required memory for 18056 words and 400 dimensions: 66807200 bytes\n",
      "2017-05-12 18:04:06,185 : INFO : resetting layer weights\n",
      "2017-05-12 18:04:06,483 : INFO : training model with 4 workers on 18056 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:04:06,484 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:04:07,493 : INFO : PROGRESS: at 2.05% examples, 331234 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:08,495 : INFO : PROGRESS: at 4.15% examples, 334659 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:04:09,499 : INFO : PROGRESS: at 6.35% examples, 340919 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:10,525 : INFO : PROGRESS: at 8.41% examples, 336617 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:04:11,526 : INFO : PROGRESS: at 10.47% examples, 335628 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:12,543 : INFO : PROGRESS: at 12.63% examples, 336626 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:04:13,550 : INFO : PROGRESS: at 14.77% examples, 337744 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:14,586 : INFO : PROGRESS: at 16.93% examples, 337400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:15,610 : INFO : PROGRESS: at 19.00% examples, 335926 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:04:16,642 : INFO : PROGRESS: at 21.15% examples, 335907 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:17,663 : INFO : PROGRESS: at 23.34% examples, 336908 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:04:18,712 : INFO : PROGRESS: at 25.53% examples, 336969 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:19,723 : INFO : PROGRESS: at 27.72% examples, 338004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:20,758 : INFO : PROGRESS: at 29.83% examples, 337259 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:21,761 : INFO : PROGRESS: at 31.89% examples, 336865 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:22,764 : INFO : PROGRESS: at 34.04% examples, 337421 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:23,777 : INFO : PROGRESS: at 36.15% examples, 337295 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:24,784 : INFO : PROGRESS: at 38.31% examples, 337697 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:25,829 : INFO : PROGRESS: at 40.42% examples, 336978 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:26,852 : INFO : PROGRESS: at 42.52% examples, 336723 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:27,873 : INFO : PROGRESS: at 44.67% examples, 336849 words/s, in_qsize 8, out_qsize 2\n",
      "2017-05-12 18:04:28,882 : INFO : PROGRESS: at 46.81% examples, 337184 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:29,901 : INFO : PROGRESS: at 48.88% examples, 336686 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:30,916 : INFO : PROGRESS: at 50.85% examples, 335698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:31,939 : INFO : PROGRESS: at 52.49% examples, 332632 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:32,941 : INFO : PROGRESS: at 54.18% examples, 330359 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:33,967 : INFO : PROGRESS: at 55.87% examples, 327958 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:34,990 : INFO : PROGRESS: at 57.76% examples, 326801 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:36,007 : INFO : PROGRESS: at 60.01% examples, 327796 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:37,023 : INFO : PROGRESS: at 62.16% examples, 328247 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:38,028 : INFO : PROGRESS: at 64.30% examples, 328784 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:39,045 : INFO : PROGRESS: at 66.50% examples, 329397 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:40,081 : INFO : PROGRESS: at 68.69% examples, 329775 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:41,091 : INFO : PROGRESS: at 70.94% examples, 330599 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:42,098 : INFO : PROGRESS: at 73.18% examples, 331406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:43,105 : INFO : PROGRESS: at 75.23% examples, 331365 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:44,125 : INFO : PROGRESS: at 77.21% examples, 330817 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:04:45,149 : INFO : PROGRESS: at 79.10% examples, 329868 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:04:46,187 : INFO : PROGRESS: at 80.97% examples, 328865 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:47,192 : INFO : PROGRESS: at 82.89% examples, 328364 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:04:48,219 : INFO : PROGRESS: at 85.03% examples, 328580 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:49,227 : INFO : PROGRESS: at 87.09% examples, 328606 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 18:04:50,264 : INFO : PROGRESS: at 89.20% examples, 328561 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:51,278 : INFO : PROGRESS: at 91.31% examples, 328704 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:52,281 : INFO : PROGRESS: at 93.32% examples, 328597 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:04:53,302 : INFO : PROGRESS: at 95.28% examples, 328213 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:54,346 : INFO : PROGRESS: at 97.49% examples, 328454 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:04:55,354 : INFO : PROGRESS: at 99.56% examples, 328471 words/s, in_qsize 8, out_qsize 2\n",
      "2017-05-12 18:04:55,510 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:04:55,520 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:04:55,533 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:04:55,543 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:04:55,545 : INFO : training on 21819630 raw words (16121788 effective words) took 49.1s, 328648 effective words/s\n",
      "2017-05-12 18:04:55,548 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:04:55,697 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_be_dim_parameter_400_model, separately None\n",
      "2017-05-12 18:04:55,698 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:04:55,699 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:04:56,377 : INFO : saved ../../Results/Drosophila/models/dims/dros_be_dim_parameter_400_model\n",
      "2017-05-12 18:04:56,674 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:04:56,676 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:04:56,734 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 18:04:56,736 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:04:56,761 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 18:04:56,762 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n",
      "2017-05-12 18:04:56,774 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 18:04:56,776 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 18:04:56,777 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 18:04:56,778 : INFO : estimated required memory for 3553 words and 500 dimensions: 15988500 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:04:56,795 : INFO : resetting layer weights\n",
      "2017-05-12 18:04:56,863 : INFO : training model with 4 workers on 3553 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:04:56,864 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:04:57,879 : INFO : PROGRESS: at 49.77% examples, 307987 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:04:58,784 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:04:58,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:04:58,816 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:04:58,826 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:04:58,828 : INFO : training on 943135 raw words (623541 effective words) took 2.0s, 318624 effective words/s\n",
      "2017-05-12 18:04:58,828 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:04:58,858 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_500_model, separately None\n",
      "2017-05-12 18:04:58,861 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:04:58,862 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:04:59,013 : INFO : saved ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_500_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:04:59,645 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:04:59,646 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:04:59,718 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 18:04:59,785 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:04:59,880 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 18:04:59,964 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 18:04:59,964 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:05:00,004 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 18:05:00,005 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 18:05:00,030 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 18:05:00,033 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 18:05:00,034 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 18:05:00,035 : INFO : estimated required memory for 8096 words and 500 dimensions: 36432000 bytes\n",
      "2017-05-12 18:05:00,078 : INFO : resetting layer weights\n",
      "2017-05-12 18:05:00,219 : INFO : training model with 4 workers on 8096 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:05:00,220 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:05:01,241 : INFO : PROGRESS: at 9.07% examples, 289114 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 18:05:02,243 : INFO : PROGRESS: at 18.55% examples, 297592 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:03,248 : INFO : PROGRESS: at 28.29% examples, 302309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:04,280 : INFO : PROGRESS: at 37.77% examples, 300859 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:05,297 : INFO : PROGRESS: at 47.48% examples, 302239 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:06,313 : INFO : PROGRESS: at 55.87% examples, 296240 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:07,320 : INFO : PROGRESS: at 64.90% examples, 295304 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:08,330 : INFO : PROGRESS: at 74.41% examples, 296340 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:09,348 : INFO : PROGRESS: at 83.67% examples, 296022 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:10,409 : INFO : PROGRESS: at 92.50% examples, 293182 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:05:11,348 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:05:11,380 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:05:11,428 : INFO : PROGRESS: at 99.78% examples, 287478 words/s, in_qsize 1, out_qsize 1\n",
      "2017-05-12 18:05:11,434 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:05:11,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:05:11,440 : INFO : training on 4523435 raw words (3225104 effective words) took 11.2s, 287792 effective words/s\n",
      "2017-05-12 18:05:11,443 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:05:11,592 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_500_model, separately None\n",
      "2017-05-12 18:05:11,596 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:05:11,597 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:05:12,171 : INFO : saved ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_500_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:05:16,129 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:05:16,133 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:05:16,194 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 18:05:16,261 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:05:16,337 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 18:05:16,396 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 18:05:16,464 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 18:05:16,537 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 18:05:16,598 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 18:05:16,662 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 18:05:16,737 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 18:05:16,814 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 18:05:16,888 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 18:05:16,958 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 18:05:17,027 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 18:05:17,096 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 18:05:17,179 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 18:05:17,252 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 18:05:17,323 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 18:05:17,388 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 18:05:17,460 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 18:05:17,496 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 18:05:17,498 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:05:17,567 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 18:05:17,568 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 18:05:17,639 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 18:05:17,642 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 18:05:17,643 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 18:05:17,644 : INFO : estimated required memory for 18056 words and 500 dimensions: 81252000 bytes\n",
      "2017-05-12 18:05:17,740 : INFO : resetting layer weights\n",
      "2017-05-12 18:05:18,031 : INFO : training model with 4 workers on 18056 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:05:18,034 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:05:19,043 : INFO : PROGRESS: at 1.42% examples, 228516 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:20,112 : INFO : PROGRESS: at 3.10% examples, 242590 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:21,144 : INFO : PROGRESS: at 4.79% examples, 249732 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:22,161 : INFO : PROGRESS: at 6.40% examples, 250835 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:05:23,190 : INFO : PROGRESS: at 7.95% examples, 249403 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:05:24,196 : INFO : PROGRESS: at 9.56% examples, 250568 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:25,247 : INFO : PROGRESS: at 11.11% examples, 248834 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:26,294 : INFO : PROGRESS: at 12.67% examples, 247678 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:27,311 : INFO : PROGRESS: at 14.13% examples, 245996 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:28,369 : INFO : PROGRESS: at 15.55% examples, 242926 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:29,399 : INFO : PROGRESS: at 17.06% examples, 242324 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:30,456 : INFO : PROGRESS: at 18.67% examples, 242492 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:31,479 : INFO : PROGRESS: at 20.19% examples, 242171 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:32,542 : INFO : PROGRESS: at 21.79% examples, 242242 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:33,561 : INFO : PROGRESS: at 23.52% examples, 244422 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:34,574 : INFO : PROGRESS: at 25.12% examples, 245070 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:05:35,608 : INFO : PROGRESS: at 26.76% examples, 245786 words/s, in_qsize 8, out_qsize 2\n",
      "2017-05-12 18:05:36,630 : INFO : PROGRESS: at 28.46% examples, 246941 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:37,638 : INFO : PROGRESS: at 30.16% examples, 248174 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:38,672 : INFO : PROGRESS: at 31.94% examples, 249691 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:39,730 : INFO : PROGRESS: at 33.72% examples, 250787 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:40,748 : INFO : PROGRESS: at 35.46% examples, 251884 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:41,767 : INFO : PROGRESS: at 36.88% examples, 250708 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:42,787 : INFO : PROGRESS: at 38.36% examples, 249917 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:05:43,806 : INFO : PROGRESS: at 39.82% examples, 249188 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:44,818 : INFO : PROGRESS: at 41.33% examples, 248860 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:45,820 : INFO : PROGRESS: at 42.84% examples, 248646 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 18:05:46,824 : INFO : PROGRESS: at 44.57% examples, 249706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:47,873 : INFO : PROGRESS: at 46.22% examples, 249833 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:05:48,894 : INFO : PROGRESS: at 47.82% examples, 249938 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:49,929 : INFO : PROGRESS: at 49.42% examples, 249913 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:50,966 : INFO : PROGRESS: at 51.17% examples, 250568 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:51,986 : INFO : PROGRESS: at 52.81% examples, 250862 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:52,989 : INFO : PROGRESS: at 54.45% examples, 251268 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:54,062 : INFO : PROGRESS: at 56.24% examples, 251774 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:05:55,063 : INFO : PROGRESS: at 57.90% examples, 252137 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:56,076 : INFO : PROGRESS: at 59.64% examples, 252795 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:57,097 : INFO : PROGRESS: at 61.24% examples, 252808 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:58,113 : INFO : PROGRESS: at 62.88% examples, 253033 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:05:59,113 : INFO : PROGRESS: at 64.53% examples, 253334 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:00,151 : INFO : PROGRESS: at 66.22% examples, 253579 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:01,160 : INFO : PROGRESS: at 67.87% examples, 253809 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:02,208 : INFO : PROGRESS: at 69.56% examples, 253971 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:06:03,221 : INFO : PROGRESS: at 71.26% examples, 254321 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:04,246 : INFO : PROGRESS: at 72.86% examples, 254268 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:05,246 : INFO : PROGRESS: at 74.50% examples, 254510 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:06,270 : INFO : PROGRESS: at 76.11% examples, 254458 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:06:07,294 : INFO : PROGRESS: at 77.67% examples, 254261 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:08,317 : INFO : PROGRESS: at 79.19% examples, 253929 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:09,332 : INFO : PROGRESS: at 80.79% examples, 253944 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:10,351 : INFO : PROGRESS: at 82.25% examples, 253512 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:11,403 : INFO : PROGRESS: at 83.94% examples, 253629 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:12,465 : INFO : PROGRESS: at 85.58% examples, 253555 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:13,493 : INFO : PROGRESS: at 87.23% examples, 253650 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:06:14,545 : INFO : PROGRESS: at 88.97% examples, 253891 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 18:06:15,603 : INFO : PROGRESS: at 90.76% examples, 254229 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:16,625 : INFO : PROGRESS: at 92.45% examples, 254447 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:17,628 : INFO : PROGRESS: at 94.14% examples, 254752 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:18,694 : INFO : PROGRESS: at 95.83% examples, 254771 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:06:19,710 : INFO : PROGRESS: at 97.44% examples, 254765 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:20,728 : INFO : PROGRESS: at 99.05% examples, 254745 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:06:21,203 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:06:21,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:06:21,227 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:06:21,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:06:21,238 : INFO : training on 21819630 raw words (16121893 effective words) took 63.2s, 255101 effective words/s\n",
      "2017-05-12 18:06:21,239 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:06:21,386 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_be_dim_parameter_500_model, separately None\n",
      "2017-05-12 18:06:21,389 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:06:21,390 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:06:22,218 : INFO : saved ../../Results/Drosophila/models/dims/dros_be_dim_parameter_500_model\n",
      "2017-05-12 18:06:22,522 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:06:22,523 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:06:22,582 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 18:06:22,583 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:06:22,603 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 18:06:22,603 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:06:22,616 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 18:06:22,617 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 18:06:22,618 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 18:06:22,618 : INFO : estimated required memory for 3553 words and 600 dimensions: 18830900 bytes\n",
      "2017-05-12 18:06:22,637 : INFO : resetting layer weights\n",
      "2017-05-12 18:06:22,707 : INFO : training model with 4 workers on 3553 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:06:22,709 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:06:23,721 : INFO : PROGRESS: at 41.29% examples, 256304 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:24,752 : INFO : PROGRESS: at 86.80% examples, 265863 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:06:24,953 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:06:24,977 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:06:24,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:06:24,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:06:24,997 : INFO : training on 943135 raw words (623325 effective words) took 2.3s, 273282 effective words/s\n",
      "2017-05-12 18:06:24,998 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:06:25,030 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_600_model, separately None\n",
      "2017-05-12 18:06:25,031 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:06:25,031 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:06:25,214 : INFO : saved ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_600_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:06:25,783 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:06:25,784 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:06:25,848 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 18:06:25,913 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 18:06:25,979 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:06:26,036 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 18:06:26,037 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:06:26,076 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 18:06:26,077 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 18:06:26,102 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 18:06:26,104 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 18:06:26,110 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 18:06:26,111 : INFO : estimated required memory for 8096 words and 600 dimensions: 42908800 bytes\n",
      "2017-05-12 18:06:26,153 : INFO : resetting layer weights\n",
      "2017-05-12 18:06:26,310 : INFO : training model with 4 workers on 8096 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:06:26,311 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:06:27,328 : INFO : PROGRESS: at 6.41% examples, 204190 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:28,346 : INFO : PROGRESS: at 13.47% examples, 214101 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:29,359 : INFO : PROGRESS: at 20.53% examples, 217643 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:30,390 : INFO : PROGRESS: at 27.61% examples, 218409 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:31,431 : INFO : PROGRESS: at 33.78% examples, 213013 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:32,433 : INFO : PROGRESS: at 39.74% examples, 209580 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:33,468 : INFO : PROGRESS: at 45.92% examples, 207060 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:34,477 : INFO : PROGRESS: at 51.89% examples, 205000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:35,499 : INFO : PROGRESS: at 57.86% examples, 203119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:36,546 : INFO : PROGRESS: at 64.46% examples, 203177 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:37,562 : INFO : PROGRESS: at 71.52% examples, 205109 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:38,591 : INFO : PROGRESS: at 77.95% examples, 204743 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:39,634 : INFO : PROGRESS: at 84.77% examples, 205236 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:40,651 : INFO : PROGRESS: at 91.85% examples, 206568 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:41,672 : INFO : PROGRESS: at 98.69% examples, 207222 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-12 18:06:41,794 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:06:41,811 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:06:41,832 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:06:41,850 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:06:41,851 : INFO : training on 4523435 raw words (3223961 effective words) took 15.5s, 207553 effective words/s\n",
      "2017-05-12 18:06:41,852 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:06:41,935 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_600_model, separately None\n",
      "2017-05-12 18:06:41,936 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:06:41,937 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:06:42,370 : INFO : saved ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_600_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:06:45,496 : INFO : collecting all words and their counts\n",
      "2017-05-12 18:06:45,497 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 18:06:45,563 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 18:06:45,637 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:06:45,720 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 18:06:45,823 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 18:06:45,914 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 18:06:46,006 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 18:06:46,073 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 18:06:46,140 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 18:06:46,209 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 18:06:46,273 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 18:06:46,333 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 18:06:46,397 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 18:06:46,462 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 18:06:46,528 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 18:06:46,597 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 18:06:46,673 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 18:06:46,744 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 18:06:46,813 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 18:06:46,885 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 18:06:46,923 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 18:06:46,924 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 18:06:46,998 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 18:06:47,003 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 18:06:47,060 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 18:06:47,063 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 18:06:47,063 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 18:06:47,064 : INFO : estimated required memory for 18056 words and 600 dimensions: 95696800 bytes\n",
      "2017-05-12 18:06:47,164 : INFO : resetting layer weights\n",
      "2017-05-12 18:06:47,507 : INFO : training model with 4 workers on 18056 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 18:06:47,508 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 18:06:48,572 : INFO : PROGRESS: at 1.19% examples, 181365 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:49,580 : INFO : PROGRESS: at 2.51% examples, 196449 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:50,609 : INFO : PROGRESS: at 3.88% examples, 202657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:51,678 : INFO : PROGRESS: at 5.25% examples, 203805 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:52,685 : INFO : PROGRESS: at 6.53% examples, 204093 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:06:53,709 : INFO : PROGRESS: at 7.81% examples, 203688 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:54,756 : INFO : PROGRESS: at 9.14% examples, 203793 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:06:55,787 : INFO : PROGRESS: at 10.43% examples, 203384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:56,801 : INFO : PROGRESS: at 11.71% examples, 203442 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:57,878 : INFO : PROGRESS: at 13.08% examples, 203657 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:06:58,900 : INFO : PROGRESS: at 14.40% examples, 204187 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:06:59,930 : INFO : PROGRESS: at 15.68% examples, 203867 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:00,971 : INFO : PROGRESS: at 17.06% examples, 204550 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:01,996 : INFO : PROGRESS: at 18.35% examples, 204339 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:03,046 : INFO : PROGRESS: at 19.68% examples, 204298 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:04,054 : INFO : PROGRESS: at 20.96% examples, 204355 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:05,075 : INFO : PROGRESS: at 22.24% examples, 204247 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:06,108 : INFO : PROGRESS: at 23.56% examples, 204424 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:07,156 : INFO : PROGRESS: at 24.94% examples, 204788 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:08,235 : INFO : PROGRESS: at 26.22% examples, 204100 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:09,311 : INFO : PROGRESS: at 27.50% examples, 203497 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:10,323 : INFO : PROGRESS: at 28.78% examples, 203501 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:11,336 : INFO : PROGRESS: at 29.97% examples, 202908 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:12,355 : INFO : PROGRESS: at 31.25% examples, 202911 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:13,368 : INFO : PROGRESS: at 32.54% examples, 202961 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:14,385 : INFO : PROGRESS: at 33.76% examples, 202694 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:15,404 : INFO : PROGRESS: at 35.14% examples, 203219 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:16,436 : INFO : PROGRESS: at 36.52% examples, 203628 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:17,439 : INFO : PROGRESS: at 37.80% examples, 203713 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:18,467 : INFO : PROGRESS: at 39.14% examples, 203851 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:19,488 : INFO : PROGRESS: at 40.42% examples, 203804 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:20,528 : INFO : PROGRESS: at 41.74% examples, 203867 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:21,587 : INFO : PROGRESS: at 43.15% examples, 204254 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:22,646 : INFO : PROGRESS: at 44.48% examples, 204194 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:23,667 : INFO : PROGRESS: at 45.81% examples, 204338 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:24,724 : INFO : PROGRESS: at 47.13% examples, 204296 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:25,729 : INFO : PROGRESS: at 48.46% examples, 204525 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:26,732 : INFO : PROGRESS: at 49.65% examples, 204180 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:27,758 : INFO : PROGRESS: at 50.94% examples, 204113 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:28,778 : INFO : PROGRESS: at 52.22% examples, 204072 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:29,817 : INFO : PROGRESS: at 53.59% examples, 204286 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:30,838 : INFO : PROGRESS: at 55.00% examples, 204753 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:31,877 : INFO : PROGRESS: at 56.38% examples, 204948 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:32,895 : INFO : PROGRESS: at 57.76% examples, 205230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:33,926 : INFO : PROGRESS: at 59.14% examples, 205440 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:34,948 : INFO : PROGRESS: at 60.38% examples, 205214 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:35,966 : INFO : PROGRESS: at 61.65% examples, 205170 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:36,989 : INFO : PROGRESS: at 63.02% examples, 205405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:38,000 : INFO : PROGRESS: at 64.26% examples, 205235 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:39,002 : INFO : PROGRESS: at 65.63% examples, 205540 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:40,014 : INFO : PROGRESS: at 66.73% examples, 204956 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:41,031 : INFO : PROGRESS: at 68.00% examples, 204921 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:42,031 : INFO : PROGRESS: at 69.29% examples, 204945 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:43,067 : INFO : PROGRESS: at 70.57% examples, 204850 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 18:07:44,071 : INFO : PROGRESS: at 71.85% examples, 204865 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:45,111 : INFO : PROGRESS: at 73.18% examples, 204875 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 18:07:46,114 : INFO : PROGRESS: at 74.41% examples, 204774 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:47,145 : INFO : PROGRESS: at 75.78% examples, 204944 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:07:48,161 : INFO : PROGRESS: at 77.07% examples, 204915 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:49,253 : INFO : PROGRESS: at 78.36% examples, 204638 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:50,352 : INFO : PROGRESS: at 79.74% examples, 204577 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 18:07:51,389 : INFO : PROGRESS: at 81.06% examples, 204611 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:52,403 : INFO : PROGRESS: at 82.39% examples, 204711 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:53,455 : INFO : PROGRESS: at 83.66% examples, 204579 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:54,490 : INFO : PROGRESS: at 84.99% examples, 204617 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:55,507 : INFO : PROGRESS: at 86.18% examples, 204375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:56,532 : INFO : PROGRESS: at 87.51% examples, 204446 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:07:57,546 : INFO : PROGRESS: at 88.83% examples, 204541 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:58,573 : INFO : PROGRESS: at 90.21% examples, 204706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:07:59,640 : INFO : PROGRESS: at 91.49% examples, 204539 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:08:00,682 : INFO : PROGRESS: at 92.82% examples, 204546 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 18:08:01,710 : INFO : PROGRESS: at 94.14% examples, 204601 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:08:02,715 : INFO : PROGRESS: at 95.42% examples, 204614 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:08:03,752 : INFO : PROGRESS: at 96.75% examples, 204637 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:08:04,814 : INFO : PROGRESS: at 98.09% examples, 204596 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 18:08:05,846 : INFO : PROGRESS: at 99.42% examples, 204634 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 18:08:06,183 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 18:08:06,239 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 18:08:06,262 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 18:08:06,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 18:08:06,264 : INFO : training on 21819630 raw words (16123179 effective words) took 78.8s, 204737 effective words/s\n",
      "2017-05-12 18:08:06,265 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 18:08:06,416 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model, separately None\n",
      "2017-05-12 18:08:06,417 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 18:08:06,418 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model.wv.syn0.npy\n",
      "2017-05-12 18:08:06,461 : INFO : not storing attribute cum_table\n",
      "2017-05-12 18:08:06,462 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model.syn1neg.npy\n",
      "2017-05-12 18:08:06,578 : INFO : saved ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model\n"
     ]
    }
   ],
   "source": [
    "dimensions = [100, 200, 300, 400, 500, 600]\n",
    "for i in dimensions:\n",
    "    w2v_parameters = [i, 5, 4, 6, 0.001]\n",
    "    strict = pred.make_w2v_model(dros_strict_real, 'Drosophila/models/dims/dros_strict_dim_parameter_' + str(i), w2v_parameters)\n",
    "    gen = pred.make_w2v_model(dros_gen_real, 'Drosophila/models/dims/dros_gen_dim_parameter_' + str(i), w2v_parameters)\n",
    "    be = pred.make_w2v_model(dros_be_real, 'Drosophila/models/dims/dros_be_dim_parameter_' + str(i), w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:22:56,642 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_100_model\n",
      "2017-05-12 18:22:56,958 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_100_model.wv.* with mmap=None\n",
      "2017-05-12 18:22:56,959 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:22:56,960 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:22:56,961 : INFO : loaded ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_100_model\n",
      "2017-05-12 18:22:56,975 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_100_model\n",
      "2017-05-12 18:22:57,049 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_100_model.wv.* with mmap=None\n",
      "2017-05-12 18:22:57,054 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:22:57,055 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:22:57,056 : INFO : loaded ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_100_model\n",
      "2017-05-12 18:22:57,086 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_100_model\n",
      "2017-05-12 18:22:57,242 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_100_model.wv.* with mmap=None\n",
      "2017-05-12 18:22:57,245 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:22:57,245 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:22:57,246 : INFO : loaded ../../Results/Drosophila/models/dims/dros_be_dim_parameter_100_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:26:26,014 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_200_model\n",
      "2017-05-12 18:26:26,055 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_200_model.wv.* with mmap=None\n",
      "2017-05-12 18:26:26,056 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:26:26,057 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:26:26,057 : INFO : loaded ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_200_model\n",
      "2017-05-12 18:26:26,068 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_200_model\n",
      "2017-05-12 18:26:26,165 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_200_model.wv.* with mmap=None\n",
      "2017-05-12 18:26:26,166 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:26:26,167 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:26:26,168 : INFO : loaded ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_200_model\n",
      "2017-05-12 18:26:26,190 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_200_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:26:26,392 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_200_model.wv.* with mmap=None\n",
      "2017-05-12 18:26:26,394 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:26:26,395 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:26:26,395 : INFO : loaded ../../Results/Drosophila/models/dims/dros_be_dim_parameter_200_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:35:31,066 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_300_model\n",
      "2017-05-12 18:35:31,153 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_300_model.wv.* with mmap=None\n",
      "2017-05-12 18:35:31,154 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:35:31,155 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:35:31,155 : INFO : loaded ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_300_model\n",
      "2017-05-12 18:35:31,170 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_300_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:35:31,341 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_300_model.wv.* with mmap=None\n",
      "2017-05-12 18:35:31,343 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:35:31,343 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:35:31,344 : INFO : loaded ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_300_model\n",
      "2017-05-12 18:35:31,378 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_300_model\n",
      "2017-05-12 18:35:31,763 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_300_model.wv.* with mmap=None\n",
      "2017-05-12 18:35:31,770 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:35:31,771 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:35:31,772 : INFO : loaded ../../Results/Drosophila/models/dims/dros_be_dim_parameter_300_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:46:54,417 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_400_model\n",
      "2017-05-12 18:46:54,514 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_400_model.wv.* with mmap=None\n",
      "2017-05-12 18:46:54,517 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:46:54,518 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:46:54,519 : INFO : loaded ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_400_model\n",
      "2017-05-12 18:46:54,539 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_400_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:46:54,761 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_400_model.wv.* with mmap=None\n",
      "2017-05-12 18:46:54,763 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:46:54,765 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:46:54,767 : INFO : loaded ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_400_model\n",
      "2017-05-12 18:46:54,797 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_400_model\n",
      "2017-05-12 18:46:55,410 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_400_model.wv.* with mmap=None\n",
      "2017-05-12 18:46:55,413 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:46:55,414 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:46:55,417 : INFO : loaded ../../Results/Drosophila/models/dims/dros_be_dim_parameter_400_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:56:11,763 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_500_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 18:56:11,913 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_500_model.wv.* with mmap=None\n",
      "2017-05-12 18:56:11,914 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:56:11,915 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:56:11,916 : INFO : loaded ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_500_model\n",
      "2017-05-12 18:56:11,931 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_500_model\n",
      "2017-05-12 18:56:12,279 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_500_model.wv.* with mmap=None\n",
      "2017-05-12 18:56:12,281 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:56:12,282 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:56:12,283 : INFO : loaded ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_500_model\n",
      "2017-05-12 18:56:12,315 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_500_model\n",
      "2017-05-12 18:56:13,118 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_500_model.wv.* with mmap=None\n",
      "2017-05-12 18:56:13,120 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 18:56:13,121 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 18:56:13,122 : INFO : loaded ../../Results/Drosophila/models/dims/dros_be_dim_parameter_500_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:04:42,783 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_600_model\n",
      "2017-05-12 19:04:42,883 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_600_model.wv.* with mmap=None\n",
      "2017-05-12 19:04:42,884 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 19:04:42,884 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 19:04:42,885 : INFO : loaded ../../Results/Drosophila/models/dims/dros_strict_dim_parameter_600_model\n",
      "2017-05-12 19:04:42,899 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_600_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:04:43,148 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_600_model.wv.* with mmap=None\n",
      "2017-05-12 19:04:43,150 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 19:04:43,151 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 19:04:43,151 : INFO : loaded ../../Results/Drosophila/models/dims/dros_gen_dim_parameter_600_model\n",
      "2017-05-12 19:04:43,176 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model\n",
      "2017-05-12 19:04:43,213 : INFO : loading wv recursively from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model.wv.* with mmap=None\n",
      "2017-05-12 19:04:43,215 : INFO : loading syn0 from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 19:04:43,249 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 19:04:43,250 : INFO : loading syn1neg from ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 19:04:43,293 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 19:04:43,294 : INFO : loaded ../../Results/Drosophila/models/dims/dros_be_dim_parameter_600_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    }
   ],
   "source": [
    "for i in dimensions:\n",
    "    w2w_model_strict = word2vec.Word2Vec.load('../../Results/Drosophila/models/dims/dros_strict_dim_parameter_'+str(i)+'_model')\n",
    "    w2w_model_gen = word2vec.Word2Vec.load('../../Results/Drosophila/models/dims/dros_gen_dim_parameter_'+str(i)+'_model')\n",
    "    w2w_model_be = word2vec.Word2Vec.load('../../Results/Drosophila/models/dims/dros_be_dim_parameter_'+str(i)+'_model')\n",
    "    \n",
    "    for seed in random_seeds:\n",
    "        data_name = '../../Results/Drosophila/train_val/dros_tr_val_split_' + str(seed)\n",
    "        train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "        train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "        validation_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "        validation_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_strict, feature_count=i)\n",
    "        \n",
    "        strict_list_SR_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_gen, feature_count=i)\n",
    "        \n",
    "        strict_list_GEN_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                      train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_be, feature_count=i)\n",
    "        \n",
    "        strict_list_BE_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        pickle.dump(strict_list_SR_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/dims/dros_strict_list_SR_dims_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_GEN_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/dims/dros_strict_list_GEN_dims_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_BE_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/dims/dros_strict_list_BE_dims_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        \n",
    "        strict_final_list = [strict_list_SR_dims_param, \n",
    "                             strict_list_GEN_dims_param, \n",
    "                             strict_list_BE_dims_param]\n",
    "        print ('\\nPredicting\\n')\n",
    "        errors = []\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "\n",
    "        for entry in strict_final_list:\n",
    "            error_w2v_norm, probs_w2v_norm = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                                 entry[2], entry[3])\n",
    "            fpr_w2v_norm, tpr_w2v_norm, _ = roc_curve(entry[3], probs_w2v_norm)\n",
    "            error_w2v_fs, probs_w2v_fs = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                             entry[2], entry[3],\n",
    "                                                             feature_selection=True)\n",
    "            fpr_w2v_fs, tpr_w2v_fs, _ = roc_curve(entry[3], probs_w2v_fs)\n",
    "\n",
    "            errors.append([error_w2v_norm, error_w2v_fs])\n",
    "            fpr.append([fpr_w2v_norm, fpr_w2v_fs])\n",
    "            tpr.append([tpr_w2v_norm, tpr_w2v_fs])\n",
    "            \n",
    "        pickle.dump(errors, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/dims/dros_dim_param'+str(i)+'_errors_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(fpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/dims/dros_dim_param'+str(i)+'_fpr_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(tpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/dims/dros_dim_param'+str(i)+'_tpr_pickle_'+str(seed)+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mult_open(direct, pattern):\n",
    "    pickle_list = []\n",
    "    file_list = []\n",
    "    for f in os.listdir(direct):\n",
    "        if re.search(pattern, f):\n",
    "            f = f.split('_')\n",
    "            f[-1] = f[-1][:-4]\n",
    "            file_list.append(f)\n",
    "    file_list.sort(key = lambda x: int(x[-1]))\n",
    "    for file in file_list:\n",
    "        file = '_'.join(file)\n",
    "        file = file + '.pkl'\n",
    "        pkl = pickle.load(open(os.path.join(direct, file), 'rb'))\n",
    "        pickle_list.append(pkl)\n",
    "    return pickle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_parameter = []\n",
    "for seed in random_seeds:\n",
    "    mean_err_dros_strict = []\n",
    "    mean_auc_dros_strict = []\n",
    "    mean_err_dros_gen = []\n",
    "    mean_auc_dros_gen = []\n",
    "    mean_err_dros_be = []\n",
    "    mean_auc_dros_be = []\n",
    "    for i in dimensions:\n",
    "        drct = '../../Results/Drosophila/error_fpr_tpr/w2v_param_search/dims/'\n",
    "        errors_dros = mult_open(drct, str(i)+'_errors_pickle_'+str(seed))\n",
    "        fpr_dros = mult_open(drct, str(i)+'_fpr_pickle_'+str(seed))\n",
    "        tpr_dros = mult_open(drct, str(i)+'_tpr_pickle_'+str(seed))\n",
    "        for e, f, t in zip(errors_dros, fpr_dros, tpr_dros):\n",
    "            input_list = [[e, f, t]]\n",
    "            name_list = ['drosophila']\n",
    "            for idx in range(3):\n",
    "                for item, name in zip(input_list, name_list):\n",
    "                    for fpr_item, tpr_item, error_item in zip(item[1][idx], item[2][idx], item[0][idx]):\n",
    "                        roc_auc = auc(fpr_item, tpr_item)\n",
    "                        auc_val = '%.3f' % roc_auc\n",
    "                        error = '%.3f' % error_item\n",
    "                        if idx == 0:\n",
    "                            mean_err_dros_strict.append(error_item)\n",
    "                            mean_auc_dros_strict.append(roc_auc)\n",
    "                        elif idx == 1:\n",
    "                            mean_err_dros_gen.append(error_item)\n",
    "                            mean_auc_dros_gen.append(roc_auc)\n",
    "                        elif idx == 2:\n",
    "                            mean_err_dros_be.append(error_item)\n",
    "                            mean_auc_dros_be.append(roc_auc)\n",
    "\n",
    "    mean_err_dros_org_strict = mean_err_dros_strict[0::2]\n",
    "    mean_err_dros_fs_strict = mean_err_dros_strict[1::2]\n",
    "    mean_auc_dros_org_strict = mean_auc_dros_strict[0::2]\n",
    "    mean_auc_dros_fs_strict = mean_auc_dros_strict[1::2]\n",
    "\n",
    "    mean_err_dros_org_gen = mean_err_dros_gen[0::2]\n",
    "    mean_err_dros_fs_gen = mean_err_dros_gen[1::2]\n",
    "    mean_auc_dros_org_gen = mean_auc_dros_gen[0::2]\n",
    "    mean_auc_dros_fs_gen = mean_auc_dros_gen[1::2]\n",
    "\n",
    "    mean_err_dros_org_be = mean_err_dros_be[0::2]\n",
    "    mean_err_dros_fs_be = mean_err_dros_be[1::2]\n",
    "    mean_auc_dros_org_be = mean_auc_dros_be[0::2]\n",
    "    mean_auc_dros_fs_be = mean_auc_dros_be[1::2]\n",
    "    \n",
    "    org_error = max([(mean_err_dros_org_strict.index(min(mean_err_dros_org_strict))+1)*100, \n",
    "                     (mean_err_dros_org_gen.index(min(mean_err_dros_org_gen))+1)*100, \n",
    "                     (mean_err_dros_org_be.index(min(mean_err_dros_org_be))+1)*100])\n",
    "    \n",
    "    fs_error = max([(mean_err_dros_fs_strict.index(min(mean_err_dros_fs_strict))+1)*100, \n",
    "                     (mean_err_dros_fs_gen.index(min(mean_err_dros_fs_gen))+1)*100, \n",
    "                     (mean_err_dros_fs_be.index(min(mean_err_dros_fs_be))+1)*100])\n",
    "    \n",
    "    org_auc = max([(mean_auc_dros_org_strict.index(max(mean_auc_dros_org_strict))+1)*100, \n",
    "                     (mean_auc_dros_org_gen.index(max(mean_auc_dros_org_gen))+1)*100, \n",
    "                     (mean_auc_dros_org_be.index(max(mean_auc_dros_org_be))+1)*100])\n",
    "    \n",
    "    fs_auc = max([(mean_auc_dros_fs_strict.index(max(mean_auc_dros_fs_strict))+1)*100, \n",
    "                     (mean_auc_dros_fs_gen.index(max(mean_auc_dros_fs_gen))+1)*100, \n",
    "                     (mean_auc_dros_fs_be.index(max(mean_auc_dros_fs_be))+1)*100])\n",
    "    \n",
    "    best_parameter.append((seed, [org_error, fs_error, org_auc, fs_auc]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, [400, 200, 500, 400])\n",
      "(235, [300, 300, 600, 600])\n",
      "(905, [400, 100, 500, 500])\n",
      "(2895, [300, 300, 200, 600])\n",
      "(3462, [400, 200, 600, 600])\n",
      "(4225, [600, 300, 600, 600])\n",
      "(5056, [600, 200, 600, 500])\n",
      "(5192, [600, 600, 600, 600])\n",
      "(7751, [100, 500, 300, 200])\n",
      "(7813, [300, 600, 300, 500])\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "best_overall = []\n",
    "for i in best_parameter:\n",
    "    best_overall.append(most_common(i[1]))\n",
    "    print(i)\n",
    "best_dims = most_common(best_overall)\n",
    "print(best_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:39:04,959 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:39:04,961 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:39:05,002 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 19:39:05,004 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:39:05,041 : INFO : min_count=1 retains 12999 unique words (100% of original 12999, drops 0)\n",
      "2017-05-12 19:39:05,042 : INFO : min_count=1 leaves 188627 word corpus (100% of original 188627, drops 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:39:05,078 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 19:39:05,080 : INFO : sample=0.001 downsamples 32 most-common words\n",
      "2017-05-12 19:39:05,080 : INFO : downsampling leaves estimated 141308 word corpus (74.9% of prior 188627)\n",
      "2017-05-12 19:39:05,081 : INFO : estimated required memory for 12999 words and 600 dimensions: 68894700 bytes\n",
      "2017-05-12 19:39:05,138 : INFO : resetting layer weights\n",
      "2017-05-12 19:39:05,380 : INFO : training model with 4 workers on 12999 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:39:05,381 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:39:06,398 : INFO : PROGRESS: at 28.58% examples, 199687 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:07,425 : INFO : PROGRESS: at 57.22% examples, 198239 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:08,450 : INFO : PROGRESS: at 87.88% examples, 202689 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:08,736 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:39:08,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:39:08,805 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:39:08,819 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:39:08,821 : INFO : training on 943135 raw words (706323 effective words) took 3.4s, 205714 effective words/s\n",
      "2017-05-12 19:39:08,823 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:39:08,943 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_1_model, separately None\n",
      "2017-05-12 19:39:08,947 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:39:08,949 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:39:09,838 : INFO : saved ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_1_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:39:10,411 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:39:10,413 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:39:10,482 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 19:39:10,547 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:39:10,610 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 19:39:10,669 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 19:39:10,670 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:39:11,034 : INFO : min_count=1 retains 22824 unique words (100% of original 22824, drops 0)\n",
      "2017-05-12 19:39:11,034 : INFO : min_count=1 leaves 904687 word corpus (100% of original 904687, drops 0)\n",
      "2017-05-12 19:39:11,102 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 19:39:11,104 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-12 19:39:11,105 : INFO : downsampling leaves estimated 672003 word corpus (74.3% of prior 904687)\n",
      "2017-05-12 19:39:11,106 : INFO : estimated required memory for 22824 words and 600 dimensions: 120967200 bytes\n",
      "2017-05-12 19:39:11,212 : INFO : resetting layer weights\n",
      "2017-05-12 19:39:11,608 : INFO : training model with 4 workers on 22824 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:39:11,610 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:39:12,624 : INFO : PROGRESS: at 5.96% examples, 198716 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:13,654 : INFO : PROGRESS: at 12.38% examples, 203858 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:14,658 : INFO : PROGRESS: at 18.77% examples, 207254 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:15,689 : INFO : PROGRESS: at 24.94% examples, 205763 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:16,729 : INFO : PROGRESS: at 31.80% examples, 208924 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:17,729 : INFO : PROGRESS: at 38.20% examples, 209898 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:18,758 : INFO : PROGRESS: at 44.15% examples, 207697 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:19,797 : INFO : PROGRESS: at 50.13% examples, 205794 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:20,800 : INFO : PROGRESS: at 55.65% examples, 203520 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:39:21,851 : INFO : PROGRESS: at 61.80% examples, 202929 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:22,882 : INFO : PROGRESS: at 68.24% examples, 203445 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:23,883 : INFO : PROGRESS: at 74.19% examples, 203183 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:24,976 : INFO : PROGRESS: at 80.80% examples, 203230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:25,986 : INFO : PROGRESS: at 87.21% examples, 203918 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:27,042 : INFO : PROGRESS: at 93.61% examples, 203887 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:27,903 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:39:27,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:39:27,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:39:27,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:39:27,977 : INFO : training on 4523435 raw words (3360124 effective words) took 16.4s, 205376 effective words/s\n",
      "2017-05-12 19:39:27,978 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:39:28,162 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model, separately None\n",
      "2017-05-12 19:39:28,163 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:39:28,163 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model.wv.syn0.npy\n",
      "2017-05-12 19:39:28,201 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:39:28,202 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model.syn1neg.npy\n",
      "2017-05-12 19:39:28,301 : INFO : saved ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:39:30,846 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:39:30,847 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:39:30,906 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 19:39:30,961 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 19:39:31,013 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:39:31,075 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 19:39:31,133 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 19:39:31,186 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 19:39:31,242 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 19:39:31,299 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 19:39:31,354 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 19:39:31,408 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 19:39:31,462 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 19:39:31,525 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 19:39:31,589 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 19:39:31,644 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 19:39:31,707 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 19:39:31,790 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 19:39:31,854 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 19:39:31,911 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 19:39:31,972 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 19:39:32,006 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 19:39:32,007 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:39:32,582 : INFO : min_count=1 retains 48808 unique words (100% of original 48808, drops 0)\n",
      "2017-05-12 19:39:32,583 : INFO : min_count=1 leaves 4363926 word corpus (100% of original 4363926, drops 0)\n",
      "2017-05-12 19:39:32,711 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 19:39:32,714 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 19:39:32,715 : INFO : downsampling leaves estimated 3280546 word corpus (75.2% of prior 4363926)\n",
      "2017-05-12 19:39:32,715 : INFO : estimated required memory for 48808 words and 600 dimensions: 258682400 bytes\n",
      "2017-05-12 19:39:32,986 : INFO : resetting layer weights\n",
      "2017-05-12 19:39:34,037 : INFO : training model with 4 workers on 48808 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:39:34,039 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:39:35,062 : INFO : PROGRESS: at 0.87% examples, 140250 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:36,083 : INFO : PROGRESS: at 1.92% examples, 154725 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:39:37,109 : INFO : PROGRESS: at 3.10% examples, 166635 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:38,171 : INFO : PROGRESS: at 4.34% examples, 172890 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:39,231 : INFO : PROGRESS: at 5.57% examples, 176643 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:40,236 : INFO : PROGRESS: at 6.67% examples, 177073 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:41,263 : INFO : PROGRESS: at 7.86% examples, 178920 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:42,289 : INFO : PROGRESS: at 9.01% examples, 179418 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:43,295 : INFO : PROGRESS: at 10.01% examples, 177762 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:44,387 : INFO : PROGRESS: at 11.11% examples, 176421 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:45,397 : INFO : PROGRESS: at 12.21% examples, 176598 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:46,409 : INFO : PROGRESS: at 13.26% examples, 176114 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:47,433 : INFO : PROGRESS: at 14.49% examples, 177789 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:48,440 : INFO : PROGRESS: at 15.64% examples, 178405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:49,445 : INFO : PROGRESS: at 16.83% examples, 179456 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:50,466 : INFO : PROGRESS: at 18.03% examples, 180204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:51,478 : INFO : PROGRESS: at 19.23% examples, 180915 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:52,500 : INFO : PROGRESS: at 20.42% examples, 181475 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:39:53,510 : INFO : PROGRESS: at 21.56% examples, 181728 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:54,553 : INFO : PROGRESS: at 22.61% examples, 180917 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:39:55,605 : INFO : PROGRESS: at 23.88% examples, 181833 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:56,629 : INFO : PROGRESS: at 25.12% examples, 182551 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:39:57,656 : INFO : PROGRESS: at 26.36% examples, 183215 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:58,705 : INFO : PROGRESS: at 27.59% examples, 183633 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:39:59,742 : INFO : PROGRESS: at 28.83% examples, 184108 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:00,782 : INFO : PROGRESS: at 30.25% examples, 185658 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:40:01,853 : INFO : PROGRESS: at 31.48% examples, 185799 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:40:02,869 : INFO : PROGRESS: at 32.58% examples, 185502 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:03,937 : INFO : PROGRESS: at 33.76% examples, 185413 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:04,939 : INFO : PROGRESS: at 34.86% examples, 185226 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:05,979 : INFO : PROGRESS: at 35.92% examples, 184601 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:06,995 : INFO : PROGRESS: at 37.07% examples, 184612 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:08,010 : INFO : PROGRESS: at 38.26% examples, 184841 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:09,028 : INFO : PROGRESS: at 39.41% examples, 184823 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:10,050 : INFO : PROGRESS: at 40.56% examples, 184786 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:11,086 : INFO : PROGRESS: at 41.74% examples, 184890 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:12,187 : INFO : PROGRESS: at 42.93% examples, 184684 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:13,230 : INFO : PROGRESS: at 44.16% examples, 184940 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:14,265 : INFO : PROGRESS: at 45.35% examples, 185030 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:15,363 : INFO : PROGRESS: at 46.63% examples, 185204 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:40:16,364 : INFO : PROGRESS: at 47.86% examples, 185616 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:17,371 : INFO : PROGRESS: at 49.01% examples, 185632 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:18,413 : INFO : PROGRESS: at 50.16% examples, 185507 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:19,429 : INFO : PROGRESS: at 51.35% examples, 185658 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-12 19:40:20,458 : INFO : PROGRESS: at 52.49% examples, 185584 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:21,487 : INFO : PROGRESS: at 53.68% examples, 185674 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:22,525 : INFO : PROGRESS: at 54.86% examples, 185726 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:40:23,549 : INFO : PROGRESS: at 56.10% examples, 185982 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:24,567 : INFO : PROGRESS: at 57.30% examples, 186102 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:40:25,630 : INFO : PROGRESS: at 58.59% examples, 186336 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:26,630 : INFO : PROGRESS: at 59.87% examples, 186787 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:27,635 : INFO : PROGRESS: at 61.10% examples, 187075 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:28,685 : INFO : PROGRESS: at 62.29% examples, 187054 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:29,760 : INFO : PROGRESS: at 63.56% examples, 187221 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:30,796 : INFO : PROGRESS: at 64.76% examples, 187235 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:31,821 : INFO : PROGRESS: at 65.99% examples, 187425 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:32,834 : INFO : PROGRESS: at 67.14% examples, 187388 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:33,893 : INFO : PROGRESS: at 68.42% examples, 187586 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:34,913 : INFO : PROGRESS: at 69.61% examples, 187646 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:35,949 : INFO : PROGRESS: at 70.85% examples, 187786 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:37,002 : INFO : PROGRESS: at 72.18% examples, 188100 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:38,008 : INFO : PROGRESS: at 73.45% examples, 188431 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:39,030 : INFO : PROGRESS: at 74.64% examples, 188473 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:40,031 : INFO : PROGRESS: at 75.78% examples, 188459 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:41,048 : INFO : PROGRESS: at 77.03% examples, 188621 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:42,069 : INFO : PROGRESS: at 78.13% examples, 188437 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:43,072 : INFO : PROGRESS: at 79.37% examples, 188635 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:44,073 : INFO : PROGRESS: at 80.47% examples, 188511 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:40:45,114 : INFO : PROGRESS: at 81.75% examples, 188710 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:46,130 : INFO : PROGRESS: at 82.89% examples, 188656 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:47,159 : INFO : PROGRESS: at 84.03% examples, 188566 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:48,180 : INFO : PROGRESS: at 85.08% examples, 188297 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:49,236 : INFO : PROGRESS: at 86.22% examples, 188153 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:50,240 : INFO : PROGRESS: at 87.32% examples, 188036 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:40:51,274 : INFO : PROGRESS: at 88.46% examples, 187951 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:52,287 : INFO : PROGRESS: at 89.61% examples, 187917 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:53,303 : INFO : PROGRESS: at 90.76% examples, 187882 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:54,306 : INFO : PROGRESS: at 91.99% examples, 188057 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:55,307 : INFO : PROGRESS: at 93.18% examples, 188145 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:56,345 : INFO : PROGRESS: at 94.37% examples, 188144 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:57,372 : INFO : PROGRESS: at 95.56% examples, 188170 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:58,429 : INFO : PROGRESS: at 96.80% examples, 188214 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:40:59,442 : INFO : PROGRESS: at 98.04% examples, 188359 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:00,498 : INFO : PROGRESS: at 99.33% examples, 188484 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:00,956 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:41:01,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:41:01,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:41:01,054 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:41:01,055 : INFO : training on 21819630 raw words (16405294 effective words) took 87.0s, 188544 effective words/s\n",
      "2017-05-12 19:41:01,055 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:41:01,518 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model, separately None\n",
      "2017-05-12 19:41:01,518 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:41:01,519 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model.wv.syn0.npy\n",
      "2017-05-12 19:41:01,612 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:41:01,614 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model.syn1neg.npy\n",
      "2017-05-12 19:41:01,867 : INFO : saved ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model\n",
      "2017-05-12 19:41:02,150 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:41:02,151 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:41:02,201 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 19:41:02,201 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:41:02,235 : INFO : min_count=2 retains 7143 unique words (54% of original 12999, drops 5856)\n",
      "2017-05-12 19:41:02,236 : INFO : min_count=2 leaves 182771 word corpus (96% of original 188627, drops 5856)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:41:02,258 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 19:41:02,261 : INFO : sample=0.001 downsamples 34 most-common words\n",
      "2017-05-12 19:41:02,262 : INFO : downsampling leaves estimated 134967 word corpus (73.8% of prior 182771)\n",
      "2017-05-12 19:41:02,263 : INFO : estimated required memory for 7143 words and 600 dimensions: 37857900 bytes\n",
      "2017-05-12 19:41:02,299 : INFO : resetting layer weights\n",
      "2017-05-12 19:41:02,451 : INFO : training model with 4 workers on 7143 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:41:02,452 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:41:03,474 : INFO : PROGRESS: at 31.82% examples, 211078 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:04,506 : INFO : PROGRESS: at 68.81% examples, 226763 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:05,284 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:41:05,290 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:41:05,320 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:41:05,323 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:41:05,324 : INFO : training on 943135 raw words (674980 effective words) took 2.9s, 235504 effective words/s\n",
      "2017-05-12 19:41:05,324 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:41:05,386 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_2_model, separately None\n",
      "2017-05-12 19:41:05,387 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:41:05,388 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:41:05,762 : INFO : saved ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:41:06,436 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:41:06,437 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:41:06,504 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 19:41:06,574 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:41:06,644 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 19:41:06,701 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 19:41:06,702 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:41:06,754 : INFO : min_count=2 retains 14084 unique words (61% of original 22824, drops 8740)\n",
      "2017-05-12 19:41:06,755 : INFO : min_count=2 leaves 895947 word corpus (99% of original 904687, drops 8740)\n",
      "2017-05-12 19:41:06,797 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 19:41:06,800 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-12 19:41:06,801 : INFO : downsampling leaves estimated 662443 word corpus (73.9% of prior 895947)\n",
      "2017-05-12 19:41:06,801 : INFO : estimated required memory for 14084 words and 600 dimensions: 74645200 bytes\n",
      "2017-05-12 19:41:06,869 : INFO : resetting layer weights\n",
      "2017-05-12 19:41:07,133 : INFO : training model with 4 workers on 14084 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:41:07,134 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:41:08,155 : INFO : PROGRESS: at 6.41% examples, 208113 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:09,157 : INFO : PROGRESS: at 13.47% examples, 220744 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:10,200 : INFO : PROGRESS: at 20.53% examples, 222005 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:11,250 : INFO : PROGRESS: at 27.84% examples, 223931 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:12,286 : INFO : PROGRESS: at 34.67% examples, 222913 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:13,300 : INFO : PROGRESS: at 40.61% examples, 218223 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:14,317 : INFO : PROGRESS: at 46.59% examples, 214763 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:15,335 : INFO : PROGRESS: at 52.54% examples, 212181 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:16,374 : INFO : PROGRESS: at 58.51% examples, 209706 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:17,391 : INFO : PROGRESS: at 64.68% examples, 208894 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:18,403 : INFO : PROGRESS: at 71.09% examples, 208986 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:19,414 : INFO : PROGRESS: at 78.16% examples, 210834 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:20,424 : INFO : PROGRESS: at 84.77% examples, 211306 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:21,504 : INFO : PROGRESS: at 91.85% examples, 211743 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:22,504 : INFO : PROGRESS: at 98.91% examples, 213199 words/s, in_qsize 5, out_qsize 0\n",
      "2017-05-12 19:41:22,575 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:41:22,598 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:41:22,618 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:41:22,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:41:22,646 : INFO : training on 4523435 raw words (3312087 effective words) took 15.5s, 213587 effective words/s\n",
      "2017-05-12 19:41:22,647 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:41:22,768 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_2_model, separately None\n",
      "2017-05-12 19:41:22,768 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:41:22,769 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:41:23,697 : INFO : saved ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:41:26,719 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:41:26,721 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:41:26,792 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 19:41:26,881 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:41:26,953 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 19:41:27,025 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 19:41:27,094 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 19:41:27,162 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 19:41:27,235 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 19:41:27,306 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 19:41:27,371 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 19:41:27,441 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 19:41:27,508 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 19:41:27,574 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 19:41:27,642 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 19:41:27,723 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 19:41:27,789 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 19:41:27,858 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 19:41:27,929 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 19:41:27,998 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 19:41:28,067 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 19:41:28,106 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 19:41:28,107 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:41:28,211 : INFO : min_count=2 retains 30594 unique words (62% of original 48808, drops 18214)\n",
      "2017-05-12 19:41:28,212 : INFO : min_count=2 leaves 4345712 word corpus (99% of original 4363926, drops 18214)\n",
      "2017-05-12 19:41:28,312 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 19:41:28,316 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 19:41:28,317 : INFO : downsampling leaves estimated 3260736 word corpus (75.0% of prior 4345712)\n",
      "2017-05-12 19:41:28,318 : INFO : estimated required memory for 30594 words and 600 dimensions: 162148200 bytes\n",
      "2017-05-12 19:41:28,481 : INFO : resetting layer weights\n",
      "2017-05-12 19:41:29,142 : INFO : training model with 4 workers on 30594 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:41:29,143 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:41:30,169 : INFO : PROGRESS: at 1.00% examples, 161554 words/s, in_qsize 6, out_qsize 2\n",
      "2017-05-12 19:41:31,172 : INFO : PROGRESS: at 2.24% examples, 181064 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:41:32,185 : INFO : PROGRESS: at 3.46% examples, 187010 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:33,186 : INFO : PROGRESS: at 4.61% examples, 186751 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:41:34,242 : INFO : PROGRESS: at 5.85% examples, 187613 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:41:35,262 : INFO : PROGRESS: at 7.13% examples, 190567 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:36,279 : INFO : PROGRESS: at 8.36% examples, 191633 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:37,306 : INFO : PROGRESS: at 9.65% examples, 193112 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:38,343 : INFO : PROGRESS: at 10.84% examples, 192497 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:39,346 : INFO : PROGRESS: at 12.03% examples, 192618 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:40,397 : INFO : PROGRESS: at 13.26% examples, 192502 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:41:41,430 : INFO : PROGRESS: at 14.54% examples, 193345 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:42,468 : INFO : PROGRESS: at 15.82% examples, 193970 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:43,550 : INFO : PROGRESS: at 16.97% examples, 192350 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:44,556 : INFO : PROGRESS: at 18.12% examples, 191896 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:45,568 : INFO : PROGRESS: at 19.23% examples, 190962 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:41:46,611 : INFO : PROGRESS: at 20.46% examples, 191081 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:47,640 : INFO : PROGRESS: at 21.60% examples, 190549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:48,644 : INFO : PROGRESS: at 22.75% examples, 190296 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:49,656 : INFO : PROGRESS: at 24.02% examples, 191093 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:50,692 : INFO : PROGRESS: at 25.17% examples, 190552 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:41:51,707 : INFO : PROGRESS: at 26.45% examples, 191244 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:41:52,755 : INFO : PROGRESS: at 27.63% examples, 190974 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:53,786 : INFO : PROGRESS: at 28.87% examples, 191145 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:54,791 : INFO : PROGRESS: at 30.11% examples, 191526 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:41:55,821 : INFO : PROGRESS: at 31.30% examples, 191406 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:56,884 : INFO : PROGRESS: at 32.49% examples, 191063 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:41:57,903 : INFO : PROGRESS: at 33.76% examples, 191564 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:41:58,947 : INFO : PROGRESS: at 35.05% examples, 191848 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:41:59,968 : INFO : PROGRESS: at 36.28% examples, 192036 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:00,994 : INFO : PROGRESS: at 37.57% examples, 192402 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:02,015 : INFO : PROGRESS: at 38.77% examples, 192321 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:03,033 : INFO : PROGRESS: at 39.96% examples, 192269 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:04,058 : INFO : PROGRESS: at 41.15% examples, 192179 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:05,148 : INFO : PROGRESS: at 42.38% examples, 191952 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:06,195 : INFO : PROGRESS: at 43.56% examples, 191763 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:07,205 : INFO : PROGRESS: at 44.67% examples, 191378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:08,223 : INFO : PROGRESS: at 45.85% examples, 191358 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:42:09,224 : INFO : PROGRESS: at 47.04% examples, 191421 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:10,244 : INFO : PROGRESS: at 48.19% examples, 191215 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:11,340 : INFO : PROGRESS: at 49.42% examples, 191017 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:42:12,372 : INFO : PROGRESS: at 50.76% examples, 191470 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:13,421 : INFO : PROGRESS: at 51.85% examples, 190978 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:14,450 : INFO : PROGRESS: at 53.04% examples, 190928 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:15,468 : INFO : PROGRESS: at 54.32% examples, 191243 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:16,491 : INFO : PROGRESS: at 55.51% examples, 191213 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:17,498 : INFO : PROGRESS: at 56.75% examples, 191393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:18,507 : INFO : PROGRESS: at 57.90% examples, 191263 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:19,534 : INFO : PROGRESS: at 59.09% examples, 191211 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:20,563 : INFO : PROGRESS: at 60.28% examples, 191156 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:21,602 : INFO : PROGRESS: at 61.47% examples, 191071 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:22,650 : INFO : PROGRESS: at 62.71% examples, 191098 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:23,656 : INFO : PROGRESS: at 63.98% examples, 191400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:24,662 : INFO : PROGRESS: at 65.12% examples, 191286 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:25,688 : INFO : PROGRESS: at 66.31% examples, 191249 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:26,754 : INFO : PROGRESS: at 67.55% examples, 191207 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:27,801 : INFO : PROGRESS: at 68.78% examples, 191221 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:28,825 : INFO : PROGRESS: at 70.07% examples, 191443 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:42:29,880 : INFO : PROGRESS: at 71.30% examples, 191435 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:30,931 : INFO : PROGRESS: at 72.54% examples, 191441 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:42:31,936 : INFO : PROGRESS: at 73.77% examples, 191584 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:32,944 : INFO : PROGRESS: at 74.96% examples, 191601 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:42:34,063 : INFO : PROGRESS: at 76.24% examples, 191518 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:42:35,075 : INFO : PROGRESS: at 77.49% examples, 191634 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:36,150 : INFO : PROGRESS: at 78.64% examples, 191339 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:37,152 : INFO : PROGRESS: at 79.87% examples, 191482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:38,201 : INFO : PROGRESS: at 81.01% examples, 191274 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:39,276 : INFO : PROGRESS: at 82.29% examples, 191322 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:40,309 : INFO : PROGRESS: at 83.48% examples, 191274 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:41,340 : INFO : PROGRESS: at 84.76% examples, 191436 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:42:42,347 : INFO : PROGRESS: at 85.99% examples, 191556 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:42:43,363 : INFO : PROGRESS: at 87.28% examples, 191750 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:44,415 : INFO : PROGRESS: at 88.51% examples, 191745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:45,429 : INFO : PROGRESS: at 89.66% examples, 191642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:46,481 : INFO : PROGRESS: at 90.85% examples, 191546 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:47,489 : INFO : PROGRESS: at 92.09% examples, 191655 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:48,545 : INFO : PROGRESS: at 93.37% examples, 191740 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:49,585 : INFO : PROGRESS: at 94.50% examples, 191580 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:50,641 : INFO : PROGRESS: at 95.74% examples, 191574 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:51,693 : INFO : PROGRESS: at 97.07% examples, 191754 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:52,753 : INFO : PROGRESS: at 98.41% examples, 191912 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:42:53,819 : INFO : PROGRESS: at 99.69% examples, 191963 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:42:53,961 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:42:53,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:42:53,992 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:42:53,997 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:42:54,000 : INFO : training on 21819630 raw words (16303058 effective words) took 84.8s, 192142 effective words/s\n",
      "2017-05-12 19:42:54,001 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:42:54,247 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model, separately None\n",
      "2017-05-12 19:42:54,248 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:42:54,250 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model.wv.syn0.npy\n",
      "2017-05-12 19:42:54,301 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:42:54,302 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model.syn1neg.npy\n",
      "2017-05-12 19:42:54,444 : INFO : saved ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model\n",
      "2017-05-12 19:42:54,755 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:42:54,756 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:42:54,808 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 19:42:54,809 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:42:54,836 : INFO : min_count=3 retains 5192 unique words (39% of original 12999, drops 7807)\n",
      "2017-05-12 19:42:54,838 : INFO : min_count=3 leaves 178869 word corpus (94% of original 188627, drops 9758)\n",
      "2017-05-12 19:42:54,853 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 19:42:54,855 : INFO : sample=0.001 downsamples 34 most-common words\n",
      "2017-05-12 19:42:54,856 : INFO : downsampling leaves estimated 130733 word corpus (73.1% of prior 178869)\n",
      "2017-05-12 19:42:54,857 : INFO : estimated required memory for 5192 words and 600 dimensions: 27517600 bytes\n",
      "2017-05-12 19:42:54,878 : INFO : resetting layer weights\n",
      "2017-05-12 19:42:54,980 : INFO : training model with 4 workers on 5192 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:42:54,981 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:42:56,053 : INFO : PROGRESS: at 38.16% examples, 234394 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:57,112 : INFO : PROGRESS: at 78.38% examples, 241368 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:42:57,601 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:42:57,630 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:42:57,634 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:42:57,646 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:42:57,647 : INFO : training on 943135 raw words (654098 effective words) took 2.7s, 245965 effective words/s\n",
      "2017-05-12 19:42:57,648 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:42:57,691 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_3_model, separately None\n",
      "2017-05-12 19:42:57,692 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:42:57,693 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:42:57,959 : INFO : saved ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:42:58,641 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:42:58,642 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:42:58,709 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 19:42:58,775 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:42:58,841 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 19:42:58,896 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 19:42:58,897 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:42:58,936 : INFO : min_count=3 retains 10970 unique words (48% of original 22824, drops 11854)\n",
      "2017-05-12 19:42:58,937 : INFO : min_count=3 leaves 889719 word corpus (98% of original 904687, drops 14968)\n",
      "2017-05-12 19:42:58,972 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 19:42:58,974 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-12 19:42:58,974 : INFO : downsampling leaves estimated 655629 word corpus (73.7% of prior 889719)\n",
      "2017-05-12 19:42:58,975 : INFO : estimated required memory for 10970 words and 600 dimensions: 58141000 bytes\n",
      "2017-05-12 19:42:59,029 : INFO : resetting layer weights\n",
      "2017-05-12 19:42:59,259 : INFO : training model with 4 workers on 10970 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:42:59,260 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:43:00,295 : INFO : PROGRESS: at 6.64% examples, 210852 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:01,324 : INFO : PROGRESS: at 13.25% examples, 211201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:02,374 : INFO : PROGRESS: at 20.09% examples, 212092 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:03,408 : INFO : PROGRESS: at 26.50% examples, 209772 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:04,420 : INFO : PROGRESS: at 33.12% examples, 210796 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:05,437 : INFO : PROGRESS: at 39.07% examples, 207692 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:06,441 : INFO : PROGRESS: at 45.25% examples, 206819 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:07,471 : INFO : PROGRESS: at 51.66% examples, 206483 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:08,510 : INFO : PROGRESS: at 57.86% examples, 205202 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:09,539 : INFO : PROGRESS: at 64.68% examples, 206486 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:10,649 : INFO : PROGRESS: at 71.52% examples, 206111 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:11,712 : INFO : PROGRESS: at 78.61% examples, 207103 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:12,753 : INFO : PROGRESS: at 85.44% examples, 207733 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:13,826 : INFO : PROGRESS: at 92.50% examples, 208312 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:14,844 : INFO : PROGRESS: at 99.34% examples, 209125 words/s, in_qsize 2, out_qsize 3\n",
      "2017-05-12 19:43:14,848 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:43:14,852 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:43:14,854 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:43:14,901 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:43:14,901 : INFO : training on 4523435 raw words (3279117 effective words) took 15.6s, 209732 effective words/s\n",
      "2017-05-12 19:43:14,902 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:43:14,998 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_3_model, separately None\n",
      "2017-05-12 19:43:14,999 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:43:15,000 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:43:15,638 : INFO : saved ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:43:18,984 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:43:18,990 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:43:19,053 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 19:43:19,116 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 19:43:19,182 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:43:19,248 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 19:43:19,306 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 19:43:19,375 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 19:43:19,438 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 19:43:19,501 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 19:43:19,563 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 19:43:19,628 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 19:43:19,690 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 19:43:19,756 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 19:43:19,819 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 19:43:19,881 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 19:43:19,945 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 19:43:20,005 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 19:43:20,073 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 19:43:20,141 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 19:43:20,207 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 19:43:20,250 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 19:43:20,250 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:43:20,329 : INFO : min_count=3 retains 24015 unique words (49% of original 48808, drops 24793)\n",
      "2017-05-12 19:43:20,331 : INFO : min_count=3 leaves 4332554 word corpus (99% of original 4363926, drops 31372)\n",
      "2017-05-12 19:43:20,401 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 19:43:20,403 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 19:43:20,404 : INFO : downsampling leaves estimated 3246423 word corpus (74.9% of prior 4332554)\n",
      "2017-05-12 19:43:20,405 : INFO : estimated required memory for 24015 words and 600 dimensions: 127279500 bytes\n",
      "2017-05-12 19:43:20,530 : INFO : resetting layer weights\n",
      "2017-05-12 19:43:21,017 : INFO : training model with 4 workers on 24015 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:43:21,018 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:43:22,025 : INFO : PROGRESS: at 1.14% examples, 185705 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:23,043 : INFO : PROGRESS: at 2.38% examples, 191462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:24,053 : INFO : PROGRESS: at 3.65% examples, 196207 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:25,080 : INFO : PROGRESS: at 4.93% examples, 197798 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:26,103 : INFO : PROGRESS: at 6.17% examples, 197516 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:27,125 : INFO : PROGRESS: at 7.31% examples, 194863 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:28,213 : INFO : PROGRESS: at 8.59% examples, 194313 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:29,222 : INFO : PROGRESS: at 9.88% examples, 195763 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:30,263 : INFO : PROGRESS: at 11.11% examples, 195401 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:31,272 : INFO : PROGRESS: at 12.39% examples, 196467 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:32,308 : INFO : PROGRESS: at 13.58% examples, 195584 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:33,312 : INFO : PROGRESS: at 14.72% examples, 194714 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:43:34,337 : INFO : PROGRESS: at 15.96% examples, 194793 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:35,363 : INFO : PROGRESS: at 17.25% examples, 195378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:36,387 : INFO : PROGRESS: at 18.49% examples, 195423 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:37,427 : INFO : PROGRESS: at 19.77% examples, 195702 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:38,471 : INFO : PROGRESS: at 20.96% examples, 195067 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:39,479 : INFO : PROGRESS: at 22.15% examples, 194855 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:40,507 : INFO : PROGRESS: at 23.34% examples, 194508 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:41,515 : INFO : PROGRESS: at 24.48% examples, 194003 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:42,532 : INFO : PROGRESS: at 25.62% examples, 193459 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:43,536 : INFO : PROGRESS: at 26.76% examples, 193099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:44,556 : INFO : PROGRESS: at 28.05% examples, 193557 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:45,566 : INFO : PROGRESS: at 29.19% examples, 193155 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:43:46,571 : INFO : PROGRESS: at 30.48% examples, 193707 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:47,575 : INFO : PROGRESS: at 31.80% examples, 194497 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:48,600 : INFO : PROGRESS: at 32.99% examples, 194275 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:49,624 : INFO : PROGRESS: at 34.31% examples, 194845 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:50,725 : INFO : PROGRESS: at 35.55% examples, 194380 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:51,751 : INFO : PROGRESS: at 36.79% examples, 194424 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:52,752 : INFO : PROGRESS: at 37.89% examples, 193913 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:53,757 : INFO : PROGRESS: at 39.09% examples, 193851 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:54,814 : INFO : PROGRESS: at 40.38% examples, 193941 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:43:55,821 : INFO : PROGRESS: at 41.56% examples, 193889 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:56,832 : INFO : PROGRESS: at 42.79% examples, 194018 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:57,855 : INFO : PROGRESS: at 43.98% examples, 193874 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:43:58,868 : INFO : PROGRESS: at 45.17% examples, 193787 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:43:59,952 : INFO : PROGRESS: at 46.45% examples, 193743 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:44:00,963 : INFO : PROGRESS: at 47.86% examples, 194601 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:01,985 : INFO : PROGRESS: at 49.24% examples, 195185 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:03,017 : INFO : PROGRESS: at 50.48% examples, 195173 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:04,065 : INFO : PROGRESS: at 51.76% examples, 195257 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:05,083 : INFO : PROGRESS: at 53.00% examples, 195295 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:06,147 : INFO : PROGRESS: at 54.27% examples, 195298 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:07,175 : INFO : PROGRESS: at 55.37% examples, 194810 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:44:08,175 : INFO : PROGRESS: at 56.56% examples, 194775 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:09,191 : INFO : PROGRESS: at 57.66% examples, 194369 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:44:10,215 : INFO : PROGRESS: at 58.77% examples, 193939 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:11,297 : INFO : PROGRESS: at 60.10% examples, 194053 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:12,382 : INFO : PROGRESS: at 61.42% examples, 194154 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:13,423 : INFO : PROGRESS: at 62.75% examples, 194413 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:44:14,428 : INFO : PROGRESS: at 63.98% examples, 194506 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:15,486 : INFO : PROGRESS: at 65.31% examples, 194679 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:16,494 : INFO : PROGRESS: at 66.45% examples, 194497 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:17,499 : INFO : PROGRESS: at 67.68% examples, 194586 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:18,506 : INFO : PROGRESS: at 68.88% examples, 194539 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:19,573 : INFO : PROGRESS: at 70.12% examples, 194421 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:20,636 : INFO : PROGRESS: at 71.35% examples, 194318 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:21,693 : INFO : PROGRESS: at 72.59% examples, 194240 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:22,705 : INFO : PROGRESS: at 73.91% examples, 194547 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:23,705 : INFO : PROGRESS: at 75.09% examples, 194520 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:24,734 : INFO : PROGRESS: at 76.29% examples, 194413 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:25,766 : INFO : PROGRESS: at 77.62% examples, 194641 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:26,780 : INFO : PROGRESS: at 78.91% examples, 194797 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:27,800 : INFO : PROGRESS: at 80.19% examples, 194938 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:28,839 : INFO : PROGRESS: at 81.52% examples, 195131 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:29,903 : INFO : PROGRESS: at 82.84% examples, 195250 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:30,916 : INFO : PROGRESS: at 84.12% examples, 195393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:31,940 : INFO : PROGRESS: at 85.49% examples, 195711 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:32,971 : INFO : PROGRESS: at 86.82% examples, 195905 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:33,993 : INFO : PROGRESS: at 88.19% examples, 196209 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:35,005 : INFO : PROGRESS: at 89.47% examples, 196337 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:36,019 : INFO : PROGRESS: at 90.76% examples, 196460 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:37,042 : INFO : PROGRESS: at 92.04% examples, 196555 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:38,067 : INFO : PROGRESS: at 93.27% examples, 196541 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:39,104 : INFO : PROGRESS: at 94.55% examples, 196597 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:44:40,166 : INFO : PROGRESS: at 95.88% examples, 196685 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:41,192 : INFO : PROGRESS: at 97.16% examples, 196766 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:42,206 : INFO : PROGRESS: at 98.41% examples, 196779 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:44:43,277 : INFO : PROGRESS: at 99.78% examples, 196921 words/s, in_qsize 5, out_qsize 0\n",
      "2017-05-12 19:44:43,304 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:44:43,330 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:44:43,346 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:44:43,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:44:43,369 : INFO : training on 21819630 raw words (16232354 effective words) took 82.3s, 197127 effective words/s\n",
      "2017-05-12 19:44:43,370 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:44:43,571 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model, separately None\n",
      "2017-05-12 19:44:43,571 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:44:43,572 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model.wv.syn0.npy\n",
      "2017-05-12 19:44:43,617 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:44:43,618 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model.syn1neg.npy\n",
      "2017-05-12 19:44:43,729 : INFO : saved ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model\n",
      "2017-05-12 19:44:44,026 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:44:44,027 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:44:44,080 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 19:44:44,082 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:44:44,110 : INFO : min_count=4 retains 4197 unique words (32% of original 12999, drops 8802)\n",
      "2017-05-12 19:44:44,110 : INFO : min_count=4 leaves 175884 word corpus (93% of original 188627, drops 12743)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:44:44,125 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 19:44:44,127 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 19:44:44,127 : INFO : downsampling leaves estimated 127490 word corpus (72.5% of prior 175884)\n",
      "2017-05-12 19:44:44,128 : INFO : estimated required memory for 4197 words and 600 dimensions: 22244100 bytes\n",
      "2017-05-12 19:44:44,148 : INFO : resetting layer weights\n",
      "2017-05-12 19:44:44,232 : INFO : training model with 4 workers on 4197 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:44:44,236 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:44:45,268 : INFO : PROGRESS: at 45.50% examples, 282538 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:46,310 : INFO : PROGRESS: at 91.06% examples, 280390 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:46,448 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:44:46,463 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:44:46,468 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:44:46,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:44:46,496 : INFO : training on 943135 raw words (637080 effective words) took 2.3s, 282514 effective words/s\n",
      "2017-05-12 19:44:46,497 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:44:46,537 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_4_model, separately None\n",
      "2017-05-12 19:44:46,538 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:44:46,539 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:44:46,751 : INFO : saved ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:44:47,392 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:44:47,393 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:44:47,470 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 19:44:47,535 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:44:47,599 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 19:44:47,653 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 19:44:47,654 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:44:47,696 : INFO : min_count=4 retains 9205 unique words (40% of original 22824, drops 13619)\n",
      "2017-05-12 19:44:47,697 : INFO : min_count=4 leaves 884424 word corpus (97% of original 904687, drops 20263)\n",
      "2017-05-12 19:44:47,727 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 19:44:47,729 : INFO : sample=0.001 downsamples 39 most-common words\n",
      "2017-05-12 19:44:47,730 : INFO : downsampling leaves estimated 649835 word corpus (73.5% of prior 884424)\n",
      "2017-05-12 19:44:47,730 : INFO : estimated required memory for 9205 words and 600 dimensions: 48786500 bytes\n",
      "2017-05-12 19:44:47,780 : INFO : resetting layer weights\n",
      "2017-05-12 19:44:47,985 : INFO : training model with 4 workers on 9205 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:44:47,986 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:44:48,997 : INFO : PROGRESS: at 6.64% examples, 214206 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:50,022 : INFO : PROGRESS: at 13.92% examples, 222855 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:51,029 : INFO : PROGRESS: at 21.41% examples, 229191 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:52,056 : INFO : PROGRESS: at 28.29% examples, 225906 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:44:53,064 : INFO : PROGRESS: at 35.78% examples, 229174 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:54,069 : INFO : PROGRESS: at 43.27% examples, 231367 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:55,131 : INFO : PROGRESS: at 50.57% examples, 230098 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:56,153 : INFO : PROGRESS: at 58.07% examples, 231167 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:44:57,154 : INFO : PROGRESS: at 65.12% examples, 230931 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:44:58,162 : INFO : PROGRESS: at 72.42% examples, 231356 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:44:59,200 : INFO : PROGRESS: at 80.14% examples, 232357 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:00,270 : INFO : PROGRESS: at 87.89% examples, 232526 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:01,316 : INFO : PROGRESS: at 95.83% examples, 233661 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:01,814 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:45:01,818 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:45:01,854 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:45:01,864 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:45:01,865 : INFO : training on 4523435 raw words (3249259 effective words) took 13.9s, 234228 effective words/s\n",
      "2017-05-12 19:45:01,866 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:45:01,942 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_4_model, separately None\n",
      "2017-05-12 19:45:01,943 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:45:01,944 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:45:02,378 : INFO : saved ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:45:05,189 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:45:05,192 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:45:05,247 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 19:45:05,301 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 19:45:05,361 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:45:05,424 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 19:45:05,480 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 19:45:05,534 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 19:45:05,593 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 19:45:05,647 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 19:45:05,704 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 19:45:05,760 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 19:45:05,814 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 19:45:05,870 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 19:45:05,925 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 19:45:05,980 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 19:45:06,039 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 19:45:06,097 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 19:45:06,154 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 19:45:06,214 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 19:45:06,273 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 19:45:06,306 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 19:45:06,307 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:45:06,374 : INFO : min_count=4 retains 20437 unique words (41% of original 48808, drops 28371)\n",
      "2017-05-12 19:45:06,376 : INFO : min_count=4 leaves 4321820 word corpus (99% of original 4363926, drops 42106)\n",
      "2017-05-12 19:45:06,436 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 19:45:06,439 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 19:45:06,439 : INFO : downsampling leaves estimated 3234747 word corpus (74.8% of prior 4321820)\n",
      "2017-05-12 19:45:06,440 : INFO : estimated required memory for 20437 words and 600 dimensions: 108316100 bytes\n",
      "2017-05-12 19:45:06,542 : INFO : resetting layer weights\n",
      "2017-05-12 19:45:06,911 : INFO : training model with 4 workers on 20437 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:45:06,912 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:45:07,949 : INFO : PROGRESS: at 1.14% examples, 179795 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:08,994 : INFO : PROGRESS: at 2.47% examples, 192719 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:45:10,007 : INFO : PROGRESS: at 3.74% examples, 196620 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:11,023 : INFO : PROGRESS: at 4.98% examples, 196631 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:12,051 : INFO : PROGRESS: at 6.26% examples, 197705 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:13,115 : INFO : PROGRESS: at 7.49% examples, 196023 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:14,134 : INFO : PROGRESS: at 8.68% examples, 194979 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:15,183 : INFO : PROGRESS: at 9.97% examples, 195353 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:16,208 : INFO : PROGRESS: at 11.20% examples, 195305 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:17,277 : INFO : PROGRESS: at 12.53% examples, 195892 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:18,330 : INFO : PROGRESS: at 13.85% examples, 196644 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:19,403 : INFO : PROGRESS: at 15.23% examples, 197541 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:45:20,444 : INFO : PROGRESS: at 16.56% examples, 198211 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:21,445 : INFO : PROGRESS: at 17.75% examples, 197809 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:45:22,459 : INFO : PROGRESS: at 19.04% examples, 198226 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:23,463 : INFO : PROGRESS: at 20.28% examples, 198265 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:24,546 : INFO : PROGRESS: at 21.60% examples, 198262 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:25,557 : INFO : PROGRESS: at 22.84% examples, 198224 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:26,606 : INFO : PROGRESS: at 24.11% examples, 198188 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:27,646 : INFO : PROGRESS: at 25.49% examples, 198953 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:28,675 : INFO : PROGRESS: at 26.72% examples, 198729 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:29,710 : INFO : PROGRESS: at 28.00% examples, 198798 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:30,710 : INFO : PROGRESS: at 29.24% examples, 198832 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:31,733 : INFO : PROGRESS: at 30.48% examples, 198702 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:45:32,782 : INFO : PROGRESS: at 31.80% examples, 198933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:33,804 : INFO : PROGRESS: at 33.08% examples, 199077 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:34,824 : INFO : PROGRESS: at 34.31% examples, 198967 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:35,829 : INFO : PROGRESS: at 35.60% examples, 199221 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:36,885 : INFO : PROGRESS: at 36.93% examples, 199369 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:37,903 : INFO : PROGRESS: at 38.22% examples, 199508 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:38,963 : INFO : PROGRESS: at 39.59% examples, 199831 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:39,977 : INFO : PROGRESS: at 40.92% examples, 200204 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:40,986 : INFO : PROGRESS: at 42.20% examples, 200358 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:42,004 : INFO : PROGRESS: at 43.43% examples, 200242 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:45:43,042 : INFO : PROGRESS: at 44.76% examples, 200420 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:44,045 : INFO : PROGRESS: at 46.04% examples, 200584 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:45,070 : INFO : PROGRESS: at 47.36% examples, 200830 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:46,083 : INFO : PROGRESS: at 48.55% examples, 200548 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:45:47,090 : INFO : PROGRESS: at 49.74% examples, 200311 words/s, in_qsize 7, out_qsize 2\n",
      "2017-05-12 19:45:48,108 : INFO : PROGRESS: at 51.03% examples, 200391 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:49,172 : INFO : PROGRESS: at 52.36% examples, 200428 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:50,176 : INFO : PROGRESS: at 53.68% examples, 200740 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:51,192 : INFO : PROGRESS: at 54.91% examples, 200653 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:52,217 : INFO : PROGRESS: at 56.20% examples, 200682 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:53,281 : INFO : PROGRESS: at 57.57% examples, 200877 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:54,295 : INFO : PROGRESS: at 58.91% examples, 201106 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:55,330 : INFO : PROGRESS: at 60.19% examples, 201086 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:56,336 : INFO : PROGRESS: at 61.47% examples, 201193 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:57,342 : INFO : PROGRESS: at 62.75% examples, 201293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:45:58,370 : INFO : PROGRESS: at 64.07% examples, 201442 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:45:59,406 : INFO : PROGRESS: at 65.35% examples, 201414 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:00,481 : INFO : PROGRESS: at 66.77% examples, 201668 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:01,550 : INFO : PROGRESS: at 68.14% examples, 201785 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:02,556 : INFO : PROGRESS: at 69.43% examples, 201856 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:03,566 : INFO : PROGRESS: at 70.71% examples, 201925 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:04,574 : INFO : PROGRESS: at 71.99% examples, 201984 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:05,614 : INFO : PROGRESS: at 73.27% examples, 201939 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:46:06,626 : INFO : PROGRESS: at 74.50% examples, 201862 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:07,626 : INFO : PROGRESS: at 75.83% examples, 202074 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:08,633 : INFO : PROGRESS: at 77.07% examples, 202020 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:09,649 : INFO : PROGRESS: at 78.31% examples, 201934 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:10,696 : INFO : PROGRESS: at 79.60% examples, 201868 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:11,716 : INFO : PROGRESS: at 80.88% examples, 201883 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:12,735 : INFO : PROGRESS: at 82.25% examples, 202132 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:13,776 : INFO : PROGRESS: at 83.43% examples, 201869 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:14,802 : INFO : PROGRESS: at 84.76% examples, 201976 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:15,812 : INFO : PROGRESS: at 85.99% examples, 201917 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:16,843 : INFO : PROGRESS: at 87.32% examples, 202013 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:17,860 : INFO : PROGRESS: at 88.60% examples, 202038 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:18,865 : INFO : PROGRESS: at 89.89% examples, 202097 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:19,874 : INFO : PROGRESS: at 91.26% examples, 202347 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:20,883 : INFO : PROGRESS: at 92.45% examples, 202190 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:21,889 : INFO : PROGRESS: at 93.73% examples, 202243 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:22,896 : INFO : PROGRESS: at 95.10% examples, 202480 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:23,903 : INFO : PROGRESS: at 96.25% examples, 202239 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:46:24,964 : INFO : PROGRESS: at 97.54% examples, 202149 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:26,001 : INFO : PROGRESS: at 98.87% examples, 202210 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:26,723 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:46:26,759 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:46:26,784 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:46:26,791 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:46:26,794 : INFO : training on 21819630 raw words (16173929 effective words) took 79.9s, 202490 effective words/s\n",
      "2017-05-12 19:46:26,795 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:46:27,006 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model, separately None\n",
      "2017-05-12 19:46:27,010 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:46:27,011 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model.wv.syn0.npy\n",
      "2017-05-12 19:46:27,049 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:46:27,050 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model.syn1neg.npy\n",
      "2017-05-12 19:46:27,151 : INFO : saved ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model\n",
      "2017-05-12 19:46:27,478 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:46:27,479 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:46:27,536 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 19:46:27,537 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:46:27,556 : INFO : min_count=5 retains 3553 unique words (27% of original 12999, drops 9446)\n",
      "2017-05-12 19:46:27,561 : INFO : min_count=5 leaves 173308 word corpus (91% of original 188627, drops 15319)\n",
      "2017-05-12 19:46:27,572 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 19:46:27,578 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-05-12 19:46:27,580 : INFO : downsampling leaves estimated 124688 word corpus (71.9% of prior 173308)\n",
      "2017-05-12 19:46:27,581 : INFO : estimated required memory for 3553 words and 600 dimensions: 18830900 bytes\n",
      "2017-05-12 19:46:27,598 : INFO : resetting layer weights\n",
      "2017-05-12 19:46:27,682 : INFO : training model with 4 workers on 3553 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:46:27,683 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:46:28,700 : INFO : PROGRESS: at 40.26% examples, 248018 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:29,729 : INFO : PROGRESS: at 86.80% examples, 265161 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:29,951 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:46:29,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:46:29,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:46:30,003 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:46:30,004 : INFO : training on 943135 raw words (623505 effective words) took 2.3s, 269376 effective words/s\n",
      "2017-05-12 19:46:30,005 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:46:30,036 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_5_model, separately None\n",
      "2017-05-12 19:46:30,037 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:46:30,038 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:46:30,216 : INFO : saved ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:46:30,818 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:46:30,819 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:46:30,887 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 19:46:30,950 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:46:31,019 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 19:46:31,074 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 19:46:31,075 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:46:31,109 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-12 19:46:31,110 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-12 19:46:31,136 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 19:46:31,138 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 19:46:31,139 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-12 19:46:31,139 : INFO : estimated required memory for 8096 words and 600 dimensions: 42908800 bytes\n",
      "2017-05-12 19:46:31,190 : INFO : resetting layer weights\n",
      "2017-05-12 19:46:31,356 : INFO : training model with 4 workers on 8096 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:46:31,357 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:46:32,439 : INFO : PROGRESS: at 7.30% examples, 218248 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:33,445 : INFO : PROGRESS: at 14.81% examples, 229513 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:34,445 : INFO : PROGRESS: at 22.06% examples, 231201 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:35,456 : INFO : PROGRESS: at 29.83% examples, 234959 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:46:36,473 : INFO : PROGRESS: at 37.55% examples, 237006 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:37,477 : INFO : PROGRESS: at 45.25% examples, 238785 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:38,478 : INFO : PROGRESS: at 52.77% examples, 239243 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:39,505 : INFO : PROGRESS: at 60.93% examples, 241406 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:46:40,515 : INFO : PROGRESS: at 68.69% examples, 241947 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:41,522 : INFO : PROGRESS: at 76.40% examples, 242532 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:42,534 : INFO : PROGRESS: at 83.02% examples, 239673 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:46:43,542 : INFO : PROGRESS: at 90.31% examples, 239124 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:44,577 : INFO : PROGRESS: at 97.37% examples, 237633 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:44,824 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:46:44,853 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:46:44,872 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:46:44,886 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:46:44,886 : INFO : training on 4523435 raw words (3224806 effective words) took 13.5s, 238494 effective words/s\n",
      "2017-05-12 19:46:44,887 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:46:44,963 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_5_model, separately None\n",
      "2017-05-12 19:46:44,966 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:46:44,967 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:46:45,571 : INFO : saved ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:46:48,854 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:46:48,855 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:46:48,920 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 19:46:48,986 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 19:46:49,053 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:46:49,115 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 19:46:49,178 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 19:46:49,242 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 19:46:49,307 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 19:46:49,370 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 19:46:49,430 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 19:46:49,491 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 19:46:49,558 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 19:46:49,625 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 19:46:49,691 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 19:46:49,754 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 19:46:49,821 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 19:46:49,884 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 19:46:49,946 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 19:46:50,004 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 19:46:50,072 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 19:46:50,114 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 19:46:50,115 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:46:50,179 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-12 19:46:50,180 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-12 19:46:50,233 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 19:46:50,239 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 19:46:50,240 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-12 19:46:50,240 : INFO : estimated required memory for 18056 words and 600 dimensions: 95696800 bytes\n",
      "2017-05-12 19:46:50,325 : INFO : resetting layer weights\n",
      "2017-05-12 19:46:50,658 : INFO : training model with 4 workers on 18056 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:46:50,660 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:46:51,673 : INFO : PROGRESS: at 1.14% examples, 184220 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:46:52,703 : INFO : PROGRESS: at 2.33% examples, 185187 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:53,716 : INFO : PROGRESS: at 3.56% examples, 188994 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:54,721 : INFO : PROGRESS: at 4.75% examples, 189399 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:55,751 : INFO : PROGRESS: at 5.89% examples, 187340 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:56,756 : INFO : PROGRESS: at 7.04% examples, 186741 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:57,759 : INFO : PROGRESS: at 8.13% examples, 185232 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:58,819 : INFO : PROGRESS: at 9.37% examples, 185562 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:46:59,845 : INFO : PROGRESS: at 10.61% examples, 186541 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:00,892 : INFO : PROGRESS: at 11.84% examples, 186922 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:01,929 : INFO : PROGRESS: at 13.08% examples, 187365 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:47:02,962 : INFO : PROGRESS: at 14.45% examples, 189643 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:03,976 : INFO : PROGRESS: at 15.68% examples, 190163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:04,977 : INFO : PROGRESS: at 16.93% examples, 190784 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:06,009 : INFO : PROGRESS: at 18.12% examples, 190473 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:07,031 : INFO : PROGRESS: at 19.37% examples, 190734 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:08,076 : INFO : PROGRESS: at 20.60% examples, 190739 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:09,096 : INFO : PROGRESS: at 21.79% examples, 190597 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:10,107 : INFO : PROGRESS: at 22.97% examples, 190566 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:11,117 : INFO : PROGRESS: at 24.16% examples, 190517 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:12,167 : INFO : PROGRESS: at 25.49% examples, 191165 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:13,167 : INFO : PROGRESS: at 26.76% examples, 191871 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:14,218 : INFO : PROGRESS: at 28.05% examples, 192078 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:15,225 : INFO : PROGRESS: at 29.33% examples, 192619 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:16,225 : INFO : PROGRESS: at 30.61% examples, 193178 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:17,237 : INFO : PROGRESS: at 31.80% examples, 193050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:18,243 : INFO : PROGRESS: at 32.99% examples, 192957 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:47:19,303 : INFO : PROGRESS: at 34.36% examples, 193559 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:20,353 : INFO : PROGRESS: at 35.64% examples, 193671 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:21,375 : INFO : PROGRESS: at 36.88% examples, 193720 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:47:22,404 : INFO : PROGRESS: at 38.13% examples, 193712 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:23,449 : INFO : PROGRESS: at 39.23% examples, 192933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:24,517 : INFO : PROGRESS: at 40.42% examples, 192514 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:47:25,518 : INFO : PROGRESS: at 41.60% examples, 192487 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:26,552 : INFO : PROGRESS: at 42.84% examples, 192492 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:27,572 : INFO : PROGRESS: at 44.07% examples, 192566 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:28,573 : INFO : PROGRESS: at 45.17% examples, 192152 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:29,639 : INFO : PROGRESS: at 46.45% examples, 192198 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:30,690 : INFO : PROGRESS: at 47.68% examples, 192126 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:31,759 : INFO : PROGRESS: at 48.97% examples, 192160 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:32,802 : INFO : PROGRESS: at 50.30% examples, 192485 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:33,826 : INFO : PROGRESS: at 51.71% examples, 193214 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:34,844 : INFO : PROGRESS: at 53.04% examples, 193600 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:35,874 : INFO : PROGRESS: at 54.32% examples, 193759 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:36,898 : INFO : PROGRESS: at 55.64% examples, 194095 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:37,974 : INFO : PROGRESS: at 57.07% examples, 194520 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:39,014 : INFO : PROGRESS: at 58.36% examples, 194609 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:40,085 : INFO : PROGRESS: at 59.73% examples, 194871 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:47:41,127 : INFO : PROGRESS: at 61.06% examples, 195093 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:42,142 : INFO : PROGRESS: at 62.20% examples, 194825 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:43,166 : INFO : PROGRESS: at 63.43% examples, 194821 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:44,189 : INFO : PROGRESS: at 64.76% examples, 195092 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:45,230 : INFO : PROGRESS: at 66.04% examples, 195153 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:46,337 : INFO : PROGRESS: at 67.27% examples, 194851 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:47:47,365 : INFO : PROGRESS: at 68.55% examples, 194961 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:48,385 : INFO : PROGRESS: at 69.88% examples, 195224 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:47:49,404 : INFO : PROGRESS: at 71.03% examples, 194975 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:50,420 : INFO : PROGRESS: at 72.13% examples, 194623 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:47:51,457 : INFO : PROGRESS: at 73.36% examples, 194577 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:52,499 : INFO : PROGRESS: at 74.64% examples, 194646 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:47:53,561 : INFO : PROGRESS: at 75.97% examples, 194760 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:54,642 : INFO : PROGRESS: at 77.21% examples, 194588 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:55,662 : INFO : PROGRESS: at 78.54% examples, 194833 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:56,728 : INFO : PROGRESS: at 79.92% examples, 195038 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:47:57,729 : INFO : PROGRESS: at 81.15% examples, 195103 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:47:58,730 : INFO : PROGRESS: at 82.39% examples, 195165 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:47:59,737 : INFO : PROGRESS: at 83.52% examples, 194990 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:00,755 : INFO : PROGRESS: at 84.81% examples, 195103 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:01,836 : INFO : PROGRESS: at 85.95% examples, 194731 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:02,876 : INFO : PROGRESS: at 87.19% examples, 194689 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:48:03,963 : INFO : PROGRESS: at 88.46% examples, 194621 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:05,001 : INFO : PROGRESS: at 89.70% examples, 194581 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:06,022 : INFO : PROGRESS: at 90.99% examples, 194688 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:07,055 : INFO : PROGRESS: at 92.22% examples, 194662 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 19:48:08,144 : INFO : PROGRESS: at 93.59% examples, 194785 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:09,179 : INFO : PROGRESS: at 94.87% examples, 194852 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:10,197 : INFO : PROGRESS: at 96.16% examples, 194955 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:11,246 : INFO : PROGRESS: at 97.44% examples, 194983 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:12,334 : INFO : PROGRESS: at 98.68% examples, 194822 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:48:13,186 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:48:13,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:48:13,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:48:13,250 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:48:13,251 : INFO : training on 21819630 raw words (16121639 effective words) took 82.6s, 195221 effective words/s\n",
      "2017-05-12 19:48:13,254 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:48:13,417 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model, separately None\n",
      "2017-05-12 19:48:13,419 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:48:13,421 : INFO : storing np array 'syn0' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model.wv.syn0.npy\n",
      "2017-05-12 19:48:13,459 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:48:13,460 : INFO : storing np array 'syn1neg' to ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model.syn1neg.npy\n",
      "2017-05-12 19:48:13,555 : INFO : saved ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model\n",
      "2017-05-12 19:48:13,890 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:48:13,891 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:48:13,946 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 19:48:13,947 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:48:13,973 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 19:48:13,974 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 19:48:13,983 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 19:48:13,988 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 19:48:13,989 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 19:48:13,990 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 19:48:14,004 : INFO : resetting layer weights\n",
      "2017-05-12 19:48:14,073 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:48:14,073 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:48:15,082 : INFO : PROGRESS: at 46.58% examples, 283780 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:16,087 : INFO : PROGRESS: at 97.43% examples, 296183 words/s, in_qsize 3, out_qsize 1\n",
      "2017-05-12 19:48:16,093 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:48:16,102 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:48:16,107 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:48:16,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:48:16,123 : INFO : training on 943135 raw words (610202 effective words) took 2.0s, 298722 effective words/s\n",
      "2017-05-12 19:48:16,124 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:48:16,148 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_6_model, separately None\n",
      "2017-05-12 19:48:16,149 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:48:16,152 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:48:16,302 : INFO : saved ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:48:16,901 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:48:16,901 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:48:16,968 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 19:48:17,037 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:48:17,102 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 19:48:17,159 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 19:48:17,160 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:48:17,190 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 19:48:17,191 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 19:48:17,212 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 19:48:17,214 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 19:48:17,216 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 19:48:17,217 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 19:48:17,251 : INFO : resetting layer weights\n",
      "2017-05-12 19:48:17,385 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:48:17,386 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:48:18,430 : INFO : PROGRESS: at 7.75% examples, 238120 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:19,461 : INFO : PROGRESS: at 15.69% examples, 242498 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:48:20,480 : INFO : PROGRESS: at 23.62% examples, 244915 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:21,509 : INFO : PROGRESS: at 31.35% examples, 243854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:22,524 : INFO : PROGRESS: at 39.30% examples, 245275 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:23,547 : INFO : PROGRESS: at 47.25% examples, 245762 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:24,564 : INFO : PROGRESS: at 55.43% examples, 247437 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:25,588 : INFO : PROGRESS: at 62.92% examples, 245850 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:26,601 : INFO : PROGRESS: at 71.31% examples, 247976 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:27,612 : INFO : PROGRESS: at 78.82% examples, 246971 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:28,669 : INFO : PROGRESS: at 86.99% examples, 247004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:29,687 : INFO : PROGRESS: at 94.72% examples, 246716 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:30,247 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:48:30,298 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:48:30,313 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:48:30,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:48:30,331 : INFO : training on 4523435 raw words (3202959 effective words) took 12.9s, 247548 effective words/s\n",
      "2017-05-12 19:48:30,332 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:48:30,402 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_6_model, separately None\n",
      "2017-05-12 19:48:30,403 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:48:30,404 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:48:30,819 : INFO : saved ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:48:34,002 : INFO : collecting all words and their counts\n",
      "2017-05-12 19:48:34,004 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 19:48:34,068 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 19:48:34,128 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 19:48:34,197 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:48:34,264 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 19:48:34,322 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 19:48:34,388 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 19:48:34,469 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 19:48:34,536 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 19:48:34,601 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 19:48:34,669 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 19:48:34,735 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 19:48:34,797 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 19:48:34,859 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 19:48:34,923 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 19:48:34,994 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 19:48:35,058 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 19:48:35,122 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 19:48:35,186 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 19:48:35,249 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 19:48:35,286 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 19:48:35,287 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 19:48:35,357 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 19:48:35,358 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 19:48:35,408 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 19:48:35,411 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 19:48:35,416 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 19:48:35,418 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 19:48:35,495 : INFO : resetting layer weights\n",
      "2017-05-12 19:48:35,794 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 19:48:35,797 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 19:48:36,811 : INFO : PROGRESS: at 1.23% examples, 198783 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:37,854 : INFO : PROGRESS: at 2.47% examples, 194720 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:38,900 : INFO : PROGRESS: at 3.74% examples, 195401 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:39,929 : INFO : PROGRESS: at 5.07% examples, 198362 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:40,940 : INFO : PROGRESS: at 6.30% examples, 198004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:41,943 : INFO : PROGRESS: at 7.59% examples, 199223 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:42,985 : INFO : PROGRESS: at 8.87% examples, 198993 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:43,993 : INFO : PROGRESS: at 10.06% examples, 197867 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:45,031 : INFO : PROGRESS: at 11.30% examples, 197137 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:46,063 : INFO : PROGRESS: at 12.49% examples, 195944 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:47,121 : INFO : PROGRESS: at 13.72% examples, 195203 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:48,135 : INFO : PROGRESS: at 14.95% examples, 195261 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:49,157 : INFO : PROGRESS: at 16.28% examples, 196276 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:48:50,204 : INFO : PROGRESS: at 17.66% examples, 197357 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:51,209 : INFO : PROGRESS: at 18.81% examples, 196399 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:48:52,233 : INFO : PROGRESS: at 20.14% examples, 197136 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:53,237 : INFO : PROGRESS: at 21.42% examples, 197610 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:54,245 : INFO : PROGRESS: at 22.75% examples, 198389 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:48:55,256 : INFO : PROGRESS: at 24.07% examples, 199027 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:56,261 : INFO : PROGRESS: at 25.35% examples, 199315 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:48:57,288 : INFO : PROGRESS: at 26.63% examples, 199370 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:48:58,367 : INFO : PROGRESS: at 27.95% examples, 199291 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:48:59,382 : INFO : PROGRESS: at 29.28% examples, 199749 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:00,467 : INFO : PROGRESS: at 30.66% examples, 199935 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:49:01,504 : INFO : PROGRESS: at 32.03% examples, 200454 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:49:02,507 : INFO : PROGRESS: at 33.31% examples, 200642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:03,509 : INFO : PROGRESS: at 34.59% examples, 200825 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:04,539 : INFO : PROGRESS: at 35.96% examples, 201313 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:05,545 : INFO : PROGRESS: at 37.29% examples, 201681 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:06,569 : INFO : PROGRESS: at 38.63% examples, 201896 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:07,579 : INFO : PROGRESS: at 39.96% examples, 202199 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:08,660 : INFO : PROGRESS: at 41.33% examples, 202274 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:09,682 : INFO : PROGRESS: at 42.66% examples, 202469 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 19:49:10,734 : INFO : PROGRESS: at 43.98% examples, 202475 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:11,736 : INFO : PROGRESS: at 45.30% examples, 202777 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:12,763 : INFO : PROGRESS: at 46.63% examples, 202911 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:13,779 : INFO : PROGRESS: at 47.91% examples, 202913 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:14,826 : INFO : PROGRESS: at 49.19% examples, 202746 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:15,860 : INFO : PROGRESS: at 50.48% examples, 202657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:16,882 : INFO : PROGRESS: at 51.80% examples, 202812 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:17,933 : INFO : PROGRESS: at 53.18% examples, 202988 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:19,012 : INFO : PROGRESS: at 54.55% examples, 203035 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:20,062 : INFO : PROGRESS: at 55.92% examples, 203209 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:21,070 : INFO : PROGRESS: at 57.25% examples, 203401 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:22,129 : INFO : PROGRESS: at 58.63% examples, 203515 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:23,137 : INFO : PROGRESS: at 59.96% examples, 203693 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:24,186 : INFO : PROGRESS: at 61.24% examples, 203539 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:25,211 : INFO : PROGRESS: at 62.61% examples, 203786 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:26,217 : INFO : PROGRESS: at 63.84% examples, 203655 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:27,246 : INFO : PROGRESS: at 65.12% examples, 203582 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:28,278 : INFO : PROGRESS: at 66.54% examples, 203929 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:29,289 : INFO : PROGRESS: at 67.91% examples, 204200 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:30,307 : INFO : PROGRESS: at 69.20% examples, 204170 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:31,315 : INFO : PROGRESS: at 70.30% examples, 203645 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 19:49:32,337 : INFO : PROGRESS: at 71.39% examples, 203088 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:33,395 : INFO : PROGRESS: at 72.59% examples, 202669 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:34,450 : INFO : PROGRESS: at 73.81% examples, 202406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:35,459 : INFO : PROGRESS: at 75.19% examples, 202681 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 19:49:36,464 : INFO : PROGRESS: at 76.61% examples, 203088 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:37,465 : INFO : PROGRESS: at 77.99% examples, 203375 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:38,471 : INFO : PROGRESS: at 80.01% examples, 205273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:39,475 : INFO : PROGRESS: at 82.25% examples, 207703 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:40,506 : INFO : PROGRESS: at 84.63% examples, 210307 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:41,507 : INFO : PROGRESS: at 85.99% examples, 210462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:42,513 : INFO : PROGRESS: at 87.37% examples, 210601 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:43,529 : INFO : PROGRESS: at 88.60% examples, 210374 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:44,601 : INFO : PROGRESS: at 89.93% examples, 210201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:45,676 : INFO : PROGRESS: at 91.17% examples, 209811 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:46,748 : INFO : PROGRESS: at 92.40% examples, 209441 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:47,762 : INFO : PROGRESS: at 93.73% examples, 209457 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:48,768 : INFO : PROGRESS: at 94.92% examples, 209193 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 19:49:49,808 : INFO : PROGRESS: at 96.25% examples, 209134 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:50,841 : INFO : PROGRESS: at 97.58% examples, 209103 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:51,874 : INFO : PROGRESS: at 98.91% examples, 209066 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 19:49:52,593 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 19:49:52,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 19:49:52,636 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 19:49:52,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 19:49:52,638 : INFO : training on 21819630 raw words (16076529 effective words) took 76.8s, 209255 effective words/s\n",
      "2017-05-12 19:49:52,639 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 19:49:52,786 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_6_model, separately None\n",
      "2017-05-12 19:49:52,787 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 19:49:52,787 : INFO : not storing attribute cum_table\n",
      "2017-05-12 19:49:54,039 : INFO : saved ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_6_model\n"
     ]
    }
   ],
   "source": [
    "min_word_count = [1, 2, 3, 4, 5, 6]\n",
    "for i in min_word_count:\n",
    "    w2v_parameters = [600, i, 4, 6, 0.001]\n",
    "    strict = pred.make_w2v_model(dros_strict_real, 'Drosophila/models/word_count/dros_strict_word_count_parameter_' + str(i), w2v_parameters)\n",
    "    gen = pred.make_w2v_model(dros_gen_real, 'Drosophila/models/word_count/dros_gen_word_count_parameter_' + str(i), w2v_parameters)\n",
    "    be = pred.make_w2v_model(dros_be_real, 'Drosophila/models/word_count/dros_be_word_count_parameter_' + str(i), w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 19:52:05,001 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_1_model\n",
      "2017-05-12 19:52:05,542 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_1_model.wv.* with mmap=None\n",
      "2017-05-12 19:52:05,544 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 19:52:05,546 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 19:52:05,546 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_1_model\n",
      "2017-05-12 19:52:05,582 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model\n",
      "2017-05-12 19:52:05,648 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model.wv.* with mmap=None\n",
      "2017-05-12 19:52:05,651 : INFO : loading syn0 from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 19:52:05,691 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 19:52:05,692 : INFO : loading syn1neg from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 19:52:05,771 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 19:52:05,772 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_1_model\n",
      "2017-05-12 19:52:05,864 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model\n",
      "2017-05-12 19:52:06,411 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model.wv.* with mmap=None\n",
      "2017-05-12 19:52:06,413 : INFO : loading syn0 from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 19:52:06,549 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 19:52:06,550 : INFO : loading syn1neg from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 19:52:06,684 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 19:52:06,685 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_1_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:08:51,357 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:08:52,231 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-12 20:08:52,232 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:08:52,233 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:08:52,233 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_2_model\n",
      "2017-05-12 20:08:52,256 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_2_model\n",
      "2017-05-12 20:08:54,369 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-12 20:08:54,371 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:08:54,372 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:08:54,372 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_2_model\n",
      "2017-05-12 20:08:54,422 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model\n",
      "2017-05-12 20:08:54,488 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-12 20:08:54,492 : INFO : loading syn0 from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 20:08:55,452 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:08:55,454 : INFO : loading syn1neg from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 20:08:56,374 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:08:56,375 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:19:36,813 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:19:37,821 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-12 20:19:37,822 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:19:37,823 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:19:37,824 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_3_model\n",
      "2017-05-12 20:19:37,845 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_3_model\n",
      "2017-05-12 20:19:39,930 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-12 20:19:39,932 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:19:39,933 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:19:39,934 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_3_model\n",
      "2017-05-12 20:19:39,978 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model\n",
      "2017-05-12 20:19:40,070 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-12 20:19:40,073 : INFO : loading syn0 from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 20:19:41,072 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:19:41,074 : INFO : loading syn1neg from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 20:19:42,092 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:19:42,093 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:24:59,825 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_4_model\n",
      "2017-05-12 20:25:00,494 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-12 20:25:00,494 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:25:00,495 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:25:00,496 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_4_model\n",
      "2017-05-12 20:25:00,508 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_4_model\n",
      "2017-05-12 20:25:01,982 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-12 20:25:01,983 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:25:01,984 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:25:01,984 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_4_model\n",
      "2017-05-12 20:25:02,007 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model\n",
      "2017-05-12 20:25:02,081 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-12 20:25:02,083 : INFO : loading syn0 from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 20:25:03,014 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:25:03,015 : INFO : loading syn1neg from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 20:25:04,541 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:25:04,553 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:29:42,815 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:29:43,457 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-12 20:29:43,458 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:29:43,459 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:29:43,460 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_5_model\n",
      "2017-05-12 20:29:43,474 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_5_model\n",
      "2017-05-12 20:29:44,890 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-12 20:29:44,892 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:29:44,892 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:29:44,893 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_5_model\n",
      "2017-05-12 20:29:44,916 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model\n",
      "2017-05-12 20:29:44,992 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-12 20:29:44,994 : INFO : loading syn0 from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model.wv.syn0.npy with mmap=None\n",
      "2017-05-12 20:29:45,828 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:29:45,829 : INFO : loading syn1neg from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model.syn1neg.npy with mmap=None\n",
      "2017-05-12 20:29:46,717 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:29:46,717 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:34:24,607 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:34:25,080 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-12 20:34:25,081 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:34:25,081 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:34:25,082 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_6_model\n",
      "2017-05-12 20:34:25,089 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_6_model\n",
      "2017-05-12 20:34:26,434 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-12 20:34:26,435 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:34:26,435 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:34:26,436 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_6_model\n",
      "2017-05-12 20:34:26,455 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_6_model\n",
      "2017-05-12 20:34:29,073 : INFO : loading wv recursively from ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-12 20:34:29,075 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:34:29,076 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:34:29,076 : INFO : loaded ../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    }
   ],
   "source": [
    "for i in min_word_count:\n",
    "    w2w_model_strict = word2vec.Word2Vec.load('../../Results/Drosophila/models/word_count/dros_strict_word_count_parameter_'+str(i)+'_model')\n",
    "    w2w_model_gen = word2vec.Word2Vec.load('../../Results/Drosophila/models/word_count/dros_gen_word_count_parameter_'+str(i)+'_model')\n",
    "    w2w_model_be = word2vec.Word2Vec.load('../../Results/Drosophila/models/word_count/dros_be_word_count_parameter_'+str(i)+'_model')\n",
    "    \n",
    "    for seed in random_seeds:\n",
    "        data_name = '../../Results/Drosophila/train_val/dros_tr_val_split_' + str(seed)\n",
    "        train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "        train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "        validation_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "        validation_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_strict, feature_count=600)\n",
    "        \n",
    "        strict_list_SR_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_gen, feature_count=600)\n",
    "        \n",
    "        strict_list_GEN_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                      train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_be, feature_count=600)\n",
    "        \n",
    "        strict_list_BE_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        pickle.dump(strict_list_SR_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/word_count/dros_strict_list_SR_word_count_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_GEN_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/word_count/dros_strict_list_GEN_word_count_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_BE_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/word_count/dros_strict_list_BE_word_count_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        \n",
    "        strict_final_list = [strict_list_SR_dims_param, \n",
    "                             strict_list_GEN_dims_param, \n",
    "                             strict_list_BE_dims_param]\n",
    "        print ('\\nPredicting\\n')\n",
    "        errors = []\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "\n",
    "        for entry in strict_final_list:\n",
    "            error_w2v_norm, probs_w2v_norm = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                                 entry[2], entry[3])\n",
    "            fpr_w2v_norm, tpr_w2v_norm, _ = roc_curve(entry[3], probs_w2v_norm)\n",
    "            error_w2v_fs, probs_w2v_fs = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                             entry[2], entry[3],\n",
    "                                                             feature_selection=True)\n",
    "            fpr_w2v_fs, tpr_w2v_fs, _ = roc_curve(entry[3], probs_w2v_fs)\n",
    "\n",
    "            errors.append([error_w2v_norm, error_w2v_fs])\n",
    "            fpr.append([fpr_w2v_norm, fpr_w2v_fs])\n",
    "            tpr.append([tpr_w2v_norm, tpr_w2v_fs])\n",
    "            \n",
    "        pickle.dump(errors, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/word_count/dros_word_count_param'+str(i)+'_errors_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(fpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/word_count/dros_word_count_param'+str(i)+'_fpr_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(tpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/word_count/dros_word_count_param'+str(i)+'_tpr_pickle_'+str(seed)+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_parameter = []\n",
    "for seed in random_seeds:\n",
    "    mean_err_dros_strict = []\n",
    "    mean_auc_dros_strict = []\n",
    "    mean_err_dros_gen = []\n",
    "    mean_auc_dros_gen = []\n",
    "    mean_err_dros_be = []\n",
    "    mean_auc_dros_be = []\n",
    "    for i in min_word_count:\n",
    "        drct = '../../Results/Drosophila/error_fpr_tpr/w2v_param_search/word_count/'\n",
    "        errors_dros = mult_open(drct, str(i)+'_errors_pickle_'+str(seed))\n",
    "        fpr_dros = mult_open(drct, str(i)+'_fpr_pickle_'+str(seed))\n",
    "        tpr_dros = mult_open(drct, str(i)+'_tpr_pickle_'+str(seed))\n",
    "        for e, f, t in zip(errors_dros, fpr_dros, tpr_dros):\n",
    "            input_list = [[e, f, t]]\n",
    "            name_list = ['drosophila']\n",
    "            for idx in range(3):\n",
    "                for item, name in zip(input_list, name_list):\n",
    "                    for fpr_item, tpr_item, error_item in zip(item[1][idx], item[2][idx], item[0][idx]):\n",
    "                        roc_auc = auc(fpr_item, tpr_item)\n",
    "                        auc_val = '%.3f' % roc_auc\n",
    "                        error = '%.3f' % error_item\n",
    "                        if idx == 0:\n",
    "                            mean_err_dros_strict.append(error_item)\n",
    "                            mean_auc_dros_strict.append(roc_auc)\n",
    "                        elif idx == 1:\n",
    "                            mean_err_dros_gen.append(error_item)\n",
    "                            mean_auc_dros_gen.append(roc_auc)\n",
    "                        elif idx == 2:\n",
    "                            mean_err_dros_be.append(error_item)\n",
    "                            mean_auc_dros_be.append(roc_auc)\n",
    "\n",
    "    mean_err_dros_org_strict = mean_err_dros_strict[0::2]\n",
    "    mean_err_dros_fs_strict = mean_err_dros_strict[1::2]\n",
    "    mean_auc_dros_org_strict = mean_auc_dros_strict[0::2]\n",
    "    mean_auc_dros_fs_strict = mean_auc_dros_strict[1::2]\n",
    "\n",
    "    mean_err_dros_org_gen = mean_err_dros_gen[0::2]\n",
    "    mean_err_dros_fs_gen = mean_err_dros_gen[1::2]\n",
    "    mean_auc_dros_org_gen = mean_auc_dros_gen[0::2]\n",
    "    mean_auc_dros_fs_gen = mean_auc_dros_gen[1::2]\n",
    "\n",
    "    mean_err_dros_org_be = mean_err_dros_be[0::2]\n",
    "    mean_err_dros_fs_be = mean_err_dros_be[1::2]\n",
    "    mean_auc_dros_org_be = mean_auc_dros_be[0::2]\n",
    "    mean_auc_dros_fs_be = mean_auc_dros_be[1::2]\n",
    "    \n",
    "    org_error = max([(mean_err_dros_org_strict.index(min(mean_err_dros_org_strict))+1), \n",
    "                     (mean_err_dros_org_gen.index(min(mean_err_dros_org_gen))+1), \n",
    "                     (mean_err_dros_org_be.index(min(mean_err_dros_org_be))+1)])\n",
    "    \n",
    "    fs_error = max([(mean_err_dros_fs_strict.index(min(mean_err_dros_fs_strict))+1), \n",
    "                     (mean_err_dros_fs_gen.index(min(mean_err_dros_fs_gen))+1), \n",
    "                     (mean_err_dros_fs_be.index(min(mean_err_dros_fs_be))+1)])\n",
    "    \n",
    "    org_auc = max([(mean_auc_dros_org_strict.index(max(mean_auc_dros_org_strict))+1), \n",
    "                     (mean_auc_dros_org_gen.index(max(mean_auc_dros_org_gen))+1), \n",
    "                     (mean_auc_dros_org_be.index(max(mean_auc_dros_org_be))+1)])\n",
    "    \n",
    "    fs_auc = max([(mean_auc_dros_fs_strict.index(max(mean_auc_dros_fs_strict))+1), \n",
    "                     (mean_auc_dros_fs_gen.index(max(mean_auc_dros_fs_gen))+1), \n",
    "                     (mean_auc_dros_fs_be.index(max(mean_auc_dros_fs_be))+1)])\n",
    "    \n",
    "    best_parameter.append((seed, [org_error, fs_error, org_auc, fs_auc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, [6, 2, 5, 6])\n",
      "(235, [2, 4, 6, 2])\n",
      "(905, [6, 3, 5, 5])\n",
      "(2895, [1, 4, 4, 3])\n",
      "(3462, [1, 6, 6, 5])\n",
      "(4225, [4, 5, 6, 6])\n",
      "(5056, [4, 4, 4, 4])\n",
      "(5192, [5, 2, 6, 5])\n",
      "(7751, [6, 6, 6, 6])\n",
      "(7813, [6, 4, 4, 2])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "best_overall = []\n",
    "for i in best_parameter:\n",
    "    best_overall.append(most_common(i[1]))\n",
    "    print(i)\n",
    "best_word_count = most_common(best_overall)\n",
    "print(best_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:44:15,195 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:44:15,195 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:44:15,232 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 20:44:15,233 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:44:15,249 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 20:44:15,249 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 20:44:15,256 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 20:44:15,257 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 20:44:15,258 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 20:44:15,258 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 20:44:15,265 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:44:15,316 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2017-05-12 20:44:15,317 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:44:16,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:44:16,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:44:16,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:44:16,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:44:16,200 : INFO : training on 943135 raw words (610641 effective words) took 0.9s, 695629 effective words/s\n",
      "2017-05-12 20:44:16,201 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:44:16,222 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_2_model, separately None\n",
      "2017-05-12 20:44:16,223 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:44:16,224 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:44:16,329 : INFO : saved ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:44:16,768 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:44:16,769 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:44:16,819 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 20:44:16,881 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 20:44:16,942 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:44:16,999 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 20:44:17,000 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:44:17,034 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 20:44:17,034 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 20:44:17,056 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 20:44:17,058 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 20:44:17,059 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 20:44:17,059 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 20:44:17,089 : INFO : resetting layer weights\n",
      "2017-05-12 20:44:17,213 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2017-05-12 20:44:17,214 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:44:18,221 : INFO : PROGRESS: at 12.59% examples, 402238 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:19,232 : INFO : PROGRESS: at 28.29% examples, 449471 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:20,236 : INFO : PROGRESS: at 46.37% examples, 492110 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:44:21,251 : INFO : PROGRESS: at 65.12% examples, 517184 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:22,269 : INFO : PROGRESS: at 83.01% examples, 526486 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:44:23,191 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:44:23,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:44:23,218 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:44:23,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:44:23,230 : INFO : training on 4523435 raw words (3202768 effective words) took 6.0s, 532820 effective words/s\n",
      "2017-05-12 20:44:23,231 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:44:23,279 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_2_model, separately None\n",
      "2017-05-12 20:44:23,279 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:44:23,280 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:44:23,602 : INFO : saved ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:44:25,747 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:44:25,748 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:44:25,795 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 20:44:25,838 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 20:44:25,884 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 20:44:25,928 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:44:25,970 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 20:44:26,016 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 20:44:26,055 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 20:44:26,097 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 20:44:26,140 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 20:44:26,182 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 20:44:26,224 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 20:44:26,266 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 20:44:26,309 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 20:44:26,351 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 20:44:26,393 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 20:44:26,437 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 20:44:26,481 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 20:44:26,539 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 20:44:26,597 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 20:44:26,643 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 20:44:26,644 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:44:26,702 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 20:44:26,703 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 20:44:26,746 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 20:44:26,753 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 20:44:26,754 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 20:44:26,755 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 20:44:26,825 : INFO : resetting layer weights\n",
      "2017-05-12 20:44:27,118 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=2\n",
      "2017-05-12 20:44:27,120 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:44:28,144 : INFO : PROGRESS: at 1.74% examples, 275136 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:44:29,148 : INFO : PROGRESS: at 4.11% examples, 327704 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:44:30,160 : INFO : PROGRESS: at 6.94% examples, 368777 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:31,188 : INFO : PROGRESS: at 9.65% examples, 382406 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:44:32,193 : INFO : PROGRESS: at 12.30% examples, 390725 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:44:33,208 : INFO : PROGRESS: at 14.91% examples, 394549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:34,214 : INFO : PROGRESS: at 17.75% examples, 402958 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:35,223 : INFO : PROGRESS: at 20.69% examples, 410925 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:36,235 : INFO : PROGRESS: at 23.20% examples, 409734 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:44:37,244 : INFO : PROGRESS: at 25.85% examples, 410975 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:38,264 : INFO : PROGRESS: at 28.64% examples, 413632 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:39,299 : INFO : PROGRESS: at 31.53% examples, 416549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:40,310 : INFO : PROGRESS: at 34.27% examples, 418055 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:41,320 : INFO : PROGRESS: at 36.75% examples, 416291 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:42,324 : INFO : PROGRESS: at 39.00% examples, 412503 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:44:43,339 : INFO : PROGRESS: at 41.10% examples, 407550 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:44,351 : INFO : PROGRESS: at 43.20% examples, 403300 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:44:45,356 : INFO : PROGRESS: at 45.17% examples, 398393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:46,372 : INFO : PROGRESS: at 47.46% examples, 396482 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:47,415 : INFO : PROGRESS: at 50.43% examples, 399672 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:48,441 : INFO : PROGRESS: at 53.27% examples, 401825 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:49,453 : INFO : PROGRESS: at 55.96% examples, 403074 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:50,459 : INFO : PROGRESS: at 58.72% examples, 404612 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:44:51,486 : INFO : PROGRESS: at 61.65% examples, 406897 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:52,487 : INFO : PROGRESS: at 63.98% examples, 405630 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:53,517 : INFO : PROGRESS: at 66.95% examples, 407913 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:44:54,523 : INFO : PROGRESS: at 69.75% examples, 409308 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:55,535 : INFO : PROGRESS: at 72.13% examples, 408197 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:56,555 : INFO : PROGRESS: at 74.59% examples, 407544 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:44:57,574 : INFO : PROGRESS: at 76.98% examples, 406481 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:44:58,595 : INFO : PROGRESS: at 79.60% examples, 406619 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:44:59,624 : INFO : PROGRESS: at 82.52% examples, 408237 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:00,636 : INFO : PROGRESS: at 85.45% examples, 409970 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:01,659 : INFO : PROGRESS: at 88.19% examples, 410604 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:02,660 : INFO : PROGRESS: at 91.08% examples, 412092 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:03,716 : INFO : PROGRESS: at 94.05% examples, 413274 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 20:45:04,723 : INFO : PROGRESS: at 96.89% examples, 414347 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:05,780 : INFO : PROGRESS: at 99.19% examples, 412524 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 20:45:06,109 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:45:06,119 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:45:06,126 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:45:06,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:45:06,136 : INFO : training on 21819630 raw words (16075155 effective words) took 39.0s, 412099 effective words/s\n",
      "2017-05-12 20:45:06,136 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:45:06,261 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_2_model, separately None\n",
      "2017-05-12 20:45:06,261 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:45:06,262 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:45:07,603 : INFO : saved ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_2_model\n",
      "2017-05-12 20:45:07,897 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:45:07,899 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:45:07,960 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 20:45:07,961 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:45:07,981 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 20:45:07,983 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 20:45:07,997 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 20:45:07,999 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 20:45:08,000 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 20:45:08,001 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 20:45:08,015 : INFO : resetting layer weights\n",
      "2017-05-12 20:45:08,109 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2017-05-12 20:45:08,110 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:45:09,126 : INFO : PROGRESS: at 77.33% examples, 468237 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:45:09,351 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:45:09,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:45:09,369 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:45:09,395 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:45:09,396 : INFO : training on 943135 raw words (610693 effective words) took 1.3s, 477763 effective words/s\n",
      "2017-05-12 20:45:09,397 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:45:09,442 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_3_model, separately None\n",
      "2017-05-12 20:45:09,444 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:45:09,446 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:45:09,634 : INFO : saved ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:45:10,350 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:45:10,351 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:45:10,409 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 20:45:10,461 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 20:45:10,516 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:45:10,558 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 20:45:10,559 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:45:10,592 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 20:45:10,592 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 20:45:10,613 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 20:45:10,615 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 20:45:10,615 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 20:45:10,616 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 20:45:10,645 : INFO : resetting layer weights\n",
      "2017-05-12 20:45:10,798 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2017-05-12 20:45:10,798 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:45:11,810 : INFO : PROGRESS: at 17.90% examples, 569759 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:12,811 : INFO : PROGRESS: at 34.23% examples, 546255 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:13,827 : INFO : PROGRESS: at 51.00% examples, 540392 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:14,837 : INFO : PROGRESS: at 68.69% examples, 545156 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:15,842 : INFO : PROGRESS: at 86.33% examples, 548577 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:16,568 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:45:16,589 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:45:16,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:45:16,600 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:45:16,601 : INFO : training on 4523435 raw words (3202215 effective words) took 5.8s, 552482 effective words/s\n",
      "2017-05-12 20:45:16,601 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:45:16,655 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_3_model, separately None\n",
      "2017-05-12 20:45:16,656 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:45:16,657 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:45:17,013 : INFO : saved ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:45:19,325 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:45:19,326 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:45:19,381 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 20:45:19,433 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 20:45:19,490 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:45:19,546 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 20:45:19,601 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 20:45:19,653 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 20:45:19,705 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 20:45:19,748 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 20:45:19,790 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 20:45:19,832 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 20:45:19,875 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 20:45:19,916 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 20:45:19,959 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 20:45:20,002 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 20:45:20,045 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 20:45:20,088 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 20:45:20,131 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 20:45:20,174 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 20:45:20,216 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 20:45:20,241 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 20:45:20,242 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:45:20,287 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 20:45:20,287 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 20:45:20,323 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 20:45:20,327 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 20:45:20,328 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 20:45:20,328 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 20:45:20,382 : INFO : resetting layer weights\n",
      "2017-05-12 20:45:20,634 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
      "2017-05-12 20:45:20,634 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:45:21,652 : INFO : PROGRESS: at 2.47% examples, 393304 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:22,661 : INFO : PROGRESS: at 4.84% examples, 386037 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:23,676 : INFO : PROGRESS: at 7.45% examples, 395173 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:24,683 : INFO : PROGRESS: at 9.88% examples, 393199 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:45:25,686 : INFO : PROGRESS: at 12.85% examples, 409748 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:26,694 : INFO : PROGRESS: at 15.32% examples, 407280 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:27,731 : INFO : PROGRESS: at 17.66% examples, 400600 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:45:28,740 : INFO : PROGRESS: at 20.10% examples, 398845 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:29,740 : INFO : PROGRESS: at 22.47% examples, 397107 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:30,769 : INFO : PROGRESS: at 24.62% examples, 390875 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:31,769 : INFO : PROGRESS: at 27.50% examples, 397400 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:45:32,802 : INFO : PROGRESS: at 30.48% examples, 402996 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:33,824 : INFO : PROGRESS: at 32.99% examples, 402456 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:34,844 : INFO : PROGRESS: at 35.60% examples, 403108 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:35,845 : INFO : PROGRESS: at 37.71% examples, 398827 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:36,854 : INFO : PROGRESS: at 40.37% examples, 400337 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:37,867 : INFO : PROGRESS: at 43.02% examples, 401571 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:38,878 : INFO : PROGRESS: at 45.58% examples, 401886 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:39,893 : INFO : PROGRESS: at 48.32% examples, 403636 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:40,924 : INFO : PROGRESS: at 50.98% examples, 404153 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:41,930 : INFO : PROGRESS: at 53.95% examples, 407523 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:42,937 : INFO : PROGRESS: at 56.98% examples, 410887 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:43,941 : INFO : PROGRESS: at 59.96% examples, 413695 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:44,942 : INFO : PROGRESS: at 62.20% examples, 411505 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:45:45,949 : INFO : PROGRESS: at 64.85% examples, 412012 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:46,953 : INFO : PROGRESS: at 67.46% examples, 412228 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:45:47,956 : INFO : PROGRESS: at 70.02% examples, 412159 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:45:48,973 : INFO : PROGRESS: at 72.27% examples, 410083 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:50,000 : INFO : PROGRESS: at 74.87% examples, 410031 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:45:51,003 : INFO : PROGRESS: at 77.62% examples, 411027 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:52,006 : INFO : PROGRESS: at 80.42% examples, 412185 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:45:53,022 : INFO : PROGRESS: at 83.25% examples, 413363 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:45:54,043 : INFO : PROGRESS: at 85.86% examples, 413268 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:55,048 : INFO : PROGRESS: at 88.69% examples, 414462 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:56,060 : INFO : PROGRESS: at 91.22% examples, 414053 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:57,075 : INFO : PROGRESS: at 93.95% examples, 414635 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:45:58,077 : INFO : PROGRESS: at 96.62% examples, 414938 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:59,095 : INFO : PROGRESS: at 99.15% examples, 414468 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:45:59,379 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:45:59,428 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:45:59,430 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:45:59,433 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:45:59,433 : INFO : training on 21819630 raw words (16075577 effective words) took 38.8s, 414397 effective words/s\n",
      "2017-05-12 20:45:59,434 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:45:59,552 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_3_model, separately None\n",
      "2017-05-12 20:45:59,553 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:45:59,554 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:46:00,454 : INFO : saved ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_3_model\n",
      "2017-05-12 20:46:00,653 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:46:00,654 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:46:00,694 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 20:46:00,695 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:46:00,720 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 20:46:00,721 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 20:46:00,729 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 20:46:00,730 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 20:46:00,730 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 20:46:00,731 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 20:46:00,747 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:00,806 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-05-12 20:46:00,807 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:46:01,776 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:46:01,782 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:46:01,783 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:46:01,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:46:01,787 : INFO : training on 943135 raw words (610666 effective words) took 1.0s, 627332 effective words/s\n",
      "2017-05-12 20:46:01,788 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:46:01,824 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_4_model, separately None\n",
      "2017-05-12 20:46:01,826 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:46:01,827 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:46:01,972 : INFO : saved ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:02,850 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:46:02,851 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:46:02,921 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 20:46:02,980 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 20:46:03,034 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:03,090 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 20:46:03,091 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:46:03,122 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 20:46:03,123 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 20:46:03,143 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 20:46:03,145 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 20:46:03,145 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 20:46:03,146 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 20:46:03,177 : INFO : resetting layer weights\n",
      "2017-05-12 20:46:03,299 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-05-12 20:46:03,300 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:46:04,309 : INFO : PROGRESS: at 15.91% examples, 507113 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:05,331 : INFO : PROGRESS: at 32.24% examples, 509487 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:06,350 : INFO : PROGRESS: at 45.25% examples, 475942 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:07,381 : INFO : PROGRESS: at 58.51% examples, 459651 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:08,400 : INFO : PROGRESS: at 76.40% examples, 480142 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:09,405 : INFO : PROGRESS: at 90.10% examples, 472925 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:10,124 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:46:10,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:46:10,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:46:10,155 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:46:10,156 : INFO : training on 4523435 raw words (3202673 effective words) took 6.9s, 467520 effective words/s\n",
      "2017-05-12 20:46:10,157 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:46:10,211 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_4_model, separately None\n",
      "2017-05-12 20:46:10,212 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:46:10,212 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:46:10,550 : INFO : saved ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:12,600 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:46:12,602 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:46:12,651 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 20:46:12,709 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 20:46:12,764 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:12,818 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 20:46:12,873 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 20:46:12,926 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 20:46:12,980 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 20:46:13,031 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 20:46:13,074 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 20:46:13,119 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 20:46:13,163 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 20:46:13,206 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 20:46:13,248 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 20:46:13,290 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 20:46:13,338 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 20:46:13,382 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 20:46:13,427 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 20:46:13,470 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 20:46:13,517 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 20:46:13,540 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 20:46:13,541 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:46:13,885 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 20:46:13,885 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 20:46:13,922 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 20:46:13,926 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 20:46:13,927 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 20:46:13,927 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 20:46:13,983 : INFO : resetting layer weights\n",
      "2017-05-12 20:46:14,239 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=4\n",
      "2017-05-12 20:46:14,240 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:46:15,251 : INFO : PROGRESS: at 2.70% examples, 432256 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:16,265 : INFO : PROGRESS: at 5.39% examples, 429942 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:17,273 : INFO : PROGRESS: at 8.22% examples, 437536 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:18,284 : INFO : PROGRESS: at 10.80% examples, 430131 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:19,328 : INFO : PROGRESS: at 13.54% examples, 428674 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:20,335 : INFO : PROGRESS: at 16.42% examples, 433882 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:21,341 : INFO : PROGRESS: at 19.09% examples, 432478 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:22,345 : INFO : PROGRESS: at 21.88% examples, 434269 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:23,352 : INFO : PROGRESS: at 23.97% examples, 423405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:24,366 : INFO : PROGRESS: at 26.22% examples, 416605 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:25,370 : INFO : PROGRESS: at 28.55% examples, 412734 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:26,373 : INFO : PROGRESS: at 31.07% examples, 411990 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:27,399 : INFO : PROGRESS: at 33.72% examples, 412313 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:28,416 : INFO : PROGRESS: at 36.47% examples, 413890 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 20:46:29,423 : INFO : PROGRESS: at 39.28% examples, 416025 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:30,456 : INFO : PROGRESS: at 42.02% examples, 416759 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:46:31,468 : INFO : PROGRESS: at 44.80% examples, 418328 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:32,482 : INFO : PROGRESS: at 47.64% examples, 420090 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 20:46:33,530 : INFO : PROGRESS: at 50.62% examples, 422070 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:34,566 : INFO : PROGRESS: at 53.54% examples, 423703 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:35,574 : INFO : PROGRESS: at 56.10% examples, 423024 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:36,595 : INFO : PROGRESS: at 58.96% examples, 424111 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:37,613 : INFO : PROGRESS: at 61.65% examples, 424229 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:38,620 : INFO : PROGRESS: at 64.44% examples, 425124 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:46:39,620 : INFO : PROGRESS: at 67.18% examples, 425761 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:40,631 : INFO : PROGRESS: at 69.56% examples, 423942 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:41,674 : INFO : PROGRESS: at 71.85% examples, 421231 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:42,675 : INFO : PROGRESS: at 74.22% examples, 419862 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:43,684 : INFO : PROGRESS: at 76.52% examples, 417964 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:44,734 : INFO : PROGRESS: at 79.23% examples, 417814 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:45,735 : INFO : PROGRESS: at 81.97% examples, 418552 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:46,748 : INFO : PROGRESS: at 84.63% examples, 418642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:47,749 : INFO : PROGRESS: at 87.14% examples, 418217 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:48,772 : INFO : PROGRESS: at 89.15% examples, 415200 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:49,786 : INFO : PROGRESS: at 91.67% examples, 414757 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:50,790 : INFO : PROGRESS: at 94.41% examples, 415431 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:51,795 : INFO : PROGRESS: at 97.07% examples, 415688 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:46:52,821 : INFO : PROGRESS: at 99.69% examples, 415490 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-12 20:46:52,885 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:46:52,887 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:46:52,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:46:52,900 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:46:52,901 : INFO : training on 21819630 raw words (16077503 effective words) took 38.7s, 415910 effective words/s\n",
      "2017-05-12 20:46:52,902 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:46:53,036 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_4_model, separately None\n",
      "2017-05-12 20:46:53,036 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:46:53,037 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:46:53,972 : INFO : saved ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_4_model\n",
      "2017-05-12 20:46:54,191 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:46:54,192 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:46:54,236 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 20:46:54,236 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:46:54,255 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 20:46:54,256 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 20:46:54,265 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 20:46:54,266 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 20:46:54,266 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 20:46:54,267 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 20:46:54,280 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:54,337 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-12 20:46:54,339 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:46:55,326 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:46:55,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:46:55,339 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:46:55,343 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:46:55,343 : INFO : training on 943135 raw words (610522 effective words) took 1.0s, 611480 effective words/s\n",
      "2017-05-12 20:46:55,344 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:46:55,366 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_5_model, separately None\n",
      "2017-05-12 20:46:55,367 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:46:55,367 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:46:55,486 : INFO : saved ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:55,950 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:46:55,951 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:46:55,999 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 20:46:56,045 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 20:46:56,092 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 20:46:56,130 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 20:46:56,130 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:46:56,158 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 20:46:56,159 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 20:46:56,176 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 20:46:56,177 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 20:46:56,178 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 20:46:56,179 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 20:46:56,204 : INFO : resetting layer weights\n",
      "2017-05-12 20:46:56,334 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-12 20:46:56,335 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:46:57,351 : INFO : PROGRESS: at 15.69% examples, 496396 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:46:58,353 : INFO : PROGRESS: at 33.12% examples, 526865 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:46:59,361 : INFO : PROGRESS: at 49.25% examples, 521770 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:47:00,368 : INFO : PROGRESS: at 65.56% examples, 521185 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:01,379 : INFO : PROGRESS: at 81.67% examples, 519182 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:02,386 : INFO : PROGRESS: at 97.37% examples, 515703 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:02,493 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:47:02,523 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:47:02,532 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:47:02,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:47:02,538 : INFO : training on 4523435 raw words (3202622 effective words) took 6.2s, 516746 effective words/s\n",
      "2017-05-12 20:47:02,538 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:47:02,591 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_5_model, separately None\n",
      "2017-05-12 20:47:02,592 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:47:02,592 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:47:02,942 : INFO : saved ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:47:05,095 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:47:05,096 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:47:05,141 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 20:47:05,185 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 20:47:05,231 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n",
      "2017-05-12 20:47:05,273 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:47:05,317 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 20:47:05,363 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 20:47:05,407 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 20:47:05,452 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 20:47:05,497 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 20:47:05,543 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 20:47:05,590 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 20:47:05,637 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 20:47:05,684 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 20:47:05,730 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 20:47:05,778 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 20:47:05,827 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 20:47:05,874 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 20:47:05,927 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 20:47:05,974 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 20:47:06,002 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 20:47:06,003 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:47:06,364 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 20:47:06,364 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 20:47:06,401 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 20:47:06,404 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 20:47:06,404 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 20:47:06,405 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 20:47:06,459 : INFO : resetting layer weights\n",
      "2017-05-12 20:47:06,782 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-05-12 20:47:06,782 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:47:07,790 : INFO : PROGRESS: at 2.38% examples, 381657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:08,801 : INFO : PROGRESS: at 4.15% examples, 332436 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:09,809 : INFO : PROGRESS: at 5.98% examples, 318905 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:47:10,817 : INFO : PROGRESS: at 7.95% examples, 317599 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:11,818 : INFO : PROGRESS: at 9.97% examples, 318697 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:12,834 : INFO : PROGRESS: at 12.12% examples, 322305 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:13,901 : INFO : PROGRESS: at 14.13% examples, 319519 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:14,926 : INFO : PROGRESS: at 16.00% examples, 316344 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:15,936 : INFO : PROGRESS: at 17.94% examples, 315230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:16,954 : INFO : PROGRESS: at 19.77% examples, 312590 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:17,976 : INFO : PROGRESS: at 21.65% examples, 311018 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:19,045 : INFO : PROGRESS: at 23.61% examples, 309716 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:20,064 : INFO : PROGRESS: at 25.62% examples, 310294 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:21,081 : INFO : PROGRESS: at 27.68% examples, 311384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:22,107 : INFO : PROGRESS: at 29.74% examples, 312129 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:23,170 : INFO : PROGRESS: at 31.85% examples, 312532 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:24,202 : INFO : PROGRESS: at 33.99% examples, 313880 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:25,239 : INFO : PROGRESS: at 36.10% examples, 314573 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:47:26,254 : INFO : PROGRESS: at 38.08% examples, 314418 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:47:27,285 : INFO : PROGRESS: at 39.87% examples, 312601 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:28,306 : INFO : PROGRESS: at 41.51% examples, 310100 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:29,315 : INFO : PROGRESS: at 43.15% examples, 307991 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:30,318 : INFO : PROGRESS: at 44.89% examples, 306730 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:47:31,338 : INFO : PROGRESS: at 46.86% examples, 306893 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:32,345 : INFO : PROGRESS: at 48.74% examples, 306594 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:33,376 : INFO : PROGRESS: at 50.71% examples, 306603 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:34,377 : INFO : PROGRESS: at 52.72% examples, 307191 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:35,423 : INFO : PROGRESS: at 54.77% examples, 307543 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:36,453 : INFO : PROGRESS: at 56.88% examples, 308273 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:37,469 : INFO : PROGRESS: at 58.96% examples, 308854 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:38,471 : INFO : PROGRESS: at 60.92% examples, 309075 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:39,477 : INFO : PROGRESS: at 62.98% examples, 309695 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:40,519 : INFO : PROGRESS: at 64.99% examples, 309727 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:41,521 : INFO : PROGRESS: at 67.09% examples, 310549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:42,545 : INFO : PROGRESS: at 69.06% examples, 310494 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:43,558 : INFO : PROGRESS: at 71.08% examples, 310755 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:44,569 : INFO : PROGRESS: at 73.09% examples, 311006 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:45,594 : INFO : PROGRESS: at 75.19% examples, 311514 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:46,596 : INFO : PROGRESS: at 77.25% examples, 311991 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:47,607 : INFO : PROGRESS: at 79.19% examples, 311828 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:47:48,615 : INFO : PROGRESS: at 81.15% examples, 311877 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:49,616 : INFO : PROGRESS: at 83.21% examples, 312329 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:50,636 : INFO : PROGRESS: at 85.31% examples, 312772 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:51,638 : INFO : PROGRESS: at 87.32% examples, 313003 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:47:52,655 : INFO : PROGRESS: at 89.47% examples, 313594 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:53,699 : INFO : PROGRESS: at 91.44% examples, 313365 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:47:54,740 : INFO : PROGRESS: at 93.46% examples, 313309 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:55,747 : INFO : PROGRESS: at 95.56% examples, 313784 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:56,771 : INFO : PROGRESS: at 97.44% examples, 313397 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:47:57,778 : INFO : PROGRESS: at 99.60% examples, 313980 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:47:57,909 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:47:57,912 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:47:57,937 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:47:57,944 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:47:57,944 : INFO : training on 21819630 raw words (16073995 effective words) took 51.2s, 314211 effective words/s\n",
      "2017-05-12 20:47:57,945 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:47:58,080 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_5_model, separately None\n",
      "2017-05-12 20:47:58,081 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:47:58,081 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:47:59,040 : INFO : saved ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_5_model\n",
      "2017-05-12 20:47:59,272 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:47:59,273 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:47:59,316 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 20:47:59,318 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:47:59,338 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 20:47:59,339 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 20:47:59,348 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 20:47:59,350 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 20:47:59,351 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 20:47:59,351 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 20:47:59,365 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:47:59,423 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 20:47:59,424 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:48:00,434 : INFO : PROGRESS: at 80.46% examples, 489214 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:00,630 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:48:00,652 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:48:00,654 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:48:00,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:48:00,663 : INFO : training on 943135 raw words (610651 effective words) took 1.2s, 495313 effective words/s\n",
      "2017-05-12 20:48:00,664 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:48:00,686 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_6_model, separately None\n",
      "2017-05-12 20:48:00,687 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:48:00,688 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:48:00,809 : INFO : saved ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:48:01,347 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:48:01,348 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:48:01,403 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 20:48:01,455 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 20:48:01,511 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:48:01,566 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 20:48:01,567 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:48:01,594 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 20:48:01,595 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 20:48:01,625 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 20:48:01,629 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 20:48:01,629 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 20:48:01,630 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 20:48:01,670 : INFO : resetting layer weights\n",
      "2017-05-12 20:48:01,836 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 20:48:01,837 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:48:02,877 : INFO : PROGRESS: at 11.05% examples, 342456 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:48:03,888 : INFO : PROGRESS: at 22.52% examples, 352822 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:04,907 : INFO : PROGRESS: at 33.34% examples, 348573 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:05,911 : INFO : PROGRESS: at 44.81% examples, 352746 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:06,950 : INFO : PROGRESS: at 54.98% examples, 344750 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:07,960 : INFO : PROGRESS: at 64.24% examples, 336347 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:08,978 : INFO : PROGRESS: at 73.53% examples, 330005 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:10,018 : INFO : PROGRESS: at 82.35% examples, 322603 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:11,041 : INFO : PROGRESS: at 93.17% examples, 324378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:11,545 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:48:11,570 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:48:11,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:48:11,593 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:48:11,595 : INFO : training on 4523435 raw words (3202241 effective words) took 9.8s, 328414 effective words/s\n",
      "2017-05-12 20:48:11,596 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:48:11,648 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_6_model, separately None\n",
      "2017-05-12 20:48:11,650 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:48:11,650 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:48:12,030 : INFO : saved ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:48:14,205 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:48:14,206 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:48:14,263 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 20:48:14,316 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 20:48:14,373 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:48:14,431 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 20:48:14,476 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 20:48:14,528 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 20:48:14,574 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 20:48:14,622 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 20:48:14,673 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 20:48:14,719 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 20:48:14,768 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 20:48:14,833 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 20:48:14,880 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 20:48:14,931 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 20:48:14,990 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 20:48:15,038 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 20:48:15,085 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 20:48:15,134 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 20:48:15,179 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 20:48:15,205 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 20:48:15,206 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:48:15,260 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 20:48:15,261 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 20:48:15,301 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 20:48:15,304 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 20:48:15,305 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 20:48:15,305 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 20:48:15,372 : INFO : resetting layer weights\n",
      "2017-05-12 20:48:15,650 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-12 20:48:15,651 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:48:16,662 : INFO : PROGRESS: at 2.01% examples, 322078 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:17,680 : INFO : PROGRESS: at 4.11% examples, 327310 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:18,682 : INFO : PROGRESS: at 5.98% examples, 318588 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:19,702 : INFO : PROGRESS: at 8.09% examples, 321928 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:20,735 : INFO : PROGRESS: at 10.29% examples, 325976 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:21,747 : INFO : PROGRESS: at 12.30% examples, 324939 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:22,772 : INFO : PROGRESS: at 14.22% examples, 321616 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:48:23,802 : INFO : PROGRESS: at 16.15% examples, 318888 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:24,804 : INFO : PROGRESS: at 18.03% examples, 316956 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:25,810 : INFO : PROGRESS: at 19.77% examples, 313051 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:26,860 : INFO : PROGRESS: at 21.56% examples, 309354 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:27,888 : INFO : PROGRESS: at 23.52% examples, 309181 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:28,914 : INFO : PROGRESS: at 25.71% examples, 311901 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:29,919 : INFO : PROGRESS: at 27.91% examples, 314663 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:30,924 : INFO : PROGRESS: at 30.06% examples, 316602 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:31,926 : INFO : PROGRESS: at 32.26% examples, 318810 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:32,946 : INFO : PROGRESS: at 34.40% examples, 320008 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:33,955 : INFO : PROGRESS: at 36.52% examples, 320854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:35,001 : INFO : PROGRESS: at 38.45% examples, 319472 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:36,007 : INFO : PROGRESS: at 40.28% examples, 318147 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:37,019 : INFO : PROGRESS: at 41.88% examples, 315141 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:38,063 : INFO : PROGRESS: at 43.52% examples, 312293 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:39,079 : INFO : PROGRESS: at 45.07% examples, 309412 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:48:40,096 : INFO : PROGRESS: at 46.81% examples, 307989 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:41,125 : INFO : PROGRESS: at 48.74% examples, 307657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:42,127 : INFO : PROGRESS: at 50.66% examples, 307694 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:43,139 : INFO : PROGRESS: at 52.72% examples, 308406 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:44,141 : INFO : PROGRESS: at 54.59% examples, 308138 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:48:45,164 : INFO : PROGRESS: at 56.52% examples, 307932 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:46,165 : INFO : PROGRESS: at 58.54% examples, 308443 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:47,173 : INFO : PROGRESS: at 60.47% examples, 308379 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:48,193 : INFO : PROGRESS: at 62.48% examples, 308676 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:49,210 : INFO : PROGRESS: at 64.35% examples, 308310 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:50,240 : INFO : PROGRESS: at 66.31% examples, 308284 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:51,246 : INFO : PROGRESS: at 68.23% examples, 308255 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:48:52,261 : INFO : PROGRESS: at 70.30% examples, 308758 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:53,283 : INFO : PROGRESS: at 72.49% examples, 309755 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:54,301 : INFO : PROGRESS: at 74.59% examples, 310353 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:55,326 : INFO : PROGRESS: at 76.61% examples, 310497 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:56,341 : INFO : PROGRESS: at 78.68% examples, 310890 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:57,372 : INFO : PROGRESS: at 80.88% examples, 311671 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:58,373 : INFO : PROGRESS: at 82.89% examples, 311947 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:48:59,407 : INFO : PROGRESS: at 85.08% examples, 312649 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:00,436 : INFO : PROGRESS: at 87.14% examples, 312857 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:01,445 : INFO : PROGRESS: at 89.29% examples, 313508 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:02,482 : INFO : PROGRESS: at 91.53% examples, 314269 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:03,545 : INFO : PROGRESS: at 93.64% examples, 314358 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:04,568 : INFO : PROGRESS: at 95.79% examples, 314855 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:05,569 : INFO : PROGRESS: at 97.95% examples, 315474 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:06,435 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:49:06,485 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:49:06,494 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:49:06,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:49:06,504 : INFO : training on 21819630 raw words (16074711 effective words) took 50.8s, 316132 effective words/s\n",
      "2017-05-12 20:49:06,505 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:49:06,621 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_6_model, separately None\n",
      "2017-05-12 20:49:06,622 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:49:06,623 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:49:07,470 : INFO : saved ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:49:07,903 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:49:07,903 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:49:07,949 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-12 20:49:07,950 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:49:07,967 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-12 20:49:07,967 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-12 20:49:07,975 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-12 20:49:07,976 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-12 20:49:07,978 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-12 20:49:07,978 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-12 20:49:07,993 : INFO : resetting layer weights\n",
      "2017-05-12 20:49:08,051 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-05-12 20:49:08,052 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:49:09,057 : INFO : PROGRESS: at 74.20% examples, 452252 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:49:09,356 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:49:09,359 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:49:09,378 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:49:09,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:49:09,384 : INFO : training on 943135 raw words (610349 effective words) took 1.3s, 459954 effective words/s\n",
      "2017-05-12 20:49:09,385 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:49:09,413 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_7_model, separately None\n",
      "2017-05-12 20:49:09,415 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:49:09,417 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:49:09,543 : INFO : saved ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_7_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:49:10,050 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:49:10,051 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:49:10,107 : INFO : PROGRESS: at sentence #10000, processed 240061 words, keeping 13294 word types\n",
      "2017-05-12 20:49:10,159 : INFO : PROGRESS: at sentence #20000, processed 479451 words, keeping 17608 word types\n",
      "2017-05-12 20:49:10,210 : INFO : PROGRESS: at sentence #30000, processed 719251 words, keeping 20837 word types\n",
      "2017-05-12 20:49:10,249 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-12 20:49:10,250 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:49:10,278 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-12 20:49:10,278 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-12 20:49:10,299 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-12 20:49:10,300 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-12 20:49:10,301 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-12 20:49:10,302 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-12 20:49:10,333 : INFO : resetting layer weights\n",
      "2017-05-12 20:49:10,463 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-05-12 20:49:10,464 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:49:11,471 : INFO : PROGRESS: at 11.93% examples, 381044 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:12,505 : INFO : PROGRESS: at 24.28% examples, 381916 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:13,535 : INFO : PROGRESS: at 36.00% examples, 375948 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:14,579 : INFO : PROGRESS: at 48.37% examples, 376674 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:15,585 : INFO : PROGRESS: at 61.14% examples, 382861 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:16,610 : INFO : PROGRESS: at 73.53% examples, 383357 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:17,612 : INFO : PROGRESS: at 86.33% examples, 386927 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:18,651 : INFO : PROGRESS: at 98.69% examples, 386327 words/s, in_qsize 5, out_qsize 1\n",
      "2017-05-12 20:49:18,675 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:49:18,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:49:18,737 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:49:18,738 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:49:18,739 : INFO : training on 4523435 raw words (3202839 effective words) took 8.3s, 387290 effective words/s\n",
      "2017-05-12 20:49:18,740 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:49:18,836 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_7_model, separately None\n",
      "2017-05-12 20:49:18,838 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:49:18,839 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:49:19,304 : INFO : saved ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_7_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:49:23,290 : INFO : collecting all words and their counts\n",
      "2017-05-12 20:49:23,293 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-12 20:49:23,351 : INFO : PROGRESS: at sentence #10000, processed 223978 words, keeping 13703 word types\n",
      "2017-05-12 20:49:23,412 : INFO : PROGRESS: at sentence #20000, processed 448086 words, keeping 18835 word types\n",
      "2017-05-12 20:49:23,473 : INFO : PROGRESS: at sentence #30000, processed 672056 words, keeping 22546 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:49:23,534 : INFO : PROGRESS: at sentence #40000, processed 896507 words, keeping 25560 word types\n",
      "2017-05-12 20:49:23,596 : INFO : PROGRESS: at sentence #50000, processed 1119720 words, keeping 28041 word types\n",
      "2017-05-12 20:49:23,656 : INFO : PROGRESS: at sentence #60000, processed 1343228 words, keeping 30340 word types\n",
      "2017-05-12 20:49:23,714 : INFO : PROGRESS: at sentence #70000, processed 1566617 words, keeping 32273 word types\n",
      "2017-05-12 20:49:23,775 : INFO : PROGRESS: at sentence #80000, processed 1790499 words, keeping 34241 word types\n",
      "2017-05-12 20:49:23,837 : INFO : PROGRESS: at sentence #90000, processed 2013749 words, keeping 35926 word types\n",
      "2017-05-12 20:49:23,884 : INFO : PROGRESS: at sentence #100000, processed 2236663 words, keeping 37438 word types\n",
      "2017-05-12 20:49:23,937 : INFO : PROGRESS: at sentence #110000, processed 2460182 words, keeping 38847 word types\n",
      "2017-05-12 20:49:23,987 : INFO : PROGRESS: at sentence #120000, processed 2683421 words, keeping 40243 word types\n",
      "2017-05-12 20:49:24,038 : INFO : PROGRESS: at sentence #130000, processed 2907227 words, keeping 41561 word types\n",
      "2017-05-12 20:49:24,085 : INFO : PROGRESS: at sentence #140000, processed 3131933 words, keeping 42850 word types\n",
      "2017-05-12 20:49:24,139 : INFO : PROGRESS: at sentence #150000, processed 3355145 words, keeping 44020 word types\n",
      "2017-05-12 20:49:24,195 : INFO : PROGRESS: at sentence #160000, processed 3577993 words, keeping 45168 word types\n",
      "2017-05-12 20:49:24,244 : INFO : PROGRESS: at sentence #170000, processed 3800487 words, keeping 46243 word types\n",
      "2017-05-12 20:49:24,298 : INFO : PROGRESS: at sentence #180000, processed 4022459 words, keeping 47272 word types\n",
      "2017-05-12 20:49:24,347 : INFO : PROGRESS: at sentence #190000, processed 4245284 words, keeping 48280 word types\n",
      "2017-05-12 20:49:24,376 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-12 20:49:24,377 : INFO : Loading a fresh vocabulary\n",
      "2017-05-12 20:49:24,435 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-12 20:49:24,436 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-12 20:49:24,479 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-12 20:49:24,482 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-12 20:49:24,483 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-12 20:49:24,484 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-12 20:49:24,554 : INFO : resetting layer weights\n",
      "2017-05-12 20:49:24,823 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-05-12 20:49:24,826 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-12 20:49:25,873 : INFO : PROGRESS: at 1.92% examples, 297150 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:26,899 : INFO : PROGRESS: at 3.83% examples, 298952 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:27,904 : INFO : PROGRESS: at 5.80% examples, 304137 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:28,938 : INFO : PROGRESS: at 7.86% examples, 308252 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:29,957 : INFO : PROGRESS: at 9.83% examples, 308630 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:49:31,009 : INFO : PROGRESS: at 11.75% examples, 306075 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:32,015 : INFO : PROGRESS: at 13.54% examples, 303179 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:33,067 : INFO : PROGRESS: at 15.46% examples, 301922 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:34,074 : INFO : PROGRESS: at 17.29% examples, 300859 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:35,088 : INFO : PROGRESS: at 19.18% examples, 300535 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-12 20:49:36,129 : INFO : PROGRESS: at 21.24% examples, 302164 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:37,141 : INFO : PROGRESS: at 23.15% examples, 302406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:38,159 : INFO : PROGRESS: at 25.17% examples, 303549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:39,168 : INFO : PROGRESS: at 27.18% examples, 304796 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:40,180 : INFO : PROGRESS: at 29.19% examples, 305800 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:49:41,195 : INFO : PROGRESS: at 31.25% examples, 307081 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-12 20:49:42,209 : INFO : PROGRESS: at 33.36% examples, 308658 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:43,220 : INFO : PROGRESS: at 35.37% examples, 309305 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:44,240 : INFO : PROGRESS: at 37.34% examples, 309369 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:45,250 : INFO : PROGRESS: at 39.37% examples, 309934 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:46,262 : INFO : PROGRESS: at 41.38% examples, 310409 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:47,276 : INFO : PROGRESS: at 43.47% examples, 311464 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:48,307 : INFO : PROGRESS: at 45.53% examples, 311883 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:49,318 : INFO : PROGRESS: at 47.64% examples, 312829 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:50,364 : INFO : PROGRESS: at 49.65% examples, 312688 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:51,373 : INFO : PROGRESS: at 51.71% examples, 313278 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:52,384 : INFO : PROGRESS: at 53.72% examples, 313547 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:53,400 : INFO : PROGRESS: at 55.78% examples, 313969 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:54,434 : INFO : PROGRESS: at 57.76% examples, 313691 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:55,480 : INFO : PROGRESS: at 59.87% examples, 314041 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:56,513 : INFO : PROGRESS: at 61.97% examples, 314489 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:49:57,536 : INFO : PROGRESS: at 64.03% examples, 314767 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:58,577 : INFO : PROGRESS: at 66.18% examples, 315299 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:49:59,578 : INFO : PROGRESS: at 68.19% examples, 315545 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:00,609 : INFO : PROGRESS: at 70.12% examples, 315090 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:01,659 : INFO : PROGRESS: at 72.08% examples, 314692 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:50:02,698 : INFO : PROGRESS: at 74.04% examples, 314422 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:03,710 : INFO : PROGRESS: at 76.01% examples, 314373 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:04,732 : INFO : PROGRESS: at 77.85% examples, 313702 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:05,768 : INFO : PROGRESS: at 79.78% examples, 313302 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:06,774 : INFO : PROGRESS: at 81.70% examples, 313159 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:50:07,789 : INFO : PROGRESS: at 83.52% examples, 312619 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:50:08,794 : INFO : PROGRESS: at 85.40% examples, 312332 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:50:09,838 : INFO : PROGRESS: at 87.32% examples, 311950 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:50:10,841 : INFO : PROGRESS: at 89.15% examples, 311543 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:11,882 : INFO : PROGRESS: at 90.62% examples, 309661 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-12 20:50:12,901 : INFO : PROGRESS: at 91.95% examples, 307532 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-12 20:50:13,912 : INFO : PROGRESS: at 93.32% examples, 305698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:14,923 : INFO : PROGRESS: at 95.10% examples, 305256 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:15,935 : INFO : PROGRESS: at 97.07% examples, 305408 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:16,939 : INFO : PROGRESS: at 98.87% examples, 305022 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-12 20:50:17,487 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-12 20:50:17,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-12 20:50:17,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-12 20:50:17,555 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-12 20:50:17,557 : INFO : training on 21819630 raw words (16075828 effective words) took 52.7s, 304891 effective words/s\n",
      "2017-05-12 20:50:17,558 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-12 20:50:17,671 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_7_model, separately None\n",
      "2017-05-12 20:50:17,672 : INFO : not storing attribute syn0norm\n",
      "2017-05-12 20:50:17,672 : INFO : not storing attribute cum_table\n",
      "2017-05-12 20:50:18,470 : INFO : saved ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_7_model\n"
     ]
    }
   ],
   "source": [
    "context_window = [2, 3, 4, 5, 6, 7]\n",
    "for i in context_window:\n",
    "    w2v_parameters = [best_dims, best_word_count, 4, i, 0.001]\n",
    "    strict = pred.make_w2v_model(dros_strict_real, 'Drosophila/models/context_window/dros_strict_context_window_parameter_' + str(i), w2v_parameters)\n",
    "    gen = pred.make_w2v_model(dros_gen_real, 'Drosophila/models/context_window/dros_gen_context_window_parameter_' + str(i), w2v_parameters)\n",
    "    be = pred.make_w2v_model(dros_be_real, 'Drosophila/models/context_window/dros_be_context_window_parameter_' + str(i), w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:50:18,639 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_2_model\n",
      "2017-05-12 20:50:18,737 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-12 20:50:18,738 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:50:18,739 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:50:18,739 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_2_model\n",
      "2017-05-12 20:50:18,748 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_2_model\n",
      "2017-05-12 20:50:18,978 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-12 20:50:18,979 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:50:18,980 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:50:18,981 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_2_model\n",
      "2017-05-12 20:50:19,004 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_2_model\n",
      "2017-05-12 20:50:19,492 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-12 20:50:19,496 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:50:19,496 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:50:19,497 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:57:19,211 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_3_model\n",
      "2017-05-12 20:57:19,306 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-12 20:57:19,307 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:57:19,307 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:57:19,308 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_3_model\n",
      "2017-05-12 20:57:19,317 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 20:57:19,562 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-12 20:57:19,564 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:57:19,564 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:57:19,565 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_3_model\n",
      "2017-05-12 20:57:19,596 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_3_model\n",
      "2017-05-12 20:57:20,279 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-12 20:57:20,282 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 20:57:20,282 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 20:57:20,283 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:03:22,779 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_4_model\n",
      "2017-05-12 21:03:22,869 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-12 21:03:22,870 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:03:22,870 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:03:22,871 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_4_model\n",
      "2017-05-12 21:03:22,878 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:03:23,093 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-12 21:03:23,094 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:03:23,095 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:03:23,096 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_4_model\n",
      "2017-05-12 21:03:23,114 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_4_model\n",
      "2017-05-12 21:03:23,635 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-12 21:03:23,637 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:03:23,637 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:03:23,638 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:10:12,650 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_5_model\n",
      "2017-05-12 21:10:12,756 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-12 21:10:12,757 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:10:12,758 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:10:12,758 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_5_model\n",
      "2017-05-12 21:10:12,767 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:10:13,025 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-12 21:10:13,026 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:10:13,027 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:10:13,028 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_5_model\n",
      "2017-05-12 21:10:13,052 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_5_model\n",
      "2017-05-12 21:10:13,826 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-12 21:10:13,829 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:10:13,829 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:10:13,830 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:15:23,004 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_6_model\n",
      "2017-05-12 21:15:23,106 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-12 21:15:23,107 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:15:23,108 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:15:23,109 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_6_model\n",
      "2017-05-12 21:15:23,118 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:15:23,368 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-12 21:15:23,369 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:15:23,370 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:15:23,370 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_6_model\n",
      "2017-05-12 21:15:23,391 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_6_model\n",
      "2017-05-12 21:15:24,019 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-12 21:15:24,021 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:15:24,021 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:15:24,022 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:19:55,608 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_7_model\n",
      "2017-05-12 21:19:55,708 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_7_model.wv.* with mmap=None\n",
      "2017-05-12 21:19:55,709 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:19:55,709 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:19:55,710 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_7_model\n",
      "2017-05-12 21:19:55,721 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_7_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-12 21:19:55,997 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_7_model.wv.* with mmap=None\n",
      "2017-05-12 21:19:55,999 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:19:55,999 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:19:56,000 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_7_model\n",
      "2017-05-12 21:19:56,025 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_7_model\n",
      "2017-05-12 21:19:56,748 : INFO : loading wv recursively from ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_7_model.wv.* with mmap=None\n",
      "2017-05-12 21:19:56,750 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-12 21:19:56,751 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-12 21:19:56,751 : INFO : loaded ../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_7_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    }
   ],
   "source": [
    "for i in context_window:\n",
    "    w2w_model_strict = word2vec.Word2Vec.load('../../Results/Drosophila/models/context_window/dros_strict_context_window_parameter_'+str(i)+'_model')\n",
    "    w2w_model_gen = word2vec.Word2Vec.load('../../Results/Drosophila/models/context_window/dros_gen_context_window_parameter_'+str(i)+'_model')\n",
    "    w2w_model_be = word2vec.Word2Vec.load('../../Results/Drosophila/models/context_window/dros_be_context_window_parameter_'+str(i)+'_model')\n",
    "    \n",
    "    for seed in random_seeds:\n",
    "        data_name = '../../Results/Drosophila/train_val/dros_tr_val_split_' + str(seed)\n",
    "        train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "        train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "        validation_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "        validation_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_strict, feature_count=600)\n",
    "        \n",
    "        strict_list_SR_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_gen, feature_count=600)\n",
    "        \n",
    "        strict_list_GEN_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                      train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_be, feature_count=600)\n",
    "        \n",
    "        strict_list_BE_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        pickle.dump(strict_list_SR_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/context_window/dros_strict_list_SR_context_window_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_GEN_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/context_window/dros_strict_list_GEN_context_window_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_BE_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/context_window/dros_strict_list_BE_context_window_param_'+str(i)+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        \n",
    "        strict_final_list = [strict_list_SR_dims_param, \n",
    "                             strict_list_GEN_dims_param, \n",
    "                             strict_list_BE_dims_param]\n",
    "        print ('\\nPredicting\\n')\n",
    "        errors = []\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "\n",
    "        for entry in strict_final_list:\n",
    "            error_w2v_norm, probs_w2v_norm = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                                 entry[2], entry[3])\n",
    "            fpr_w2v_norm, tpr_w2v_norm, _ = roc_curve(entry[3], probs_w2v_norm)\n",
    "            error_w2v_fs, probs_w2v_fs = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                             entry[2], entry[3],\n",
    "                                                             feature_selection=True)\n",
    "            fpr_w2v_fs, tpr_w2v_fs, _ = roc_curve(entry[3], probs_w2v_fs)\n",
    "\n",
    "            errors.append([error_w2v_norm, error_w2v_fs])\n",
    "            fpr.append([fpr_w2v_norm, fpr_w2v_fs])\n",
    "            tpr.append([tpr_w2v_norm, tpr_w2v_fs])\n",
    "            \n",
    "        pickle.dump(errors, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/context_window/dros_context_window_param'+str(i)+'_errors_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(fpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/context_window/dros_context_window_param'+str(i)+'_fpr_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(tpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/context_window/dros_context_window_param'+str(i)+'_tpr_pickle_'+str(seed)+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_parameter = []\n",
    "for seed in random_seeds:\n",
    "    mean_err_dros_strict = []\n",
    "    mean_auc_dros_strict = []\n",
    "    mean_err_dros_gen = []\n",
    "    mean_auc_dros_gen = []\n",
    "    mean_err_dros_be = []\n",
    "    mean_auc_dros_be = []\n",
    "    for i in context_window:\n",
    "        drct = '../../Results/Drosophila/error_fpr_tpr/w2v_param_search/context_window/'\n",
    "        errors_dros = mult_open(drct, str(i)+'_errors_pickle_'+str(seed))\n",
    "        fpr_dros = mult_open(drct, str(i)+'_fpr_pickle_'+str(seed))\n",
    "        tpr_dros = mult_open(drct, str(i)+'_tpr_pickle_'+str(seed))\n",
    "        for e, f, t in zip(errors_dros, fpr_dros, tpr_dros):\n",
    "            input_list = [[e, f, t]]\n",
    "            name_list = ['drosophila']\n",
    "            for idx in range(3):\n",
    "                for item, name in zip(input_list, name_list):\n",
    "                    for fpr_item, tpr_item, error_item in zip(item[1][idx], item[2][idx], item[0][idx]):\n",
    "                        roc_auc = auc(fpr_item, tpr_item)\n",
    "                        auc_val = '%.3f' % roc_auc\n",
    "                        error = '%.3f' % error_item\n",
    "                        if idx == 0:\n",
    "                            mean_err_dros_strict.append(error_item)\n",
    "                            mean_auc_dros_strict.append(roc_auc)\n",
    "                        elif idx == 1:\n",
    "                            mean_err_dros_gen.append(error_item)\n",
    "                            mean_auc_dros_gen.append(roc_auc)\n",
    "                        elif idx == 2:\n",
    "                            mean_err_dros_be.append(error_item)\n",
    "                            mean_auc_dros_be.append(roc_auc)\n",
    "\n",
    "    mean_err_dros_org_strict = mean_err_dros_strict[0::2]\n",
    "    mean_err_dros_fs_strict = mean_err_dros_strict[1::2]\n",
    "    mean_auc_dros_org_strict = mean_auc_dros_strict[0::2]\n",
    "    mean_auc_dros_fs_strict = mean_auc_dros_strict[1::2]\n",
    "\n",
    "    mean_err_dros_org_gen = mean_err_dros_gen[0::2]\n",
    "    mean_err_dros_fs_gen = mean_err_dros_gen[1::2]\n",
    "    mean_auc_dros_org_gen = mean_auc_dros_gen[0::2]\n",
    "    mean_auc_dros_fs_gen = mean_auc_dros_gen[1::2]\n",
    "\n",
    "    mean_err_dros_org_be = mean_err_dros_be[0::2]\n",
    "    mean_err_dros_fs_be = mean_err_dros_be[1::2]\n",
    "    mean_auc_dros_org_be = mean_auc_dros_be[0::2]\n",
    "    mean_auc_dros_fs_be = mean_auc_dros_be[1::2]\n",
    "    \n",
    "    org_error = max([(mean_err_dros_org_strict.index(min(mean_err_dros_org_strict))+1), \n",
    "                     (mean_err_dros_org_gen.index(min(mean_err_dros_org_gen))+1), \n",
    "                     (mean_err_dros_org_be.index(min(mean_err_dros_org_be))+1)])\n",
    "    \n",
    "    fs_error = max([(mean_err_dros_fs_strict.index(min(mean_err_dros_fs_strict))+1), \n",
    "                     (mean_err_dros_fs_gen.index(min(mean_err_dros_fs_gen))+1), \n",
    "                     (mean_err_dros_fs_be.index(min(mean_err_dros_fs_be))+1)])\n",
    "    \n",
    "    org_auc = max([(mean_auc_dros_org_strict.index(max(mean_auc_dros_org_strict))+1), \n",
    "                     (mean_auc_dros_org_gen.index(max(mean_auc_dros_org_gen))+1), \n",
    "                     (mean_auc_dros_org_be.index(max(mean_auc_dros_org_be))+1)])\n",
    "    \n",
    "    fs_auc = max([(mean_auc_dros_fs_strict.index(max(mean_auc_dros_fs_strict))+1), \n",
    "                     (mean_auc_dros_fs_gen.index(max(mean_auc_dros_fs_gen))+1), \n",
    "                     (mean_auc_dros_fs_be.index(max(mean_auc_dros_fs_be))+1)])\n",
    "    \n",
    "    best_parameter.append((seed, [org_error, fs_error, org_auc, fs_auc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, [1, 6, 6, 3])\n",
      "(235, [1, 5, 6, 6])\n",
      "(905, [3, 4, 6, 5])\n",
      "(2895, [4, 1, 5, 3])\n",
      "(3462, [5, 5, 4, 4])\n",
      "(4225, [6, 5, 5, 5])\n",
      "(5056, [5, 6, 6, 5])\n",
      "(5192, [5, 4, 2, 2])\n",
      "(7751, [4, 6, 6, 6])\n",
      "(7813, [4, 6, 3, 4])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "best_overall = []\n",
    "for i in best_parameter:\n",
    "    best_overall.append(most_common(i[1]))\n",
    "    print(i)\n",
    "best_context_window = most_common(best_overall)\n",
    "print(best_context_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:00:03,120 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:00:03,121 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:00:03,154 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-13 11:00:03,155 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:00:03,169 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-13 11:00:03,170 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-13 11:00:03,177 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-13 11:00:03,178 : INFO : sample=0.1 downsamples 0 most-common words\n",
      "2017-05-13 11:00:03,178 : INFO : downsampling leaves estimated 170913 word corpus (100.0% of prior 170913)\n",
      "2017-05-13 11:00:03,179 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-13 11:00:03,188 : INFO : resetting layer weights\n",
      "2017-05-13 11:00:03,235 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.1 negative=5 window=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:00:03,236 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:00:04,263 : INFO : PROGRESS: at 75.09% examples, 628871 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:04,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:00:04,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:00:04,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:00:04,585 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:00:04,586 : INFO : training on 943135 raw words (854565 effective words) took 1.3s, 635647 effective words/s\n",
      "2017-05-13 11:00:04,587 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:00:04,606 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_1_model, separately None\n",
      "2017-05-13 11:00:04,607 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:00:04,607 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:00:04,712 : INFO : saved ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_1_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:00:05,181 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:00:05,182 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:00:05,238 : INFO : PROGRESS: at sentence #10000, processed 240504 words, keeping 13182 word types\n",
      "2017-05-13 11:00:05,291 : INFO : PROGRESS: at sentence #20000, processed 479354 words, keeping 17779 word types\n",
      "2017-05-13 11:00:05,337 : INFO : PROGRESS: at sentence #30000, processed 719840 words, keeping 20864 word types\n",
      "2017-05-13 11:00:05,374 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-13 11:00:05,375 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:00:05,402 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-13 11:00:05,403 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-13 11:00:05,423 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-13 11:00:05,425 : INFO : sample=0.1 downsamples 0 most-common words\n",
      "2017-05-13 11:00:05,425 : INFO : downsampling leaves estimated 875943 word corpus (100.0% of prior 875943)\n",
      "2017-05-13 11:00:05,426 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-13 11:00:05,457 : INFO : resetting layer weights\n",
      "2017-05-13 11:00:05,572 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.1 negative=5 window=6\n",
      "2017-05-13 11:00:05,572 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:00:06,590 : INFO : PROGRESS: at 10.38% examples, 450530 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:07,598 : INFO : PROGRESS: at 22.49% examples, 489116 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:00:08,602 : INFO : PROGRESS: at 35.77% examples, 518546 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:09,615 : INFO : PROGRESS: at 48.34% examples, 525038 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:10,648 : INFO : PROGRESS: at 60.49% examples, 522972 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:11,667 : INFO : PROGRESS: at 73.08% examples, 525866 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:00:12,668 : INFO : PROGRESS: at 85.43% examples, 528024 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:13,676 : INFO : PROGRESS: at 97.59% examples, 528018 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:13,802 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:00:13,830 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:00:13,858 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:00:13,861 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:00:13,862 : INFO : training on 4523435 raw words (4379715 effective words) took 8.3s, 528949 effective words/s\n",
      "2017-05-13 11:00:13,862 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:00:13,912 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_1_model, separately None\n",
      "2017-05-13 11:00:13,913 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:00:13,913 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:00:14,224 : INFO : saved ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_1_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:00:16,540 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:00:16,541 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:00:16,586 : INFO : PROGRESS: at sentence #10000, processed 225415 words, keeping 13684 word types\n",
      "2017-05-13 11:00:16,629 : INFO : PROGRESS: at sentence #20000, processed 449496 words, keeping 18758 word types\n",
      "2017-05-13 11:00:16,677 : INFO : PROGRESS: at sentence #30000, processed 672246 words, keeping 22544 word types\n",
      "2017-05-13 11:00:16,718 : INFO : PROGRESS: at sentence #40000, processed 895570 words, keeping 25577 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:00:16,761 : INFO : PROGRESS: at sentence #50000, processed 1119209 words, keeping 28211 word types\n",
      "2017-05-13 11:00:16,804 : INFO : PROGRESS: at sentence #60000, processed 1341936 words, keeping 30318 word types\n",
      "2017-05-13 11:00:16,847 : INFO : PROGRESS: at sentence #70000, processed 1563772 words, keeping 32389 word types\n",
      "2017-05-13 11:00:16,890 : INFO : PROGRESS: at sentence #80000, processed 1787418 words, keeping 34236 word types\n",
      "2017-05-13 11:00:16,932 : INFO : PROGRESS: at sentence #90000, processed 2009809 words, keeping 35914 word types\n",
      "2017-05-13 11:00:16,975 : INFO : PROGRESS: at sentence #100000, processed 2233773 words, keeping 37463 word types\n",
      "2017-05-13 11:00:17,016 : INFO : PROGRESS: at sentence #110000, processed 2457504 words, keeping 38901 word types\n",
      "2017-05-13 11:00:17,058 : INFO : PROGRESS: at sentence #120000, processed 2679065 words, keeping 40246 word types\n",
      "2017-05-13 11:00:17,100 : INFO : PROGRESS: at sentence #130000, processed 2905623 words, keeping 41592 word types\n",
      "2017-05-13 11:00:17,143 : INFO : PROGRESS: at sentence #140000, processed 3127929 words, keeping 42809 word types\n",
      "2017-05-13 11:00:17,186 : INFO : PROGRESS: at sentence #150000, processed 3352349 words, keeping 43963 word types\n",
      "2017-05-13 11:00:17,227 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 45063 word types\n",
      "2017-05-13 11:00:17,268 : INFO : PROGRESS: at sentence #170000, processed 3799509 words, keeping 46159 word types\n",
      "2017-05-13 11:00:17,310 : INFO : PROGRESS: at sentence #180000, processed 4023821 words, keeping 47240 word types\n",
      "2017-05-13 11:00:17,354 : INFO : PROGRESS: at sentence #190000, processed 4246576 words, keeping 48288 word types\n",
      "2017-05-13 11:00:17,376 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-13 11:00:17,376 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:00:17,423 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-13 11:00:17,424 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-13 11:00:17,456 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-13 11:00:17,459 : INFO : sample=0.1 downsamples 0 most-common words\n",
      "2017-05-13 11:00:17,460 : INFO : downsampling leaves estimated 4303761 word corpus (100.0% of prior 4303761)\n",
      "2017-05-13 11:00:17,460 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-13 11:00:17,521 : INFO : resetting layer weights\n",
      "2017-05-13 11:00:17,767 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.1 negative=5 window=6\n",
      "2017-05-13 11:00:17,767 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:00:18,792 : INFO : PROGRESS: at 2.23% examples, 473691 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:19,798 : INFO : PROGRESS: at 4.52% examples, 481309 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:20,824 : INFO : PROGRESS: at 6.91% examples, 487393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:21,842 : INFO : PROGRESS: at 9.34% examples, 493627 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:22,881 : INFO : PROGRESS: at 11.68% examples, 491593 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:23,905 : INFO : PROGRESS: at 13.41% examples, 470581 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:24,926 : INFO : PROGRESS: at 15.56% examples, 468132 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:25,926 : INFO : PROGRESS: at 17.43% examples, 460248 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:26,934 : INFO : PROGRESS: at 19.03% examples, 447234 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:27,955 : INFO : PROGRESS: at 20.46% examples, 432362 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:00:28,986 : INFO : PROGRESS: at 22.42% examples, 430384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:30,004 : INFO : PROGRESS: at 24.66% examples, 434009 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:31,021 : INFO : PROGRESS: at 27.05% examples, 439390 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:32,023 : INFO : PROGRESS: at 29.30% examples, 442338 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:00:33,025 : INFO : PROGRESS: at 31.64% examples, 446239 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:34,035 : INFO : PROGRESS: at 33.87% examples, 448186 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:35,042 : INFO : PROGRESS: at 36.07% examples, 449434 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:36,044 : INFO : PROGRESS: at 37.93% examples, 446877 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:37,046 : INFO : PROGRESS: at 40.09% examples, 447671 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:38,090 : INFO : PROGRESS: at 42.01% examples, 445026 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:39,098 : INFO : PROGRESS: at 44.11% examples, 445233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:40,116 : INFO : PROGRESS: at 46.04% examples, 443457 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:41,149 : INFO : PROGRESS: at 47.92% examples, 441137 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:42,170 : INFO : PROGRESS: at 49.76% examples, 438822 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:43,202 : INFO : PROGRESS: at 51.68% examples, 437274 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:44,206 : INFO : PROGRESS: at 53.32% examples, 434084 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:45,210 : INFO : PROGRESS: at 55.06% examples, 431853 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:46,213 : INFO : PROGRESS: at 56.71% examples, 429085 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:47,215 : INFO : PROGRESS: at 58.86% examples, 430201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:48,216 : INFO : PROGRESS: at 61.19% examples, 432556 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:49,222 : INFO : PROGRESS: at 63.56% examples, 435015 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:50,274 : INFO : PROGRESS: at 65.53% examples, 433967 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:51,297 : INFO : PROGRESS: at 67.79% examples, 435124 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:52,316 : INFO : PROGRESS: at 69.95% examples, 435685 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:53,338 : INFO : PROGRESS: at 72.24% examples, 437016 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:54,362 : INFO : PROGRESS: at 74.43% examples, 437710 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:55,387 : INFO : PROGRESS: at 76.71% examples, 438876 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:56,395 : INFO : PROGRESS: at 79.00% examples, 440176 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:00:57,398 : INFO : PROGRESS: at 80.96% examples, 439716 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:58,414 : INFO : PROGRESS: at 82.88% examples, 438914 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:00:59,431 : INFO : PROGRESS: at 84.53% examples, 436697 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:00,471 : INFO : PROGRESS: at 86.37% examples, 435290 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:01:01,526 : INFO : PROGRESS: at 88.21% examples, 433802 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-13 11:01:02,574 : INFO : PROGRESS: at 89.86% examples, 431563 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:03,579 : INFO : PROGRESS: at 91.74% examples, 430916 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:01:04,588 : INFO : PROGRESS: at 93.93% examples, 431729 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:05,606 : INFO : PROGRESS: at 96.21% examples, 432836 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:06,633 : INFO : PROGRESS: at 98.54% examples, 434024 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:07,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:01:07,260 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:01:07,263 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:01:07,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:01:07,277 : INFO : training on 21819630 raw words (21518805 effective words) took 49.5s, 434677 effective words/s\n",
      "2017-05-13 11:01:07,278 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:01:07,403 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_1_model, separately None\n",
      "2017-05-13 11:01:07,404 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:01:07,405 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:01:08,427 : INFO : saved ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_1_model\n",
      "2017-05-13 11:01:08,647 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:01:08,648 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:01:08,689 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-13 11:01:08,690 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:01:08,703 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-13 11:01:08,705 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-13 11:01:08,715 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-13 11:01:08,717 : INFO : sample=0.01 downsamples 4 most-common words\n",
      "2017-05-13 11:01:08,718 : INFO : downsampling leaves estimated 157712 word corpus (92.3% of prior 170913)\n",
      "2017-05-13 11:01:08,718 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-13 11:01:08,731 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:01:08,797 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.01 negative=5 window=6\n",
      "2017-05-13 11:01:08,798 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:01:09,841 : INFO : PROGRESS: at 60.40% examples, 459583 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:10,240 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:01:10,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:01:10,259 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:01:10,297 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:01:10,298 : INFO : training on 943135 raw words (788633 effective words) took 1.5s, 528454 effective words/s\n",
      "2017-05-13 11:01:10,299 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:01:10,336 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_2_model, separately None\n",
      "2017-05-13 11:01:10,338 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:01:10,338 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:01:10,516 : INFO : saved ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:01:11,420 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:01:11,423 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:01:11,513 : INFO : PROGRESS: at sentence #10000, processed 240504 words, keeping 13182 word types\n",
      "2017-05-13 11:01:11,603 : INFO : PROGRESS: at sentence #20000, processed 479354 words, keeping 17779 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:01:11,693 : INFO : PROGRESS: at sentence #30000, processed 719840 words, keeping 20864 word types\n",
      "2017-05-13 11:01:11,771 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-13 11:01:11,774 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:01:11,815 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-13 11:01:11,816 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-13 11:01:11,847 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-13 11:01:11,850 : INFO : sample=0.01 downsamples 4 most-common words\n",
      "2017-05-13 11:01:11,852 : INFO : downsampling leaves estimated 815309 word corpus (93.1% of prior 875943)\n",
      "2017-05-13 11:01:11,854 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-13 11:01:11,897 : INFO : resetting layer weights\n",
      "2017-05-13 11:01:12,132 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.01 negative=5 window=6\n",
      "2017-05-13 11:01:12,134 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:01:13,142 : INFO : PROGRESS: at 7.94% examples, 323887 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:14,173 : INFO : PROGRESS: at 15.90% examples, 318867 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:01:15,181 : INFO : PROGRESS: at 28.92% examples, 387766 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:16,183 : INFO : PROGRESS: at 41.70% examples, 420837 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:01:17,191 : INFO : PROGRESS: at 52.55% examples, 424047 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:18,202 : INFO : PROGRESS: at 67.54% examples, 454351 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:01:19,205 : INFO : PROGRESS: at 80.14% examples, 462521 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:20,207 : INFO : PROGRESS: at 94.28% examples, 476405 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:20,573 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:01:20,584 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:01:20,585 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:01:20,603 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:01:20,604 : INFO : training on 4523435 raw words (4076653 effective words) took 8.5s, 481732 effective words/s\n",
      "2017-05-13 11:01:20,604 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:01:20,651 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_2_model, separately None\n",
      "2017-05-13 11:01:20,652 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:01:20,653 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:01:20,921 : INFO : saved ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:01:23,382 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:01:23,383 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:01:23,427 : INFO : PROGRESS: at sentence #10000, processed 225415 words, keeping 13684 word types\n",
      "2017-05-13 11:01:23,471 : INFO : PROGRESS: at sentence #20000, processed 449496 words, keeping 18758 word types\n",
      "2017-05-13 11:01:23,516 : INFO : PROGRESS: at sentence #30000, processed 672246 words, keeping 22544 word types\n",
      "2017-05-13 11:01:23,559 : INFO : PROGRESS: at sentence #40000, processed 895570 words, keeping 25577 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:01:23,603 : INFO : PROGRESS: at sentence #50000, processed 1119209 words, keeping 28211 word types\n",
      "2017-05-13 11:01:23,649 : INFO : PROGRESS: at sentence #60000, processed 1341936 words, keeping 30318 word types\n",
      "2017-05-13 11:01:23,690 : INFO : PROGRESS: at sentence #70000, processed 1563772 words, keeping 32389 word types\n",
      "2017-05-13 11:01:23,731 : INFO : PROGRESS: at sentence #80000, processed 1787418 words, keeping 34236 word types\n",
      "2017-05-13 11:01:23,771 : INFO : PROGRESS: at sentence #90000, processed 2009809 words, keeping 35914 word types\n",
      "2017-05-13 11:01:23,812 : INFO : PROGRESS: at sentence #100000, processed 2233773 words, keeping 37463 word types\n",
      "2017-05-13 11:01:23,851 : INFO : PROGRESS: at sentence #110000, processed 2457504 words, keeping 38901 word types\n",
      "2017-05-13 11:01:23,892 : INFO : PROGRESS: at sentence #120000, processed 2679065 words, keeping 40246 word types\n",
      "2017-05-13 11:01:23,933 : INFO : PROGRESS: at sentence #130000, processed 2905623 words, keeping 41592 word types\n",
      "2017-05-13 11:01:23,973 : INFO : PROGRESS: at sentence #140000, processed 3127929 words, keeping 42809 word types\n",
      "2017-05-13 11:01:24,015 : INFO : PROGRESS: at sentence #150000, processed 3352349 words, keeping 43963 word types\n",
      "2017-05-13 11:01:24,056 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 45063 word types\n",
      "2017-05-13 11:01:24,097 : INFO : PROGRESS: at sentence #170000, processed 3799509 words, keeping 46159 word types\n",
      "2017-05-13 11:01:24,138 : INFO : PROGRESS: at sentence #180000, processed 4023821 words, keeping 47240 word types\n",
      "2017-05-13 11:01:24,179 : INFO : PROGRESS: at sentence #190000, processed 4246576 words, keeping 48288 word types\n",
      "2017-05-13 11:01:24,201 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-13 11:01:24,202 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:01:24,249 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-13 11:01:24,250 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-13 11:01:24,287 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-13 11:01:24,289 : INFO : sample=0.01 downsamples 4 most-common words\n",
      "2017-05-13 11:01:24,290 : INFO : downsampling leaves estimated 4037824 word corpus (93.8% of prior 4303761)\n",
      "2017-05-13 11:01:24,291 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-13 11:01:24,345 : INFO : resetting layer weights\n",
      "2017-05-13 11:01:24,587 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.01 negative=5 window=6\n",
      "2017-05-13 11:01:24,588 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:01:25,619 : INFO : PROGRESS: at 2.42% examples, 477562 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:01:26,624 : INFO : PROGRESS: at 4.89% examples, 486791 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:27,644 : INFO : PROGRESS: at 7.37% examples, 487554 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:28,669 : INFO : PROGRESS: at 9.71% examples, 480644 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:29,670 : INFO : PROGRESS: at 11.58% examples, 460468 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:30,682 : INFO : PROGRESS: at 14.19% examples, 470493 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:31,706 : INFO : PROGRESS: at 16.20% examples, 459993 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:32,706 : INFO : PROGRESS: at 18.53% examples, 461361 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:33,730 : INFO : PROGRESS: at 20.41% examples, 451095 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:34,767 : INFO : PROGRESS: at 22.37% examples, 444193 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:35,777 : INFO : PROGRESS: at 24.34% examples, 439605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:36,783 : INFO : PROGRESS: at 26.41% examples, 437422 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:37,789 : INFO : PROGRESS: at 29.02% examples, 443995 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:38,821 : INFO : PROGRESS: at 31.64% examples, 448819 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:39,829 : INFO : PROGRESS: at 34.20% examples, 453075 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:40,829 : INFO : PROGRESS: at 36.62% examples, 455367 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:41,832 : INFO : PROGRESS: at 39.13% examples, 458334 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:42,866 : INFO : PROGRESS: at 41.64% examples, 460216 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:01:43,886 : INFO : PROGRESS: at 44.21% examples, 462711 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:44,888 : INFO : PROGRESS: at 46.73% examples, 464908 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:45,889 : INFO : PROGRESS: at 49.21% examples, 466485 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:46,914 : INFO : PROGRESS: at 51.73% examples, 467844 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:47,938 : INFO : PROGRESS: at 54.24% examples, 469086 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:48,957 : INFO : PROGRESS: at 56.75% examples, 470335 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:49,961 : INFO : PROGRESS: at 59.31% examples, 472112 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:50,969 : INFO : PROGRESS: at 61.87% examples, 473696 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:52,001 : INFO : PROGRESS: at 64.03% examples, 471715 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:53,020 : INFO : PROGRESS: at 65.86% examples, 467825 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:01:54,032 : INFO : PROGRESS: at 67.65% examples, 463981 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:55,040 : INFO : PROGRESS: at 69.35% examples, 459845 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:01:56,044 : INFO : PROGRESS: at 71.27% examples, 457506 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:57,069 : INFO : PROGRESS: at 73.19% examples, 455016 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:58,103 : INFO : PROGRESS: at 75.20% examples, 453119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:01:59,113 : INFO : PROGRESS: at 77.35% examples, 452453 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:00,141 : INFO : PROGRESS: at 79.83% examples, 453419 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:02:01,181 : INFO : PROGRESS: at 82.29% examples, 454171 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:02:02,209 : INFO : PROGRESS: at 84.76% examples, 455021 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:03,210 : INFO : PROGRESS: at 87.15% examples, 455663 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:04,219 : INFO : PROGRESS: at 89.63% examples, 456659 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:05,230 : INFO : PROGRESS: at 92.24% examples, 458268 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:06,246 : INFO : PROGRESS: at 94.89% examples, 459952 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:07,252 : INFO : PROGRESS: at 97.54% examples, 461682 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:08,181 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:02:08,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:02:08,196 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:02:08,213 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:02:08,215 : INFO : training on 21819630 raw words (20190452 effective words) took 43.6s, 462852 effective words/s\n",
      "2017-05-13 11:02:08,216 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:02:08,343 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_2_model, separately None\n",
      "2017-05-13 11:02:08,344 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:02:08,344 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:02:09,193 : INFO : saved ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_2_model\n",
      "2017-05-13 11:02:09,402 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:02:09,403 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:02:09,445 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-13 11:02:09,446 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:02:09,468 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-13 11:02:09,469 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-13 11:02:09,477 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-13 11:02:09,479 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-05-13 11:02:09,479 : INFO : downsampling leaves estimated 122076 word corpus (71.4% of prior 170913)\n",
      "2017-05-13 11:02:09,480 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-13 11:02:09,493 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:09,554 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-13 11:02:09,554 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:02:10,570 : INFO : PROGRESS: at 94.13% examples, 570124 words/s, in_qsize 5, out_qsize 1\n",
      "2017-05-13 11:02:10,603 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:02:10,609 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:02:10,610 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:02:10,618 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:02:10,619 : INFO : training on 943135 raw words (610701 effective words) took 1.1s, 576828 effective words/s\n",
      "2017-05-13 11:02:10,620 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:02:10,644 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_3_model, separately None\n",
      "2017-05-13 11:02:10,645 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:02:10,646 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:02:10,778 : INFO : saved ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:11,290 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:02:11,291 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:02:11,338 : INFO : PROGRESS: at sentence #10000, processed 240504 words, keeping 13182 word types\n",
      "2017-05-13 11:02:11,381 : INFO : PROGRESS: at sentence #20000, processed 479354 words, keeping 17779 word types\n",
      "2017-05-13 11:02:11,427 : INFO : PROGRESS: at sentence #30000, processed 719840 words, keeping 20864 word types\n",
      "2017-05-13 11:02:11,464 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-13 11:02:11,465 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:11,492 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-13 11:02:11,493 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n",
      "2017-05-13 11:02:11,512 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-13 11:02:11,514 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-13 11:02:11,515 : INFO : downsampling leaves estimated 640540 word corpus (73.1% of prior 875943)\n",
      "2017-05-13 11:02:11,515 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-13 11:02:11,540 : INFO : resetting layer weights\n",
      "2017-05-13 11:02:11,663 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-13 11:02:11,664 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:02:12,678 : INFO : PROGRESS: at 17.22% examples, 546759 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:13,693 : INFO : PROGRESS: at 35.56% examples, 562211 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:14,697 : INFO : PROGRESS: at 53.44% examples, 565069 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:15,709 : INFO : PROGRESS: at 71.53% examples, 566999 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:16,716 : INFO : PROGRESS: at 89.40% examples, 567289 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-13 11:02:17,267 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:02:17,280 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:02:17,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:02:17,291 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:02:17,292 : INFO : training on 4523435 raw words (3202600 effective words) took 5.6s, 569583 effective words/s\n",
      "2017-05-13 11:02:17,293 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:02:17,342 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_3_model, separately None\n",
      "2017-05-13 11:02:17,343 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:02:17,343 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:02:17,618 : INFO : saved ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:19,550 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:02:19,551 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:02:19,592 : INFO : PROGRESS: at sentence #10000, processed 225415 words, keeping 13684 word types\n",
      "2017-05-13 11:02:19,629 : INFO : PROGRESS: at sentence #20000, processed 449496 words, keeping 18758 word types\n",
      "2017-05-13 11:02:19,669 : INFO : PROGRESS: at sentence #30000, processed 672246 words, keeping 22544 word types\n",
      "2017-05-13 11:02:19,710 : INFO : PROGRESS: at sentence #40000, processed 895570 words, keeping 25577 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:19,751 : INFO : PROGRESS: at sentence #50000, processed 1119209 words, keeping 28211 word types\n",
      "2017-05-13 11:02:19,796 : INFO : PROGRESS: at sentence #60000, processed 1341936 words, keeping 30318 word types\n",
      "2017-05-13 11:02:19,837 : INFO : PROGRESS: at sentence #70000, processed 1563772 words, keeping 32389 word types\n",
      "2017-05-13 11:02:19,879 : INFO : PROGRESS: at sentence #80000, processed 1787418 words, keeping 34236 word types\n",
      "2017-05-13 11:02:19,919 : INFO : PROGRESS: at sentence #90000, processed 2009809 words, keeping 35914 word types\n",
      "2017-05-13 11:02:19,960 : INFO : PROGRESS: at sentence #100000, processed 2233773 words, keeping 37463 word types\n",
      "2017-05-13 11:02:20,001 : INFO : PROGRESS: at sentence #110000, processed 2457504 words, keeping 38901 word types\n",
      "2017-05-13 11:02:20,043 : INFO : PROGRESS: at sentence #120000, processed 2679065 words, keeping 40246 word types\n",
      "2017-05-13 11:02:20,085 : INFO : PROGRESS: at sentence #130000, processed 2905623 words, keeping 41592 word types\n",
      "2017-05-13 11:02:20,125 : INFO : PROGRESS: at sentence #140000, processed 3127929 words, keeping 42809 word types\n",
      "2017-05-13 11:02:20,168 : INFO : PROGRESS: at sentence #150000, processed 3352349 words, keeping 43963 word types\n",
      "2017-05-13 11:02:20,211 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 45063 word types\n",
      "2017-05-13 11:02:20,252 : INFO : PROGRESS: at sentence #170000, processed 3799509 words, keeping 46159 word types\n",
      "2017-05-13 11:02:20,295 : INFO : PROGRESS: at sentence #180000, processed 4023821 words, keeping 47240 word types\n",
      "2017-05-13 11:02:20,337 : INFO : PROGRESS: at sentence #190000, processed 4246576 words, keeping 48288 word types\n",
      "2017-05-13 11:02:20,359 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-13 11:02:20,360 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:02:20,804 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-13 11:02:20,804 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-13 11:02:20,844 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-13 11:02:20,846 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-13 11:02:20,847 : INFO : downsampling leaves estimated 3215101 word corpus (74.7% of prior 4303761)\n",
      "2017-05-13 11:02:20,848 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-13 11:02:20,904 : INFO : resetting layer weights\n",
      "2017-05-13 11:02:21,149 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-13 11:02:21,149 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:02:22,158 : INFO : PROGRESS: at 2.83% examples, 454781 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:23,159 : INFO : PROGRESS: at 5.85% examples, 469854 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:24,172 : INFO : PROGRESS: at 8.93% examples, 475455 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:25,182 : INFO : PROGRESS: at 11.91% examples, 474813 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:02:26,191 : INFO : PROGRESS: at 14.78% examples, 471771 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:27,200 : INFO : PROGRESS: at 17.75% examples, 472230 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:28,203 : INFO : PROGRESS: at 20.77% examples, 473893 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:29,206 : INFO : PROGRESS: at 23.70% examples, 473401 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:30,210 : INFO : PROGRESS: at 26.31% examples, 467169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:31,219 : INFO : PROGRESS: at 28.84% examples, 460564 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:32,235 : INFO : PROGRESS: at 31.13% examples, 451576 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:33,249 : INFO : PROGRESS: at 33.78% examples, 448944 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:34,251 : INFO : PROGRESS: at 36.80% examples, 451720 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:35,280 : INFO : PROGRESS: at 39.87% examples, 453700 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:02:36,293 : INFO : PROGRESS: at 42.88% examples, 455406 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:37,307 : INFO : PROGRESS: at 45.95% examples, 457336 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:38,312 : INFO : PROGRESS: at 49.03% examples, 459263 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:39,313 : INFO : PROGRESS: at 52.05% examples, 460701 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:40,317 : INFO : PROGRESS: at 54.93% examples, 460778 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:02:41,322 : INFO : PROGRESS: at 57.76% examples, 460432 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:42,329 : INFO : PROGRESS: at 60.64% examples, 460408 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:02:43,351 : INFO : PROGRESS: at 63.56% examples, 460422 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:02:44,355 : INFO : PROGRESS: at 66.41% examples, 460142 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:45,375 : INFO : PROGRESS: at 69.03% examples, 458078 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:46,398 : INFO : PROGRESS: at 71.55% examples, 455552 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:47,399 : INFO : PROGRESS: at 74.11% examples, 453893 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:48,414 : INFO : PROGRESS: at 76.71% examples, 452391 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:49,427 : INFO : PROGRESS: at 79.32% examples, 451028 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:50,459 : INFO : PROGRESS: at 81.79% examples, 448689 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:02:51,465 : INFO : PROGRESS: at 84.58% examples, 448608 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:02:52,468 : INFO : PROGRESS: at 87.47% examples, 449055 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:53,487 : INFO : PROGRESS: at 90.50% examples, 449933 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:54,520 : INFO : PROGRESS: at 93.51% examples, 450546 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:55,551 : INFO : PROGRESS: at 96.39% examples, 450533 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:56,567 : INFO : PROGRESS: at 99.32% examples, 450903 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:02:56,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:02:56,772 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:02:56,774 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:02:56,780 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:02:56,781 : INFO : training on 21819630 raw words (16075691 effective words) took 35.6s, 451229 effective words/s\n",
      "2017-05-13 11:02:56,782 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:02:56,887 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_3_model, separately None\n",
      "2017-05-13 11:02:56,888 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:02:56,888 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:02:57,584 : INFO : saved ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_3_model\n",
      "2017-05-13 11:02:57,777 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:02:57,778 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:02:57,815 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-13 11:02:57,817 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:02:57,833 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-13 11:02:57,834 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-13 11:02:57,842 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-13 11:02:57,844 : INFO : sample=0.0001 downsamples 571 most-common words\n",
      "2017-05-13 11:02:57,845 : INFO : downsampling leaves estimated 75104 word corpus (43.9% of prior 170913)\n",
      "2017-05-13 11:02:57,845 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-13 11:02:57,859 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:57,912 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-05-13 11:02:57,913 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:02:58,560 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:02:58,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:02:58,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:02:58,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:02:58,576 : INFO : training on 943135 raw words (375312 effective words) took 0.7s, 570685 effective words/s\n",
      "2017-05-13 11:02:58,576 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:02:58,596 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_4_model, separately None\n",
      "2017-05-13 11:02:58,598 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:02:58,600 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:02:58,711 : INFO : saved ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:59,169 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:02:59,169 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:02:59,215 : INFO : PROGRESS: at sentence #10000, processed 240504 words, keeping 13182 word types\n",
      "2017-05-13 11:02:59,260 : INFO : PROGRESS: at sentence #20000, processed 479354 words, keeping 17779 word types\n",
      "2017-05-13 11:02:59,305 : INFO : PROGRESS: at sentence #30000, processed 719840 words, keeping 20864 word types\n",
      "2017-05-13 11:02:59,340 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-13 11:02:59,341 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:02:59,367 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-13 11:02:59,368 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:02:59,386 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-13 11:02:59,387 : INFO : sample=0.0001 downsamples 522 most-common words\n",
      "2017-05-13 11:02:59,388 : INFO : downsampling leaves estimated 411892 word corpus (47.0% of prior 875943)\n",
      "2017-05-13 11:02:59,388 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-13 11:02:59,412 : INFO : resetting layer weights\n",
      "2017-05-13 11:02:59,532 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-05-13 11:02:59,533 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:03:00,547 : INFO : PROGRESS: at 23.16% examples, 473506 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:01,548 : INFO : PROGRESS: at 47.02% examples, 481677 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:02,554 : INFO : PROGRESS: at 70.87% examples, 483782 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:03,558 : INFO : PROGRESS: at 94.73% examples, 485162 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:03,756 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:03:03,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:03:03,773 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:03:03,779 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:03:03,779 : INFO : training on 4523435 raw words (2059573 effective words) took 4.2s, 485513 effective words/s\n",
      "2017-05-13 11:03:03,780 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:03:03,829 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_4_model, separately None\n",
      "2017-05-13 11:03:03,830 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:03:03,830 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:03:04,097 : INFO : saved ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:06,227 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:03:06,228 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:03:06,274 : INFO : PROGRESS: at sentence #10000, processed 225415 words, keeping 13684 word types\n",
      "2017-05-13 11:03:06,315 : INFO : PROGRESS: at sentence #20000, processed 449496 words, keeping 18758 word types\n",
      "2017-05-13 11:03:06,359 : INFO : PROGRESS: at sentence #30000, processed 672246 words, keeping 22544 word types\n",
      "2017-05-13 11:03:06,403 : INFO : PROGRESS: at sentence #40000, processed 895570 words, keeping 25577 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:06,447 : INFO : PROGRESS: at sentence #50000, processed 1119209 words, keeping 28211 word types\n",
      "2017-05-13 11:03:06,497 : INFO : PROGRESS: at sentence #60000, processed 1341936 words, keeping 30318 word types\n",
      "2017-05-13 11:03:06,539 : INFO : PROGRESS: at sentence #70000, processed 1563772 words, keeping 32389 word types\n",
      "2017-05-13 11:03:06,583 : INFO : PROGRESS: at sentence #80000, processed 1787418 words, keeping 34236 word types\n",
      "2017-05-13 11:03:06,627 : INFO : PROGRESS: at sentence #90000, processed 2009809 words, keeping 35914 word types\n",
      "2017-05-13 11:03:06,670 : INFO : PROGRESS: at sentence #100000, processed 2233773 words, keeping 37463 word types\n",
      "2017-05-13 11:03:06,714 : INFO : PROGRESS: at sentence #110000, processed 2457504 words, keeping 38901 word types\n",
      "2017-05-13 11:03:06,757 : INFO : PROGRESS: at sentence #120000, processed 2679065 words, keeping 40246 word types\n",
      "2017-05-13 11:03:06,801 : INFO : PROGRESS: at sentence #130000, processed 2905623 words, keeping 41592 word types\n",
      "2017-05-13 11:03:06,844 : INFO : PROGRESS: at sentence #140000, processed 3127929 words, keeping 42809 word types\n",
      "2017-05-13 11:03:06,900 : INFO : PROGRESS: at sentence #150000, processed 3352349 words, keeping 43963 word types\n",
      "2017-05-13 11:03:06,954 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 45063 word types\n",
      "2017-05-13 11:03:06,999 : INFO : PROGRESS: at sentence #170000, processed 3799509 words, keeping 46159 word types\n",
      "2017-05-13 11:03:07,043 : INFO : PROGRESS: at sentence #180000, processed 4023821 words, keeping 47240 word types\n",
      "2017-05-13 11:03:07,087 : INFO : PROGRESS: at sentence #190000, processed 4246576 words, keeping 48288 word types\n",
      "2017-05-13 11:03:07,112 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-13 11:03:07,112 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:03:07,164 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-13 11:03:07,164 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-13 11:03:07,206 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-13 11:03:07,209 : INFO : sample=0.0001 downsamples 526 most-common words\n",
      "2017-05-13 11:03:07,209 : INFO : downsampling leaves estimated 2142107 word corpus (49.8% of prior 4303761)\n",
      "2017-05-13 11:03:07,210 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-13 11:03:07,273 : INFO : resetting layer weights\n",
      "2017-05-13 11:03:07,540 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=0.0001 negative=5 window=6\n",
      "2017-05-13 11:03:07,541 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:03:08,554 : INFO : PROGRESS: at 3.88% examples, 412920 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:09,591 : INFO : PROGRESS: at 7.82% examples, 409510 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:10,601 : INFO : PROGRESS: at 11.72% examples, 410456 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:11,611 : INFO : PROGRESS: at 15.70% examples, 413446 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:12,616 : INFO : PROGRESS: at 19.63% examples, 414737 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:13,622 : INFO : PROGRESS: at 23.20% examples, 408941 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:14,625 : INFO : PROGRESS: at 27.15% examples, 410509 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:15,627 : INFO : PROGRESS: at 30.95% examples, 410047 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:16,629 : INFO : PROGRESS: at 34.79% examples, 410068 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:03:17,639 : INFO : PROGRESS: at 38.07% examples, 404095 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:18,640 : INFO : PROGRESS: at 41.28% examples, 398505 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-13 11:03:19,659 : INFO : PROGRESS: at 44.62% examples, 394498 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:20,661 : INFO : PROGRESS: at 48.11% examples, 392772 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:03:21,666 : INFO : PROGRESS: at 51.91% examples, 393580 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:22,680 : INFO : PROGRESS: at 55.84% examples, 395101 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:23,698 : INFO : PROGRESS: at 59.73% examples, 396000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:24,706 : INFO : PROGRESS: at 63.38% examples, 395615 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:25,720 : INFO : PROGRESS: at 67.06% examples, 395088 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:26,732 : INFO : PROGRESS: at 70.81% examples, 395181 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-13 11:03:27,743 : INFO : PROGRESS: at 74.70% examples, 396033 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:28,751 : INFO : PROGRESS: at 78.63% examples, 397116 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:29,752 : INFO : PROGRESS: at 82.52% examples, 398001 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:30,753 : INFO : PROGRESS: at 86.46% examples, 398987 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:31,762 : INFO : PROGRESS: at 90.36% examples, 399591 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:32,769 : INFO : PROGRESS: at 94.29% examples, 400304 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:33,773 : INFO : PROGRESS: at 97.90% examples, 399769 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:34,303 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:03:34,315 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:03:34,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:03:34,322 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:03:34,322 : INFO : training on 21819630 raw words (10708796 effective words) took 26.8s, 399929 effective words/s\n",
      "2017-05-13 11:03:34,323 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:03:34,431 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_4_model, separately None\n",
      "2017-05-13 11:03:34,431 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:03:34,432 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:03:35,132 : INFO : saved ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_4_model\n",
      "2017-05-13 11:03:35,328 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:03:35,329 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:03:35,367 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-13 11:03:35,368 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:03:35,386 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-13 11:03:35,387 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-13 11:03:35,398 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-13 11:03:35,400 : INFO : sample=1e-05 downsamples 3074 most-common words\n",
      "2017-05-13 11:03:35,400 : INFO : downsampling leaves estimated 26217 word corpus (15.3% of prior 170913)\n",
      "2017-05-13 11:03:35,401 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-13 11:03:35,411 : INFO : resetting layer weights\n",
      "2017-05-13 11:03:35,465 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=1e-05 negative=5 window=6\n",
      "2017-05-13 11:03:35,466 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:35,899 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:03:35,905 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:03:35,906 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:03:35,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:03:35,907 : INFO : training on 943135 raw words (131575 effective words) took 0.4s, 301314 effective words/s\n",
      "2017-05-13 11:03:35,908 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:03:35,928 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_5_model, separately None\n",
      "2017-05-13 11:03:35,929 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:03:35,930 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:03:36,045 : INFO : saved ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:36,794 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:03:36,795 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:03:36,842 : INFO : PROGRESS: at sentence #10000, processed 240504 words, keeping 13182 word types\n",
      "2017-05-13 11:03:36,887 : INFO : PROGRESS: at sentence #20000, processed 479354 words, keeping 17779 word types\n",
      "2017-05-13 11:03:36,932 : INFO : PROGRESS: at sentence #30000, processed 719840 words, keeping 20864 word types\n",
      "2017-05-13 11:03:36,968 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-13 11:03:36,969 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:03:36,992 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-13 11:03:36,993 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:37,017 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-13 11:03:37,019 : INFO : sample=1e-05 downsamples 3222 most-common words\n",
      "2017-05-13 11:03:37,019 : INFO : downsampling leaves estimated 178992 word corpus (20.4% of prior 875943)\n",
      "2017-05-13 11:03:37,020 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-13 11:03:37,043 : INFO : resetting layer weights\n",
      "2017-05-13 11:03:37,164 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=1e-05 negative=5 window=6\n",
      "2017-05-13 11:03:37,165 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:03:38,175 : INFO : PROGRESS: at 34.68% examples, 308986 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:39,177 : INFO : PROGRESS: at 69.31% examples, 309447 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:40,022 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:03:40,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:03:40,036 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:03:40,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:03:40,037 : INFO : training on 4523435 raw words (895428 effective words) took 2.9s, 312308 effective words/s\n",
      "2017-05-13 11:03:40,037 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:03:40,083 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_5_model, separately None\n",
      "2017-05-13 11:03:40,083 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:03:40,084 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:03:40,333 : INFO : saved ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:42,271 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:03:42,272 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:03:42,316 : INFO : PROGRESS: at sentence #10000, processed 225415 words, keeping 13684 word types\n",
      "2017-05-13 11:03:42,356 : INFO : PROGRESS: at sentence #20000, processed 449496 words, keeping 18758 word types\n",
      "2017-05-13 11:03:42,398 : INFO : PROGRESS: at sentence #30000, processed 672246 words, keeping 22544 word types\n",
      "2017-05-13 11:03:42,440 : INFO : PROGRESS: at sentence #40000, processed 895570 words, keeping 25577 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:03:42,481 : INFO : PROGRESS: at sentence #50000, processed 1119209 words, keeping 28211 word types\n",
      "2017-05-13 11:03:42,525 : INFO : PROGRESS: at sentence #60000, processed 1341936 words, keeping 30318 word types\n",
      "2017-05-13 11:03:42,566 : INFO : PROGRESS: at sentence #70000, processed 1563772 words, keeping 32389 word types\n",
      "2017-05-13 11:03:42,607 : INFO : PROGRESS: at sentence #80000, processed 1787418 words, keeping 34236 word types\n",
      "2017-05-13 11:03:42,648 : INFO : PROGRESS: at sentence #90000, processed 2009809 words, keeping 35914 word types\n",
      "2017-05-13 11:03:42,689 : INFO : PROGRESS: at sentence #100000, processed 2233773 words, keeping 37463 word types\n",
      "2017-05-13 11:03:42,730 : INFO : PROGRESS: at sentence #110000, processed 2457504 words, keeping 38901 word types\n",
      "2017-05-13 11:03:42,770 : INFO : PROGRESS: at sentence #120000, processed 2679065 words, keeping 40246 word types\n",
      "2017-05-13 11:03:42,812 : INFO : PROGRESS: at sentence #130000, processed 2905623 words, keeping 41592 word types\n",
      "2017-05-13 11:03:42,853 : INFO : PROGRESS: at sentence #140000, processed 3127929 words, keeping 42809 word types\n",
      "2017-05-13 11:03:42,896 : INFO : PROGRESS: at sentence #150000, processed 3352349 words, keeping 43963 word types\n",
      "2017-05-13 11:03:42,935 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 45063 word types\n",
      "2017-05-13 11:03:42,976 : INFO : PROGRESS: at sentence #170000, processed 3799509 words, keeping 46159 word types\n",
      "2017-05-13 11:03:43,017 : INFO : PROGRESS: at sentence #180000, processed 4023821 words, keeping 47240 word types\n",
      "2017-05-13 11:03:43,059 : INFO : PROGRESS: at sentence #190000, processed 4246576 words, keeping 48288 word types\n",
      "2017-05-13 11:03:43,083 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-13 11:03:43,084 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:03:43,130 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-13 11:03:43,131 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-13 11:03:43,173 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-13 11:03:43,175 : INFO : sample=1e-05 downsamples 3210 most-common words\n",
      "2017-05-13 11:03:43,176 : INFO : downsampling leaves estimated 992425 word corpus (23.1% of prior 4303761)\n",
      "2017-05-13 11:03:43,176 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-13 11:03:43,235 : INFO : resetting layer weights\n",
      "2017-05-13 11:03:43,474 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=1e-05 negative=5 window=6\n",
      "2017-05-13 11:03:43,475 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:03:44,493 : INFO : PROGRESS: at 5.99% examples, 292885 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:45,503 : INFO : PROGRESS: at 11.91% examples, 291291 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:46,511 : INFO : PROGRESS: at 17.89% examples, 292862 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:47,511 : INFO : PROGRESS: at 23.88% examples, 294223 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-13 11:03:48,521 : INFO : PROGRESS: at 29.99% examples, 295153 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:49,535 : INFO : PROGRESS: at 35.93% examples, 294446 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:50,542 : INFO : PROGRESS: at 41.92% examples, 294625 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:03:51,552 : INFO : PROGRESS: at 47.78% examples, 293707 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:03:52,555 : INFO : PROGRESS: at 53.78% examples, 293939 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:53,566 : INFO : PROGRESS: at 59.82% examples, 294268 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:54,569 : INFO : PROGRESS: at 65.67% examples, 293911 words/s, in_qsize 8, out_qsize 2\n",
      "2017-05-13 11:03:55,574 : INFO : PROGRESS: at 71.87% examples, 294802 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:03:56,581 : INFO : PROGRESS: at 77.89% examples, 295079 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-13 11:03:57,585 : INFO : PROGRESS: at 83.84% examples, 294984 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:03:58,588 : INFO : PROGRESS: at 89.81% examples, 294883 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:03:59,598 : INFO : PROGRESS: at 95.76% examples, 294735 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-13 11:04:00,315 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:04:00,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:04:00,324 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:04:00,326 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:04:00,326 : INFO : training on 21819630 raw words (4961642 effective words) took 16.8s, 294519 effective words/s\n",
      "2017-05-13 11:04:00,327 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:04:00,424 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_5_model, separately None\n",
      "2017-05-13 11:04:00,425 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:04:00,425 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:04:01,025 : INFO : saved ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_5_model\n",
      "2017-05-13 11:04:01,214 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:04:01,214 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:04:01,250 : INFO : collected 12999 word types from a corpus of 188627 raw words and 7408 sentences\n",
      "2017-05-13 11:04:01,251 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:04:01,263 : INFO : min_count=6 retains 3074 unique words (23% of original 12999, drops 9925)\n",
      "2017-05-13 11:04:01,264 : INFO : min_count=6 leaves 170913 word corpus (90% of original 188627, drops 17714)\n",
      "2017-05-13 11:04:01,275 : INFO : deleting the raw counts dictionary of 12999 items\n",
      "2017-05-13 11:04:01,276 : INFO : sample=1e-06 downsamples 3074 most-common words\n",
      "2017-05-13 11:04:01,277 : INFO : downsampling leaves estimated 7154 word corpus (4.2% of prior 170913)\n",
      "2017-05-13 11:04:01,277 : INFO : estimated required memory for 3074 words and 600 dimensions: 16292200 bytes\n",
      "2017-05-13 11:04:01,290 : INFO : resetting layer weights\n",
      "2017-05-13 11:04:01,339 : INFO : training model with 4 workers on 3074 vocabulary and 600 features, using sg=0 hs=0 sample=1e-06 negative=5 window=6\n",
      "2017-05-13 11:04:01,339 : INFO : expecting 7408 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:04:01,642 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:04:01,642 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:04:01,643 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:04:01,643 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:04:01,644 : INFO : training on 943135 raw words (35788 effective words) took 0.3s, 119317 effective words/s\n",
      "2017-05-13 11:04:01,644 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:04:01,664 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_6_model, separately None\n",
      "2017-05-13 11:04:01,666 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:04:01,667 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:04:01,775 : INFO : saved ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:04:02,465 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:04:02,466 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:04:02,510 : INFO : PROGRESS: at sentence #10000, processed 240504 words, keeping 13182 word types\n",
      "2017-05-13 11:04:02,549 : INFO : PROGRESS: at sentence #20000, processed 479354 words, keeping 17779 word types\n",
      "2017-05-13 11:04:02,591 : INFO : PROGRESS: at sentence #30000, processed 719840 words, keeping 20864 word types\n",
      "2017-05-13 11:04:02,625 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-13 11:04:02,625 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:04:02,646 : INFO : min_count=6 retains 7287 unique words (31% of original 22824, drops 15537)\n",
      "2017-05-13 11:04:02,648 : INFO : min_count=6 leaves 875943 word corpus (96% of original 904687, drops 28744)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:04:02,673 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-13 11:04:02,675 : INFO : sample=1e-06 downsamples 7287 most-common words\n",
      "2017-05-13 11:04:02,676 : INFO : downsampling leaves estimated 52117 word corpus (5.9% of prior 875943)\n",
      "2017-05-13 11:04:02,677 : INFO : estimated required memory for 7287 words and 600 dimensions: 38621100 bytes\n",
      "2017-05-13 11:04:02,699 : INFO : resetting layer weights\n",
      "2017-05-13 11:04:02,807 : INFO : training model with 4 workers on 7287 vocabulary and 600 features, using sg=0 hs=0 sample=1e-06 negative=5 window=6\n",
      "2017-05-13 11:04:02,808 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:04:03,818 : INFO : PROGRESS: at 53.87% examples, 139719 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-13 11:04:04,640 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:04:04,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:04:04,642 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:04:04,643 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:04:04,643 : INFO : training on 4523435 raw words (260159 effective words) took 1.8s, 142105 effective words/s\n",
      "2017-05-13 11:04:04,644 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:04:04,690 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_6_model, separately None\n",
      "2017-05-13 11:04:04,691 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:04:04,691 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:04:04,947 : INFO : saved ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:04:06,877 : INFO : collecting all words and their counts\n",
      "2017-05-13 11:04:06,878 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-13 11:04:06,921 : INFO : PROGRESS: at sentence #10000, processed 225415 words, keeping 13684 word types\n",
      "2017-05-13 11:04:06,962 : INFO : PROGRESS: at sentence #20000, processed 449496 words, keeping 18758 word types\n",
      "2017-05-13 11:04:07,002 : INFO : PROGRESS: at sentence #30000, processed 672246 words, keeping 22544 word types\n",
      "2017-05-13 11:04:07,043 : INFO : PROGRESS: at sentence #40000, processed 895570 words, keeping 25577 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:04:07,084 : INFO : PROGRESS: at sentence #50000, processed 1119209 words, keeping 28211 word types\n",
      "2017-05-13 11:04:07,126 : INFO : PROGRESS: at sentence #60000, processed 1341936 words, keeping 30318 word types\n",
      "2017-05-13 11:04:07,167 : INFO : PROGRESS: at sentence #70000, processed 1563772 words, keeping 32389 word types\n",
      "2017-05-13 11:04:07,208 : INFO : PROGRESS: at sentence #80000, processed 1787418 words, keeping 34236 word types\n",
      "2017-05-13 11:04:07,253 : INFO : PROGRESS: at sentence #90000, processed 2009809 words, keeping 35914 word types\n",
      "2017-05-13 11:04:07,294 : INFO : PROGRESS: at sentence #100000, processed 2233773 words, keeping 37463 word types\n",
      "2017-05-13 11:04:07,335 : INFO : PROGRESS: at sentence #110000, processed 2457504 words, keeping 38901 word types\n",
      "2017-05-13 11:04:07,375 : INFO : PROGRESS: at sentence #120000, processed 2679065 words, keeping 40246 word types\n",
      "2017-05-13 11:04:07,416 : INFO : PROGRESS: at sentence #130000, processed 2905623 words, keeping 41592 word types\n",
      "2017-05-13 11:04:07,456 : INFO : PROGRESS: at sentence #140000, processed 3127929 words, keeping 42809 word types\n",
      "2017-05-13 11:04:07,499 : INFO : PROGRESS: at sentence #150000, processed 3352349 words, keeping 43963 word types\n",
      "2017-05-13 11:04:07,540 : INFO : PROGRESS: at sentence #160000, processed 3576077 words, keeping 45063 word types\n",
      "2017-05-13 11:04:07,581 : INFO : PROGRESS: at sentence #170000, processed 3799509 words, keeping 46159 word types\n",
      "2017-05-13 11:04:07,622 : INFO : PROGRESS: at sentence #180000, processed 4023821 words, keeping 47240 word types\n",
      "2017-05-13 11:04:07,663 : INFO : PROGRESS: at sentence #190000, processed 4246576 words, keeping 48288 word types\n",
      "2017-05-13 11:04:07,685 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-13 11:04:07,686 : INFO : Loading a fresh vocabulary\n",
      "2017-05-13 11:04:07,734 : INFO : min_count=6 retains 16349 unique words (33% of original 48808, drops 32459)\n",
      "2017-05-13 11:04:07,735 : INFO : min_count=6 leaves 4303761 word corpus (98% of original 4363926, drops 60165)\n",
      "2017-05-13 11:04:07,783 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-13 11:04:07,786 : INFO : sample=1e-06 downsamples 11326 most-common words\n",
      "2017-05-13 11:04:07,786 : INFO : downsampling leaves estimated 348429 word corpus (8.1% of prior 4303761)\n",
      "2017-05-13 11:04:07,787 : INFO : estimated required memory for 16349 words and 600 dimensions: 86649700 bytes\n",
      "2017-05-13 11:04:07,841 : INFO : resetting layer weights\n",
      "2017-05-13 11:04:08,082 : INFO : training model with 4 workers on 16349 vocabulary and 600 features, using sg=0 hs=0 sample=1e-06 negative=5 window=6\n",
      "2017-05-13 11:04:08,083 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-13 11:04:09,088 : INFO : PROGRESS: at 9.07% examples, 157830 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:10,100 : INFO : PROGRESS: at 18.17% examples, 157627 words/s, in_qsize 6, out_qsize 4\n",
      "2017-05-13 11:04:11,107 : INFO : PROGRESS: at 27.83% examples, 160698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:12,116 : INFO : PROGRESS: at 37.21% examples, 161102 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-13 11:04:13,121 : INFO : PROGRESS: at 46.22% examples, 160173 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:14,124 : INFO : PROGRESS: at 55.52% examples, 160381 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:15,131 : INFO : PROGRESS: at 64.80% examples, 160470 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:16,139 : INFO : PROGRESS: at 74.11% examples, 160404 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:17,140 : INFO : PROGRESS: at 83.21% examples, 160246 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-13 11:04:18,143 : INFO : PROGRESS: at 92.42% examples, 160167 words/s, in_qsize 6, out_qsize 2\n",
      "2017-05-13 11:04:18,918 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-13 11:04:18,919 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-13 11:04:18,920 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-13 11:04:18,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-13 11:04:18,921 : INFO : training on 21819630 raw words (1743393 effective words) took 10.8s, 160927 effective words/s\n",
      "2017-05-13 11:04:18,921 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-13 11:04:19,017 : INFO : saving Word2Vec object under ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_6_model, separately None\n",
      "2017-05-13 11:04:19,018 : INFO : not storing attribute syn0norm\n",
      "2017-05-13 11:04:19,019 : INFO : not storing attribute cum_table\n",
      "2017-05-13 11:04:19,647 : INFO : saved ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_6_model\n"
     ]
    }
   ],
   "source": [
    "downsample = [(1, 0.1), (2, 0.01), (3, 0.001), (4, 0.0001), (5, 0.00001), (6, 0.000001)]\n",
    "for i in downsample:\n",
    "    w2v_parameters = [best_dims, best_word_count, 4, best_context_window, i[1]]\n",
    "    strict = pred.make_w2v_model(dros_strict_real, 'Drosophila/models/downsample/dros_strict_downsample_parameter_' + str(i[0]), w2v_parameters)\n",
    "    gen = pred.make_w2v_model(dros_gen_real, 'Drosophila/models/downsample/dros_gen_downsample_parameter_' + str(i[0]), w2v_parameters)\n",
    "    be = pred.make_w2v_model(dros_be_real, 'Drosophila/models/downsample/dros_be_downsample_parameter_' + str(i[0]), w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:04:19,786 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_1_model\n",
      "2017-05-13 11:04:20,574 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_1_model.wv.* with mmap=None\n",
      "2017-05-13 11:04:20,575 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:04:20,576 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:04:20,577 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_1_model\n",
      "2017-05-13 11:04:20,589 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_1_model\n",
      "2017-05-13 11:04:21,536 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_1_model.wv.* with mmap=None\n",
      "2017-05-13 11:04:21,537 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:04:21,538 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:04:21,538 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_1_model\n",
      "2017-05-13 11:04:21,556 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_1_model\n",
      "2017-05-13 11:04:22,002 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_1_model.wv.* with mmap=None\n",
      "2017-05-13 11:04:22,004 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:04:22,004 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:04:22,004 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_1_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:08:53,296 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_2_model\n",
      "2017-05-13 11:08:53,385 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-13 11:08:53,385 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:08:53,386 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:08:53,386 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_2_model\n",
      "2017-05-13 11:08:53,397 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:08:53,603 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-13 11:08:53,603 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:08:53,604 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:08:53,605 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_2_model\n",
      "2017-05-13 11:08:53,628 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_2_model\n",
      "2017-05-13 11:08:54,118 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_2_model.wv.* with mmap=None\n",
      "2017-05-13 11:08:54,119 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:08:54,120 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:08:54,121 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_2_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:13:10,650 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_3_model\n",
      "2017-05-13 11:13:10,740 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-13 11:13:10,741 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:13:10,741 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:13:10,742 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_3_model\n",
      "2017-05-13 11:13:10,751 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:13:10,975 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-13 11:13:10,975 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:13:10,976 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:13:10,977 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_3_model\n",
      "2017-05-13 11:13:10,994 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_3_model\n",
      "2017-05-13 11:13:11,467 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_3_model.wv.* with mmap=None\n",
      "2017-05-13 11:13:11,469 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:13:11,469 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:13:11,470 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_3_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:17:27,157 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_4_model\n",
      "2017-05-13 11:17:27,244 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-13 11:17:27,245 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:17:27,245 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:17:27,246 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_4_model\n",
      "2017-05-13 11:17:27,252 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:17:27,450 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-13 11:17:27,451 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:17:27,451 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:17:27,452 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_4_model\n",
      "2017-05-13 11:17:27,468 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_4_model\n",
      "2017-05-13 11:17:27,898 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_4_model.wv.* with mmap=None\n",
      "2017-05-13 11:17:27,899 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:17:27,899 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:17:27,900 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_4_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:21:46,738 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_5_model\n",
      "2017-05-13 11:21:46,838 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-13 11:21:46,839 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:21:46,840 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:21:46,840 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_5_model\n",
      "2017-05-13 11:21:46,852 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:21:47,092 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-13 11:21:47,093 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:21:47,094 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:21:47,095 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_5_model\n",
      "2017-05-13 11:21:47,112 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_5_model\n",
      "2017-05-13 11:21:47,650 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_5_model.wv.* with mmap=None\n",
      "2017-05-13 11:21:47,651 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:21:47,652 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:21:47,652 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_5_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:26:03,384 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_6_model\n",
      "2017-05-13 11:26:03,474 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-13 11:26:03,475 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:26:03,475 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:26:03,476 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_6_model\n",
      "2017-05-13 11:26:03,487 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-13 11:26:03,704 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-13 11:26:03,706 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:26:03,706 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:26:03,707 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_6_model\n",
      "2017-05-13 11:26:03,728 : INFO : loading Word2Vec object from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_6_model\n",
      "2017-05-13 11:26:04,188 : INFO : loading wv recursively from ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_6_model.wv.* with mmap=None\n",
      "2017-05-13 11:26:04,189 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-05-13 11:26:04,190 : INFO : setting ignored attribute cum_table to None\n",
      "2017-05-13 11:26:04,191 : INFO : loaded ../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_6_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Core-scripts/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    }
   ],
   "source": [
    "for i in downsample:\n",
    "    w2w_model_strict = word2vec.Word2Vec.load('../../Results/Drosophila/models/downsample/dros_strict_downsample_parameter_'+str(i[0])+'_model')\n",
    "    w2w_model_gen = word2vec.Word2Vec.load('../../Results/Drosophila/models/downsample/dros_gen_downsample_parameter_'+str(i[0])+'_model')\n",
    "    w2w_model_be = word2vec.Word2Vec.load('../../Results/Drosophila/models/downsample/dros_be_downsample_parameter_'+str(i[0])+'_model')\n",
    "    \n",
    "    for seed in random_seeds:\n",
    "        data_name = '../../Results/Drosophila/train_val/dros_tr_val_split_' + str(seed)\n",
    "        train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "        train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "        validation_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "        validation_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_strict, feature_count=600)\n",
    "        \n",
    "        strict_list_SR_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_gen, feature_count=600)\n",
    "        \n",
    "        strict_list_GEN_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                      train_labels, validation_labels]\n",
    "        \n",
    "        w2v_train_vecs, w2v_val_vecs = pred.word_2_vec_feat_vecs(train_data, validation_data, w2w_model_be, feature_count=600)\n",
    "        \n",
    "        strict_list_BE_dims_param = [w2v_train_vecs, w2v_val_vecs,\n",
    "                                     train_labels, validation_labels]\n",
    "        \n",
    "        pickle.dump(strict_list_SR_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/downsample/dros_strict_list_SR_downsample_param_'+str(i[0])+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_GEN_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/downsample/dros_strict_list_GEN_downsample_param_'+str(i[0])+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        pickle.dump(strict_list_BE_dims_param, open('../../Results/Drosophila/result_list/w2v_param_search/downsample/dros_strict_list_BE_downsample_param_'+str(i[0])+'_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "        \n",
    "        strict_final_list = [strict_list_SR_dims_param, \n",
    "                             strict_list_GEN_dims_param, \n",
    "                             strict_list_BE_dims_param]\n",
    "        print ('\\nPredicting\\n')\n",
    "        errors = []\n",
    "        fpr = []\n",
    "        tpr = []\n",
    "\n",
    "        for entry in strict_final_list:\n",
    "            error_w2v_norm, probs_w2v_norm = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                                 entry[2], entry[3])\n",
    "            fpr_w2v_norm, tpr_w2v_norm, _ = roc_curve(entry[3], probs_w2v_norm)\n",
    "            error_w2v_fs, probs_w2v_fs = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                             entry[2], entry[3],\n",
    "                                                             feature_selection=True)\n",
    "            fpr_w2v_fs, tpr_w2v_fs, _ = roc_curve(entry[3], probs_w2v_fs)\n",
    "\n",
    "            errors.append([error_w2v_norm, error_w2v_fs])\n",
    "            fpr.append([fpr_w2v_norm, fpr_w2v_fs])\n",
    "            tpr.append([tpr_w2v_norm, tpr_w2v_fs])\n",
    "            \n",
    "        pickle.dump(errors, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/downsample/dros_downsample_param'+str(i[0])+'_errors_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(fpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/downsample/dros_downsample_param'+str(i[0])+'_fpr_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "        pickle.dump(tpr, open('../../Results/Drosophila/error_fpr_tpr/w2v_param_search/downsample/dros_downsample_param'+str(i[0])+'_tpr_pickle_'+str(seed)+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "best_parameter = []\n",
    "for seed in random_seeds:\n",
    "    mean_err_dros_strict = []\n",
    "    mean_auc_dros_strict = []\n",
    "    mean_err_dros_gen = []\n",
    "    mean_auc_dros_gen = []\n",
    "    mean_err_dros_be = []\n",
    "    mean_auc_dros_be = []\n",
    "    for i in downsample:\n",
    "        drct = '../../Results/Drosophila/error_fpr_tpr/w2v_param_search/downsample/'\n",
    "        errors_dros = mult_open(drct, str(i[0])+'_errors_pickle_'+str(seed))\n",
    "        fpr_dros = mult_open(drct, str(i[0])+'_fpr_pickle_'+str(seed))\n",
    "        tpr_dros = mult_open(drct, str(i[0])+'_tpr_pickle_'+str(seed))\n",
    "        for e, f, t in zip(errors_dros, fpr_dros, tpr_dros):\n",
    "            input_list = [[e, f, t]]\n",
    "            name_list = ['drosophila']\n",
    "            for idx in range(3):\n",
    "                for item, name in zip(input_list, name_list):\n",
    "                    for fpr_item, tpr_item, error_item in zip(item[1][idx], item[2][idx], item[0][idx]):\n",
    "                        roc_auc = auc(fpr_item, tpr_item)\n",
    "                        auc_val = '%.3f' % roc_auc\n",
    "                        error = '%.3f' % error_item\n",
    "                        if idx == 0:\n",
    "                            mean_err_dros_strict.append(error_item)\n",
    "                            mean_auc_dros_strict.append(roc_auc)\n",
    "                        elif idx == 1:\n",
    "                            mean_err_dros_gen.append(error_item)\n",
    "                            mean_auc_dros_gen.append(roc_auc)\n",
    "                        elif idx == 2:\n",
    "                            mean_err_dros_be.append(error_item)\n",
    "                            mean_auc_dros_be.append(roc_auc)\n",
    "\n",
    "    mean_err_dros_org_strict = mean_err_dros_strict[0::2]\n",
    "    mean_err_dros_fs_strict = mean_err_dros_strict[1::2]\n",
    "    mean_auc_dros_org_strict = mean_auc_dros_strict[0::2]\n",
    "    mean_auc_dros_fs_strict = mean_auc_dros_strict[1::2]\n",
    "\n",
    "    mean_err_dros_org_gen = mean_err_dros_gen[0::2]\n",
    "    mean_err_dros_fs_gen = mean_err_dros_gen[1::2]\n",
    "    mean_auc_dros_org_gen = mean_auc_dros_gen[0::2]\n",
    "    mean_auc_dros_fs_gen = mean_auc_dros_gen[1::2]\n",
    "\n",
    "    mean_err_dros_org_be = mean_err_dros_be[0::2]\n",
    "    mean_err_dros_fs_be = mean_err_dros_be[1::2]\n",
    "    mean_auc_dros_org_be = mean_auc_dros_be[0::2]\n",
    "    mean_auc_dros_fs_be = mean_auc_dros_be[1::2]\n",
    "    \n",
    "    org_error = max([(mean_err_dros_org_strict.index(min(mean_err_dros_org_strict))+1), \n",
    "                     (mean_err_dros_org_gen.index(min(mean_err_dros_org_gen))+1), \n",
    "                     (mean_err_dros_org_be.index(min(mean_err_dros_org_be))+1)])\n",
    "    \n",
    "    fs_error = max([(mean_err_dros_fs_strict.index(min(mean_err_dros_fs_strict))+1), \n",
    "                     (mean_err_dros_fs_gen.index(min(mean_err_dros_fs_gen))+1), \n",
    "                     (mean_err_dros_fs_be.index(min(mean_err_dros_fs_be))+1)])\n",
    "    \n",
    "    org_auc = max([(mean_auc_dros_org_strict.index(max(mean_auc_dros_org_strict))+1), \n",
    "                     (mean_auc_dros_org_gen.index(max(mean_auc_dros_org_gen))+1), \n",
    "                     (mean_auc_dros_org_be.index(max(mean_auc_dros_org_be))+1)])\n",
    "    \n",
    "    fs_auc = max([(mean_auc_dros_fs_strict.index(max(mean_auc_dros_fs_strict))+1), \n",
    "                     (mean_auc_dros_fs_gen.index(max(mean_auc_dros_fs_gen))+1), \n",
    "                     (mean_auc_dros_fs_be.index(max(mean_auc_dros_fs_be))+1)])\n",
    "    \n",
    "    best_parameter.append((seed, [org_error, fs_error, org_auc, fs_auc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, [3, 5, 6, 5])\n",
      "(235, [3, 6, 6, 5])\n",
      "(905, [5, 5, 6, 6])\n",
      "(2895, [5, 3, 5, 3])\n",
      "(3462, [3, 5, 6, 6])\n",
      "(4225, [2, 5, 5, 5])\n",
      "(5056, [5, 6, 3, 3])\n",
      "(5192, [4, 6, 5, 5])\n",
      "(7751, [4, 5, 6, 5])\n",
      "(7813, [6, 3, 6, 5])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "best_overall = []\n",
    "for i in best_parameter:\n",
    "    best_overall.append(most_common(i[1]))\n",
    "    print(i)\n",
    "best_downsample = most_common(best_overall)\n",
    "print(best_downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600, 6, 4, 6, 1e-06]\n"
     ]
    }
   ],
   "source": [
    "best_w2v_parameters = [best_dims, best_word_count, 4, best_context_window, downsample[5][1]]\n",
    "print(best_w2v_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.01\n",
      "0.001\n",
      "0.0001\n",
      "1e-05\n",
      "1e-06\n"
     ]
    }
   ],
   "source": [
    "for i in downsample:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
