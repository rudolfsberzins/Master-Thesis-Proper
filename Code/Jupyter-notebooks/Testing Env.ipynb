{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from parse_and_prepare import ProteinProteinInteractionClassifier as ppi\n",
    "import file_readers as fr\n",
    "import prediction as pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_dros_strict = pickle.load(open('Results/drosophila/Results/drosophila_mentions_strict_real.pkl', 'rb'))\n",
    "new_dros_gen = pickle.load(open('Results/drosophila/Results/drosophila_mentions_gen_real.pkl', 'rb'))\n",
    "new_dros_be = pickle.load(open('Results/drosophila/Results/drosophila_mentions_be_real.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mult_open(direct, pattern):\n",
    "    pickle_list = []\n",
    "    file_list = []\n",
    "    for f in os.listdir(direct):\n",
    "        if re.search(pattern, f):\n",
    "            f = f.split('_')\n",
    "            f[-1] = f[-1][:-4]\n",
    "            file_list.append(f)\n",
    "    file_list.sort(key = lambda x: int(x[-1]))\n",
    "    for file in file_list:\n",
    "        file = '_'.join(file)\n",
    "        file = file + '.pkl'\n",
    "        pkl = pickle.load(open(os.path.join(direct, file), 'rb'))\n",
    "        pickle_list.append(pkl)\n",
    "    return pickle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7508"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_dros_strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-10 09:30:41,989 : INFO : collecting all words and their counts\n",
      "2017-05-10 09:30:41,990 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-10 09:30:42,037 : INFO : PROGRESS: at sentence #10000, processed 239827 words, keeping 12999 word types\n",
      "2017-05-10 09:30:42,083 : INFO : PROGRESS: at sentence #20000, processed 480039 words, keeping 17635 word types\n",
      "2017-05-10 09:30:42,130 : INFO : PROGRESS: at sentence #30000, processed 719699 words, keeping 20774 word types\n",
      "2017-05-10 09:30:42,167 : INFO : collected 22824 word types from a corpus of 904687 raw words and 37697 sentences\n",
      "2017-05-10 09:30:42,168 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-10 09:30:42,192 : INFO : min_count=5 retains 8096 unique words (35% of original 22824, drops 14728)\n",
      "2017-05-10 09:30:42,193 : INFO : min_count=5 leaves 879988 word corpus (97% of original 904687, drops 24699)\n",
      "2017-05-10 09:30:42,214 : INFO : deleting the raw counts dictionary of 22824 items\n",
      "2017-05-10 09:30:42,216 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2017-05-10 09:30:42,217 : INFO : downsampling leaves estimated 644974 word corpus (73.3% of prior 879988)\n",
      "2017-05-10 09:30:42,217 : INFO : estimated required memory for 8096 words and 300 dimensions: 23478400 bytes\n",
      "2017-05-10 09:30:42,249 : INFO : resetting layer weights\n",
      "2017-05-10 09:30:42,344 : INFO : training model with 4 workers on 8096 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-10 09:30:42,345 : INFO : expecting 37697 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-10 09:30:43,356 : INFO : PROGRESS: at 26.27% examples, 841593 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-10 09:30:44,377 : INFO : PROGRESS: at 52.74% examples, 839378 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:45,391 : INFO : PROGRESS: at 79.68% examples, 844880 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-10 09:30:46,084 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-10 09:30:46,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-10 09:30:46,094 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-10 09:30:46,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-10 09:30:46,108 : INFO : training on 4523435 raw words (3224640 effective words) took 3.8s, 858007 effective words/s\n",
      "2017-05-10 09:30:46,109 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-10 09:30:46,150 : INFO : saving Word2Vec object under Results/dros_gen_real_new_model, separately None\n",
      "2017-05-10 09:30:46,152 : INFO : not storing attribute syn0norm\n",
      "2017-05-10 09:30:46,152 : INFO : not storing attribute cum_table\n",
      "2017-05-10 09:30:46,366 : INFO : saved Results/dros_gen_real_new_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-10 09:30:48,456 : INFO : collecting all words and their counts\n",
      "2017-05-10 09:30:48,457 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-10 09:30:48,497 : INFO : PROGRESS: at sentence #10000, processed 224982 words, keeping 13718 word types\n",
      "2017-05-10 09:30:48,541 : INFO : PROGRESS: at sentence #20000, processed 450127 words, keeping 18907 word types\n",
      "2017-05-10 09:30:48,584 : INFO : PROGRESS: at sentence #30000, processed 673400 words, keeping 22545 word types\n",
      "2017-05-10 09:30:48,627 : INFO : PROGRESS: at sentence #40000, processed 897083 words, keeping 25463 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-10 09:30:48,671 : INFO : PROGRESS: at sentence #50000, processed 1121246 words, keeping 27934 word types\n",
      "2017-05-10 09:30:48,716 : INFO : PROGRESS: at sentence #60000, processed 1344770 words, keeping 30124 word types\n",
      "2017-05-10 09:30:48,759 : INFO : PROGRESS: at sentence #70000, processed 1567689 words, keeping 32207 word types\n",
      "2017-05-10 09:30:48,802 : INFO : PROGRESS: at sentence #80000, processed 1792261 words, keeping 34066 word types\n",
      "2017-05-10 09:30:48,846 : INFO : PROGRESS: at sentence #90000, processed 2014903 words, keeping 35758 word types\n",
      "2017-05-10 09:30:48,889 : INFO : PROGRESS: at sentence #100000, processed 2238809 words, keeping 37347 word types\n",
      "2017-05-10 09:30:48,932 : INFO : PROGRESS: at sentence #110000, processed 2462219 words, keeping 38780 word types\n",
      "2017-05-10 09:30:48,976 : INFO : PROGRESS: at sentence #120000, processed 2684714 words, keeping 40156 word types\n",
      "2017-05-10 09:30:49,019 : INFO : PROGRESS: at sentence #130000, processed 2908276 words, keeping 41494 word types\n",
      "2017-05-10 09:30:49,063 : INFO : PROGRESS: at sentence #140000, processed 3132974 words, keeping 42776 word types\n",
      "2017-05-10 09:30:49,107 : INFO : PROGRESS: at sentence #150000, processed 3357456 words, keeping 44010 word types\n",
      "2017-05-10 09:30:49,163 : INFO : PROGRESS: at sentence #160000, processed 3580818 words, keeping 45148 word types\n",
      "2017-05-10 09:30:49,217 : INFO : PROGRESS: at sentence #170000, processed 3802590 words, keeping 46228 word types\n",
      "2017-05-10 09:30:49,266 : INFO : PROGRESS: at sentence #180000, processed 4025074 words, keeping 47294 word types\n",
      "2017-05-10 09:30:49,309 : INFO : PROGRESS: at sentence #190000, processed 4246964 words, keeping 48301 word types\n",
      "2017-05-10 09:30:49,335 : INFO : collected 48808 word types from a corpus of 4363926 raw words and 195299 sentences\n",
      "2017-05-10 09:30:49,336 : INFO : Loading a fresh vocabulary\n",
      "2017-05-10 09:30:49,388 : INFO : min_count=5 retains 18056 unique words (36% of original 48808, drops 30752)\n",
      "2017-05-10 09:30:49,388 : INFO : min_count=5 leaves 4312296 word corpus (98% of original 4363926, drops 51630)\n",
      "2017-05-10 09:30:49,428 : INFO : deleting the raw counts dictionary of 48808 items\n",
      "2017-05-10 09:30:49,431 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-10 09:30:49,432 : INFO : downsampling leaves estimated 3224386 word corpus (74.8% of prior 4312296)\n",
      "2017-05-10 09:30:49,433 : INFO : estimated required memory for 18056 words and 300 dimensions: 52362400 bytes\n",
      "2017-05-10 09:30:49,498 : INFO : resetting layer weights\n",
      "2017-05-10 09:30:49,708 : INFO : training model with 4 workers on 18056 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-10 09:30:49,709 : INFO : expecting 195299 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-10 09:30:50,718 : INFO : PROGRESS: at 5.52% examples, 890715 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:51,730 : INFO : PROGRESS: at 10.78% examples, 863756 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:52,732 : INFO : PROGRESS: at 16.09% examples, 860716 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:53,741 : INFO : PROGRESS: at 20.41% examples, 817335 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:54,763 : INFO : PROGRESS: at 25.38% examples, 811183 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-10 09:30:55,782 : INFO : PROGRESS: at 29.55% examples, 785675 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:56,795 : INFO : PROGRESS: at 34.13% examples, 777529 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:57,806 : INFO : PROGRESS: at 38.30% examples, 763275 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:58,814 : INFO : PROGRESS: at 42.65% examples, 755730 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:30:59,818 : INFO : PROGRESS: at 46.95% examples, 749266 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-10 09:31:00,826 : INFO : PROGRESS: at 50.79% examples, 737054 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:01,836 : INFO : PROGRESS: at 55.04% examples, 732249 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:02,865 : INFO : PROGRESS: at 59.36% examples, 727756 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:03,880 : INFO : PROGRESS: at 63.65% examples, 724607 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:04,890 : INFO : PROGRESS: at 67.91% examples, 721565 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-10 09:31:05,905 : INFO : PROGRESS: at 72.30% examples, 720137 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-10 09:31:06,908 : INFO : PROGRESS: at 76.97% examples, 721859 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:07,912 : INFO : PROGRESS: at 81.83% examples, 725048 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:08,917 : INFO : PROGRESS: at 85.21% examples, 715577 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-10 09:31:09,931 : INFO : PROGRESS: at 88.50% examples, 705940 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-10 09:31:10,942 : INFO : PROGRESS: at 92.08% examples, 699468 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-10 09:31:11,951 : INFO : PROGRESS: at 95.64% examples, 693609 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-10 09:31:12,938 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-10 09:31:12,939 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-10 09:31:12,947 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-10 09:31:12,949 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-10 09:31:12,950 : INFO : training on 21819630 raw words (16122707 effective words) took 23.2s, 693875 effective words/s\n",
      "2017-05-10 09:31:12,952 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-10 09:31:13,047 : INFO : saving Word2Vec object under Results/dros_both_ents_new_model, separately None\n",
      "2017-05-10 09:31:13,048 : INFO : not storing attribute syn0norm\n",
      "2017-05-10 09:31:13,049 : INFO : not storing attribute cum_table\n",
      "2017-05-10 09:31:13,440 : INFO : saved Results/dros_both_ents_new_model\n",
      "2017-05-10 09:31:13,645 : INFO : collecting all words and their counts\n",
      "2017-05-10 09:31:13,645 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-10 09:31:13,678 : INFO : collected 13129 word types from a corpus of 191220 raw words and 7508 sentences\n",
      "2017-05-10 09:31:13,678 : INFO : Loading a fresh vocabulary\n",
      "2017-05-10 09:31:13,692 : INFO : min_count=5 retains 3585 unique words (27% of original 13129, drops 9544)\n",
      "2017-05-10 09:31:13,693 : INFO : min_count=5 leaves 175729 word corpus (91% of original 191220, drops 15491)\n",
      "2017-05-10 09:31:13,701 : INFO : deleting the raw counts dictionary of 13129 items\n",
      "2017-05-10 09:31:13,702 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2017-05-10 09:31:13,702 : INFO : downsampling leaves estimated 126466 word corpus (72.0% of prior 175729)\n",
      "2017-05-10 09:31:13,703 : INFO : estimated required memory for 3585 words and 300 dimensions: 10396500 bytes\n",
      "2017-05-10 09:31:13,712 : INFO : resetting layer weights\n",
      "2017-05-10 09:31:13,753 : INFO : training model with 4 workers on 3585 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-10 09:31:13,754 : INFO : expecting 7508 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-10 09:31:14,386 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-10 09:31:14,399 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-10 09:31:14,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-10 09:31:14,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-10 09:31:14,406 : INFO : training on 956100 raw words (632427 effective words) took 0.6s, 977395 effective words/s\n",
      "2017-05-10 09:31:14,407 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-10 09:31:14,427 : INFO : saving Word2Vec object under Results/dros_strict_real_new_model, separately None\n",
      "2017-05-10 09:31:14,430 : INFO : not storing attribute syn0norm\n",
      "2017-05-10 09:31:14,431 : INFO : not storing attribute cum_table\n",
      "2017-05-10 09:31:14,501 : INFO : saved Results/dros_strict_real_new_model\n"
     ]
    }
   ],
   "source": [
    "gen_model = pred.make_w2v_model(new_dros_gen, 'dros_gen_real_new')\n",
    "both_ents_model = pred.make_w2v_model(new_dros_be, 'dros_both_ents_new')\n",
    "strict_model = pred.make_w2v_model(new_dros_strict, 'dros_strict_real_new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bioinformatics/Desktop/KU/Master_Thesis/Code/Proper_Class/2.0/prediction.py:174: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vec = np.divide(feature_vec,nwords)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting\n",
      "\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n",
      "\n",
      " Fitting XGBoost Model!\n",
      "\n",
      " Making Predictions\n"
     ]
    }
   ],
   "source": [
    "random_seeds = [144, 235, 905, 2895, 3462, 4225, 5056, 5192, 7751, 7813]\n",
    "dros_strict_real = pickle.load(open('Results/drosophila/Results/drosophila_mentions_strict_real.pkl', 'rb'))\n",
    "for seed in random_seeds:\n",
    "    strict_list_SR_comb = pred.make_models(dros_strict_real, \n",
    "                                             'dros_new_strict_'+str(seed),\n",
    "                                             prev_model=strict_model, \n",
    "                                             ran_state=seed)\n",
    "    \n",
    "    strict_list_GEN_comb = pred.make_models(dros_strict_real, \n",
    "                                             'dros_new_gen_'+str(seed),\n",
    "                                             prev_model=gen_model, \n",
    "                                             ran_state=seed)\n",
    "    \n",
    "    strict_list_BE_comb = pred.make_models(dros_strict_real, \n",
    "                                             'dros_new_be_'+str(seed),\n",
    "                                             prev_model=both_ents_model, \n",
    "                                             ran_state=seed)\n",
    "    strict_final_list = [strict_list_SR_comb, \n",
    "                         strict_list_GEN_comb, \n",
    "                         strict_list_BE_comb]\n",
    "    \n",
    "    print ('\\nPredicting\\n')\n",
    "    errors = []\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "\n",
    "    for entry in strict_final_list:\n",
    "        error_w2v_norm, probs_w2v_norm = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                             entry[2], entry[3])\n",
    "        fpr_w2v_norm, tpr_w2v_norm, _ = roc_curve(entry[3], probs_w2v_norm)\n",
    "        error_w2v_fs, probs_w2v_fs = pred.XGB_classifier(entry[0], entry[1],\n",
    "                                                         entry[2], entry[3],\n",
    "                                                         feature_selection=True)\n",
    "        fpr_w2v_fs, tpr_w2v_fs, _ = roc_curve(entry[3], probs_w2v_fs)\n",
    "\n",
    "        errors.append([error_w2v_norm, error_w2v_fs])\n",
    "        fpr.append([fpr_w2v_norm, fpr_w2v_fs])\n",
    "        tpr.append([tpr_w2v_norm, tpr_w2v_fs])\n",
    "\n",
    "    pickle.dump(errors, open('Results/dros_new_errors_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "    pickle.dump(fpr, open('Results/dros_new_fpr_pickle_'+str(seed)+'.pkl', 'wb'))\n",
    "    pickle.dump(tpr, open('Results/dros_new_tpr_pickle_'+str(seed)+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "drct = 'Results/drosophila/Results/Seeded/'\n",
    "errors_dros = mult_open(drct, '_errors_')\n",
    "fpr_dros = mult_open(drct, '_fpr_')\n",
    "tpr_dros = mult_open(drct, '_tpr_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict error drosophila auc=0.526 error=0.335\n",
      "Strict error drosophila auc=0.524 error=0.325\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.588 error=0.335\n",
      "Gen error drosophila auc=0.600 error=0.328\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.581 error=0.313\n",
      "BE error drosophila auc=0.591 error=0.324\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.471 error=0.484\n",
      "Strict error drosophila auc=0.465 error=0.477\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.535 error=0.464\n",
      "Gen error drosophila auc=0.544 error=0.451\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.502 error=0.464\n",
      "BE error drosophila auc=0.499 error=0.472\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.565 error=0.346\n",
      "Strict error drosophila auc=0.562 error=0.349\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.622 error=0.321\n",
      "Gen error drosophila auc=0.621 error=0.325\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.636 error=0.329\n",
      "BE error drosophila auc=0.624 error=0.323\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.499 error=0.392\n",
      "Strict error drosophila auc=0.506 error=0.387\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.573 error=0.376\n",
      "Gen error drosophila auc=0.592 error=0.363\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.588 error=0.347\n",
      "BE error drosophila auc=0.598 error=0.349\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.554 error=0.418\n",
      "Strict error drosophila auc=0.532 error=0.418\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.598 error=0.395\n",
      "Gen error drosophila auc=0.625 error=0.401\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.578 error=0.403\n",
      "BE error drosophila auc=0.597 error=0.410\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.483 error=0.482\n",
      "Strict error drosophila auc=0.478 error=0.483\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.516 error=0.469\n",
      "Gen error drosophila auc=0.503 error=0.474\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.490 error=0.466\n",
      "BE error drosophila auc=0.468 error=0.460\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.571 error=0.325\n",
      "Strict error drosophila auc=0.571 error=0.329\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.614 error=0.333\n",
      "Gen error drosophila auc=0.622 error=0.310\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.633 error=0.309\n",
      "BE error drosophila auc=0.637 error=0.313\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.535 error=0.406\n",
      "Strict error drosophila auc=0.533 error=0.410\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.606 error=0.381\n",
      "Gen error drosophila auc=0.621 error=0.379\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.626 error=0.359\n",
      "BE error drosophila auc=0.620 error=0.367\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.575 error=0.381\n",
      "Strict error drosophila auc=0.574 error=0.383\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.594 error=0.383\n",
      "Gen error drosophila auc=0.601 error=0.370\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.613 error=0.356\n",
      "BE error drosophila auc=0.609 error=0.380\n",
      "\n",
      "\n",
      "Strict error drosophila auc=0.570 error=0.422\n",
      "Strict error drosophila auc=0.580 error=0.414\n",
      "\n",
      "\n",
      "Gen error drosophila auc=0.636 error=0.383\n",
      "Gen error drosophila auc=0.640 error=0.376\n",
      "\n",
      "\n",
      "BE error drosophila auc=0.619 error=0.382\n",
      "BE error drosophila auc=0.623 error=0.387\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_err_dros_strict = []\n",
    "mean_auc_dros_strict = []\n",
    "mean_err_dros_gen = []\n",
    "mean_auc_dros_gen = []\n",
    "mean_err_dros_be = []\n",
    "mean_auc_dros_be = []\n",
    "for e, f, t in zip(errors_dros, fpr_dros, tpr_dros):\n",
    "    input_list = [[e, f, t]]\n",
    "    name_list = ['drosophila']\n",
    "    for idx in range(3):\n",
    "        for item, name in zip(input_list, name_list):\n",
    "            for fpr_item, tpr_item, error_item in zip(item[1][idx], item[2][idx], item[0][idx]):\n",
    "                roc_auc = auc(fpr_item, tpr_item)\n",
    "                auc_val = '%.3f' % roc_auc\n",
    "                error = '%.3f' % error_item\n",
    "                if idx == 0:\n",
    "                    mean_err_dros_strict.append(error_item)\n",
    "                    mean_auc_dros_strict.append(roc_auc)\n",
    "                    legend_label = 'Strict error ' + name + ' auc=' + str(auc_val) + ' error=' + str(error)\n",
    "                    print(legend_label)\n",
    "                elif idx == 1:\n",
    "                    mean_err_dros_gen.append(error_item)\n",
    "                    mean_auc_dros_gen.append(roc_auc)\n",
    "                    legend_label = 'Gen error ' + name + ' auc=' + str(auc_val) + ' error=' + str(error)\n",
    "                    print(legend_label)\n",
    "                elif idx == 2:\n",
    "                    mean_err_dros_be.append(error_item)\n",
    "                    mean_auc_dros_be.append(roc_auc)\n",
    "                    legend_label = 'BE error ' + name + ' auc=' + str(auc_val) + ' error=' + str(error)\n",
    "                    print(legend_label)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean_err_dros_org_strict = mean_err_dros_strict[0::2]\n",
    "mean_err_dros_fs_strict = mean_err_dros_strict[1::2]\n",
    "mean_auc_dros_org_strict = mean_auc_dros_strict[0::2]\n",
    "mean_auc_dros_fs_strict = mean_auc_dros_strict[1::2]\n",
    "\n",
    "mean_err_dros_org_gen = mean_err_dros_gen[0::2]\n",
    "mean_err_dros_fs_gen = mean_err_dros_gen[1::2]\n",
    "mean_auc_dros_org_gen = mean_auc_dros_gen[0::2]\n",
    "mean_auc_dros_fs_gen = mean_auc_dros_gen[1::2]\n",
    "\n",
    "mean_err_dros_org_be = mean_err_dros_be[0::2]\n",
    "mean_err_dros_fs_be = mean_err_dros_be[1::2]\n",
    "mean_auc_dros_org_be = mean_auc_dros_be[0::2]\n",
    "mean_auc_dros_fs_be = mean_auc_dros_be[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drosophila mean strict error original -  0.3990380362\n",
      "Drosophila mean strict error feature selection -  0.397609716508\n",
      "Drosophila mean strict AUC original -  0.534836787203\n",
      "Drosophila mean strict AUC feature selection -  0.532443067834\n",
      "\n",
      "\n",
      "Drosophila mean gen error original -  0.384021840951\n",
      "Drosophila mean gen error feature selection -  0.37778008806\n",
      "Drosophila mean gen AUC original -  0.588174304157\n",
      "Drosophila mean gen AUC feature selection -  0.596937560764\n",
      "\n",
      "\n",
      "Drosophila mean BE error original -  0.372779861786\n",
      "Drosophila mean BE error feature selection -  0.378436777746\n",
      "Drosophila mean BE AUC original -  0.586549018854\n",
      "Drosophila mean BE AUC feature selection -  0.586663948773\n"
     ]
    }
   ],
   "source": [
    "print('Drosophila mean strict error original - ', np.mean(mean_err_dros_org_strict))\n",
    "print('Drosophila mean strict error feature selection - ', np.mean(mean_err_dros_fs_strict))\n",
    "print('Drosophila mean strict AUC original - ', np.mean(mean_auc_dros_org_strict))\n",
    "print('Drosophila mean strict AUC feature selection - ', np.mean(mean_auc_dros_fs_strict))\n",
    "print('\\n')\n",
    "print('Drosophila mean gen error original - ', np.mean(mean_err_dros_org_gen))\n",
    "print('Drosophila mean gen error feature selection - ', np.mean(mean_err_dros_fs_gen))\n",
    "print('Drosophila mean gen AUC original - ', np.mean(mean_auc_dros_org_gen))\n",
    "print('Drosophila mean gen AUC feature selection - ', np.mean(mean_auc_dros_fs_gen))\n",
    "print('\\n')\n",
    "print('Drosophila mean BE error original - ', np.mean(mean_err_dros_org_be))\n",
    "print('Drosophila mean BE error feature selection - ', np.mean(mean_err_dros_fs_be))\n",
    "print('Drosophila mean BE AUC original - ', np.mean(mean_auc_dros_org_be))\n",
    "print('Drosophila mean BE AUC feature selection - ', np.mean(mean_auc_dros_fs_be))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pickle_144 = pickle.load(open('../Computerome-scripts/Results/result_list/dros_strict_list_BE_dims_param_100_144_results_list.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pickle_144[2]\n",
    "test = pickle_144[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = pickle.load(open('../Computerome-scripts/Results/metrics/dims_parameter_labels_pickle_144.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sum(pred[0])\n",
    "print(sum(pred[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_new = {'BE_1': [[0.48151577503429349],\n",
    "          [0.52329881656804733],\n",
    "          [0.5048468221307727],\n",
    "          [0.54288476851325218],\n",
    "          [0.54480258283052829],\n",
    "          [0.46460176991150443],\n",
    "          [0.5175911936110511],\n",
    "          [0.48262786596119933],\n",
    "          [0.46838410596026492],\n",
    "          [0.48198575617930456]],\n",
    " 'BE_2': [[0.48696844993141297],\n",
    "          [0.49796597633136097],\n",
    "          [0.5180155464106081],\n",
    "          [0.55841201228529169],\n",
    "          [0.49221975230231824],\n",
    "          [0.42601769911504428],\n",
    "          [0.52552341895100374],\n",
    "          [0.48042328042328042],\n",
    "          [0.50439735099337746],\n",
    "          [0.50345622119815669]],\n",
    " 'BE_3': [[0.39492455418381345],\n",
    "          [0.50443786982248517],\n",
    "          [0.48102423411065387],\n",
    "          [0.51177340461835963],\n",
    "          [0.52453159733248644],\n",
    "          [0.49725663716814156],\n",
    "          [0.51181739693503125],\n",
    "          [0.56534391534391537],\n",
    "          [0.49260927152317874],\n",
    "          [0.4796292417260159]],\n",
    " 'BE_4': [[0.4169410150891632],\n",
    "          [0.44193786982248523],\n",
    "          [0.46319158664837673],\n",
    "          [0.42196564668410874],\n",
    "          [0.48962633640309089],\n",
    "          [0.47637168141592917],\n",
    "          [0.48224692423915388],\n",
    "          [0.51904761904761909],\n",
    "          [0.48864900662251654],\n",
    "          [0.49005027230833687]],\n",
    " 'BE_5': [[0.44722222222222224],\n",
    "          [0.46606878698224857],\n",
    "          [0.48600823045267488],\n",
    "          [0.48674781026049369],\n",
    "          [0.48200486927066799],\n",
    "          [0.52619469026548671],\n",
    "          [0.49379451759119358],\n",
    "          [0.39947089947089942],\n",
    "          [0.51123178807947023],\n",
    "          [0.46308127356514456]],\n",
    " 'BE_6': [[0.40898491083676269],\n",
    "          [0.39607988165680474],\n",
    "          [0.49981710105166888],\n",
    "          [0.50910021613013323],\n",
    "          [0.48870011643908123],\n",
    "          [0.50283185840707967],\n",
    "          [0.53518238722210221],\n",
    "          [0.46913580246913572],\n",
    "          [0.54437086092715237],\n",
    "          [0.53649979053204866]],\n",
    " 'BE_7': [[0.48429355281207132],\n",
    "          [0.41586538461538458],\n",
    "          [0.48847736625514399],\n",
    "          [0.47241496985553399],\n",
    "          [0.52535196358632374],\n",
    "          [0.48238938053097341],\n",
    "          [0.52136844377293334],\n",
    "          [0.42469135802469132],\n",
    "          [0.5357218543046357],\n",
    "          [0.41432760787599499]],\n",
    " 'BE_8': [[0.45912208504801094],\n",
    "          [0.56434911242603547],\n",
    "          [0.50416095107453129],\n",
    "          [0.51359344784438621],\n",
    "          [0.41494654387636293],\n",
    "          [0.52061946902654865],\n",
    "          [0.52762788689833806],\n",
    "          [0.53156966490299828],\n",
    "          [0.49617218543046354],\n",
    "          [0.49319229157938826]],\n",
    " 'BE_9': [[0.45130315500685869],\n",
    "          [0.51229659763313617],\n",
    "          [0.48175582990397803],\n",
    "          [0.55619383460357186],\n",
    "          [0.51032073674182277],\n",
    "          [0.53132743362831858],\n",
    "          [0.5400388517159509],\n",
    "          [0.47610229276895938],\n",
    "          [0.45974834437086093],\n",
    "          [0.50827398408043567]],\n",
    " 'GEN_1': [[0.49756515775034299],\n",
    "           [0.4911242603550296],\n",
    "           [0.53950617283950619],\n",
    "           [0.57433739051302479],\n",
    "           [0.5443527045622949],\n",
    "           [0.57238938053097344],\n",
    "           [0.47949492769263968],\n",
    "           [0.52583774250440929],\n",
    "           [0.483841059602649],\n",
    "           [0.53786133221617094]],\n",
    " 'GEN_2': [[0.46951303155006857],\n",
    "           [0.5494637573964497],\n",
    "           [0.52171925011431186],\n",
    "           [0.46610169491525422],\n",
    "           [0.57632052503440245],\n",
    "           [0.44973451327433633],\n",
    "           [0.57295488884092383],\n",
    "           [0.50220458553791891],\n",
    "           [0.50075496688741716],\n",
    "           [0.49539170506912439]],\n",
    " 'GEN_3': [[0.53600823045267498],\n",
    "           [0.46588387573964496],\n",
    "           [0.51037951531778691],\n",
    "           [0.46280286656808101],\n",
    "           [0.58558272467449968],\n",
    "           [0.55920353982300885],\n",
    "           [0.55687459529462546],\n",
    "           [0.48236331569664903],\n",
    "           [0.4851788079470199],\n",
    "           [0.43621700879765396]],\n",
    " 'GEN_4': [[0.44564471879286693],\n",
    "           [0.53836908284023666],\n",
    "           [0.53589391860996793],\n",
    "           [0.4095097258559891],\n",
    "           [0.52114427860696511],\n",
    "           [0.49123893805309737],\n",
    "           [0.51742931146125626],\n",
    "           [0.44320987654320992],\n",
    "           [0.47409271523178809],\n",
    "           [0.5436740678676163]],\n",
    " 'GEN_5': [[0.47650891632373116],\n",
    "           [0.51053994082840226],\n",
    "           [0.47567443987197078],\n",
    "           [0.49033102036173359],\n",
    "           [0.59651212024981481],\n",
    "           [0.48176991150442483],\n",
    "           [0.46443988776170941],\n",
    "           [0.43694885361552027],\n",
    "           [0.47112582781456952],\n",
    "           [0.51272517804775875]],\n",
    " 'GEN_6': [[0.494684499314129],\n",
    "           [0.51331360946745563],\n",
    "           [0.46424325560128032],\n",
    "           [0.55949266295074507],\n",
    "           [0.56928125330792834],\n",
    "           [0.41734513274336282],\n",
    "           [0.46654435570904385],\n",
    "           [0.47460317460317464],\n",
    "           [0.46490066225165561],\n",
    "           [0.54252199413489732]],\n",
    " 'GEN_7': [[0.48981481481481481],\n",
    "           [0.49583949704142011],\n",
    "           [0.56556927297668047],\n",
    "           [0.43072460470936186],\n",
    "           [0.49171694717899866],\n",
    "           [0.49566371681415922],\n",
    "           [0.46541118066047915],\n",
    "           [0.52010582010582018],\n",
    "           [0.47170860927152314],\n",
    "           [0.47245496439044826]],\n",
    " 'GEN_8': [[0.45908779149519885],\n",
    "           [0.53855399408284022],\n",
    "           [0.50132601737540006],\n",
    "           [0.46962802866568076],\n",
    "           [0.50762146713242295],\n",
    "           [0.49858407079646017],\n",
    "           [0.50361536801208717],\n",
    "           [0.49832451499118163],\n",
    "           [0.47218543046357619],\n",
    "           [0.4894218684541265]],\n",
    " 'GEN_9': [[0.43168724279835391],\n",
    "           [0.50416050295857984],\n",
    "           [0.46099679926840414],\n",
    "           [0.46610169491525422],\n",
    "           [0.54829575526622221],\n",
    "           [0.42637168141592924],\n",
    "           [0.49692423915389594],\n",
    "           [0.48430335097001764],\n",
    "           [0.48650331125827817],\n",
    "           [0.48753665689149567]],\n",
    " 'SR_1': [[0.51313443072702336],\n",
    "          [0.45238535502958582],\n",
    "          [0.49739368998628258],\n",
    "          [0.4890797406438403],\n",
    "          [0.54723721816449666],\n",
    "          [0.51504424778761071],\n",
    "          [0.52751996546514135],\n",
    "          [0.47231040564373888],\n",
    "          [0.48517880794701984],\n",
    "          [0.50418935902806872]],\n",
    " 'SR_2': [[0.49375857338820295],\n",
    "          [0.55029585798816572],\n",
    "          [0.53511659807956102],\n",
    "          [0.50813331816630647],\n",
    "          [0.46594156875198484],\n",
    "          [0.50451327433628324],\n",
    "          [0.48597021368443771],\n",
    "          [0.53615520282186946],\n",
    "          [0.43145695364238412],\n",
    "          [0.51675743611227476]],\n",
    " 'SR_3': [[0.47434842249657061],\n",
    "          [0.49519230769230771],\n",
    "          [0.52299954275262905],\n",
    "          [0.54527357524741216],\n",
    "          [0.54705197417169471],\n",
    "          [0.39752212389380526],\n",
    "          [0.52816749406432117],\n",
    "          [0.476278659611993],\n",
    "          [0.41177483443708612],\n",
    "          [0.54561164641809801]],\n",
    " 'SR_4': [[0.521159122085048],\n",
    "          [0.52052514792899407],\n",
    "          [0.55601280292638311],\n",
    "          [0.48293709475600044],\n",
    "          [0.51812744786704767],\n",
    "          [0.46929203539823006],\n",
    "          [0.56135333477228577],\n",
    "          [0.45758377425044094],\n",
    "          [0.46599999999999997],\n",
    "          [0.49465856723921242]],\n",
    " 'SR_5': [[0.47668038408779145],\n",
    "          [0.45432692307692302],\n",
    "          [0.48943758573388207],\n",
    "          [0.50807644181549316],\n",
    "          [0.5270456229490843],\n",
    "          [0.47353982300884956],\n",
    "          [0.5075005396071659],\n",
    "          [0.46992945326278657],\n",
    "          [0.41374834437086094],\n",
    "          [0.45454545454545453]],\n",
    " 'SR_6': [[0.53007544581618671],\n",
    "          [0.42557322485207094],\n",
    "          [0.47370827617741196],\n",
    "          [0.50290069389147996],\n",
    "          [0.50693341801630143],\n",
    "          [0.44769911504424775],\n",
    "          [0.50372328944528377],\n",
    "          [0.52239858906525571],\n",
    "          [0.45511258278145705],\n",
    "          [0.44632383745286974]],\n",
    " 'SR_7': [[0.48288751714677636],\n",
    "          [0.50231139053254437],\n",
    "          [0.50420667581161405],\n",
    "          [0.5333295415766125],\n",
    "          [0.47501852439928016],\n",
    "          [0.47079646017699117],\n",
    "          [0.57554500323764302],\n",
    "          [0.53518518518518521],\n",
    "          [0.4633245033112583],\n",
    "          [0.46643276078759949]],\n",
    " 'SR_8': [[0.49629629629629635],\n",
    "          [0.49556213017751477],\n",
    "          [0.53191586648376776],\n",
    "          [0.5154134910704129],\n",
    "          [0.53876892135069332],\n",
    "          [0.47592920353982299],\n",
    "          [0.55169436650118708],\n",
    "          [0.48941798941798936],\n",
    "          [0.50158940397350982],\n",
    "          [0.46271470465018849]],\n",
    " 'SR_9': [[0.47877229080932787],\n",
    "          [0.49103180473372782],\n",
    "          [0.48678555098308185],\n",
    "          [0.46371288818109424],\n",
    "          [0.55612892981899009],\n",
    "          [0.46353982300884955],\n",
    "          [0.50857975393913235],\n",
    "          [0.49656084656084659],\n",
    "          [0.45838410596026491],\n",
    "          [0.55346669459572684]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.57433739051302479, 2: 0.57632052503440245, 3: 0.58558272467449968, 4: 0.56135333477228577, 5: 0.59651212024981481, 6: 0.56928125330792834, 7: 0.57554500323764302, 8: 0.56434911242603547, 9: 0.55619383460357186}\n"
     ]
    }
   ],
   "source": [
    "max_aucs = {}\n",
    "for i in range(9):\n",
    "    single_max = np.max([np.max(dict_new['SR_'+str(i+1)]), np.max(dict_new['GEN_'+str(i+1)]), np.max(dict_new['BE_'+str(i+1)])])\n",
    "    max_aucs[i+1] = single_max\n",
    "print(max_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "max(max_aucs.items(), key=operator.itemgetter(1))[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
