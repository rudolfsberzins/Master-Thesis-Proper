{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Core-scripts/')\n",
    "\n",
    "from parse_and_prepare import ProteinProteinInteractionClassifier as ppi\n",
    "import file_readers as fr\n",
    "import prediction as pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yeast_strict_real = pickle.load(open('../../Results/Yeast/yeast_mentions_strict_real.pkl', 'rb'))\n",
    "yeast_gen_real = pickle.load(open('../../Results/Yeast/yeast_mentions_gen_real.pkl', 'rb'))\n",
    "yeast_be_real = pickle.load(open('../../Results/Yeast/yeast_mentions_be_real.pkl', 'rb'))\n",
    "random_seeds = [144, 235, 905, 2895, 3462, 4225, 5056, 5192, 7751, 7813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    real_tr_te_name = 'Yeast/train_test/yeast_tr_te_split_' + str(seed)\n",
    "    train_data, b, c, d = pred.manual_train_test_split(yeast_strict_real, real_tr_te_name, random_state=seed ,test_set_prop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 12:31:58,774 : INFO : collecting all words and their counts\n",
      "2017-05-16 12:31:58,774 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-16 12:31:58,816 : INFO : collected 11431 word types from a corpus of 188505 raw words and 7145 sentences\n",
      "2017-05-16 12:31:58,817 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 12:31:58,830 : INFO : min_count=5 retains 3346 unique words (29% of original 11431, drops 8085)\n",
      "2017-05-16 12:31:58,832 : INFO : min_count=5 leaves 175425 word corpus (93% of original 188505, drops 13080)\n",
      "2017-05-16 12:31:58,840 : INFO : deleting the raw counts dictionary of 11431 items\n",
      "2017-05-16 12:31:58,841 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2017-05-16 12:31:58,842 : INFO : downsampling leaves estimated 125739 word corpus (71.7% of prior 175425)\n",
      "2017-05-16 12:31:58,843 : INFO : estimated required memory for 3346 words and 300 dimensions: 9703400 bytes\n",
      "2017-05-16 12:31:58,852 : INFO : resetting layer weights\n",
      "2017-05-16 12:31:58,895 : INFO : training model with 4 workers on 3346 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-16 12:31:58,895 : INFO : expecting 7145 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 12:31:59,409 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 12:31:59,414 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 12:31:59,418 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 12:31:59,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 12:31:59,421 : INFO : training on 942525 raw words (628611 effective words) took 0.5s, 1211803 effective words/s\n",
      "2017-05-16 12:31:59,421 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-16 12:31:59,439 : INFO : saving Word2Vec object under ../../Results/Yeast/yeast_strict_model, separately None\n",
      "2017-05-16 12:31:59,440 : INFO : not storing attribute syn0norm\n",
      "2017-05-16 12:31:59,440 : INFO : not storing attribute cum_table\n",
      "2017-05-16 12:31:59,513 : INFO : saved ../../Results/Yeast/yeast_strict_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 12:32:00,025 : INFO : collecting all words and their counts\n",
      "2017-05-16 12:32:00,026 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-16 12:32:00,070 : INFO : PROGRESS: at sentence #10000, processed 243957 words, keeping 11334 word types\n",
      "2017-05-16 12:32:00,118 : INFO : PROGRESS: at sentence #20000, processed 486019 words, keeping 14899 word types\n",
      "2017-05-16 12:32:00,163 : INFO : PROGRESS: at sentence #30000, processed 729058 words, keeping 17564 word types\n",
      "2017-05-16 12:32:00,207 : INFO : PROGRESS: at sentence #40000, processed 970773 words, keeping 19761 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 12:32:00,242 : INFO : collected 21147 word types from a corpus of 1155938 raw words and 47662 sentences\n",
      "2017-05-16 12:32:00,243 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 12:32:00,266 : INFO : min_count=5 retains 8077 unique words (38% of original 21147, drops 13070)\n",
      "2017-05-16 12:32:00,266 : INFO : min_count=5 leaves 1134115 word corpus (98% of original 1155938, drops 21823)\n",
      "2017-05-16 12:32:00,288 : INFO : deleting the raw counts dictionary of 21147 items\n",
      "2017-05-16 12:32:00,290 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2017-05-16 12:32:00,290 : INFO : downsampling leaves estimated 829411 word corpus (73.1% of prior 1134115)\n",
      "2017-05-16 12:32:00,291 : INFO : estimated required memory for 8077 words and 300 dimensions: 23423300 bytes\n",
      "2017-05-16 12:32:00,319 : INFO : resetting layer weights\n",
      "2017-05-16 12:32:00,411 : INFO : training model with 4 workers on 8077 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-16 12:32:00,411 : INFO : expecting 47662 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-16 12:32:01,427 : INFO : PROGRESS: at 21.07% examples, 864959 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:02,432 : INFO : PROGRESS: at 45.43% examples, 934920 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:03,434 : INFO : PROGRESS: at 70.84% examples, 973289 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:04,439 : INFO : PROGRESS: at 97.09% examples, 1000718 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:04,535 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 12:32:04,540 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 12:32:04,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 12:32:04,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 12:32:04,549 : INFO : training on 5779690 raw words (4146729 effective words) took 4.1s, 1003388 effective words/s\n",
      "2017-05-16 12:32:04,549 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-16 12:32:04,591 : INFO : saving Word2Vec object under ../../Results/Yeast/yeast_gen_model, separately None\n",
      "2017-05-16 12:32:04,592 : INFO : not storing attribute syn0norm\n",
      "2017-05-16 12:32:04,592 : INFO : not storing attribute cum_table\n",
      "2017-05-16 12:32:04,764 : INFO : saved ../../Results/Yeast/yeast_gen_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 12:32:07,052 : INFO : collecting all words and their counts\n",
      "2017-05-16 12:32:07,053 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-16 12:32:07,100 : INFO : PROGRESS: at sentence #10000, processed 229844 words, keeping 12368 word types\n",
      "2017-05-16 12:32:07,150 : INFO : PROGRESS: at sentence #20000, processed 458659 words, keeping 16803 word types\n",
      "2017-05-16 12:32:07,198 : INFO : PROGRESS: at sentence #30000, processed 687887 words, keeping 20163 word types\n",
      "2017-05-16 12:32:07,251 : INFO : PROGRESS: at sentence #40000, processed 916854 words, keeping 22943 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-16 12:32:07,302 : INFO : PROGRESS: at sentence #50000, processed 1145367 words, keeping 25239 word types\n",
      "2017-05-16 12:32:07,353 : INFO : PROGRESS: at sentence #60000, processed 1372926 words, keeping 27354 word types\n",
      "2017-05-16 12:32:07,393 : INFO : PROGRESS: at sentence #70000, processed 1602439 words, keeping 29086 word types\n",
      "2017-05-16 12:32:07,436 : INFO : PROGRESS: at sentence #80000, processed 1832259 words, keeping 30779 word types\n",
      "2017-05-16 12:32:07,479 : INFO : PROGRESS: at sentence #90000, processed 2061578 words, keeping 32268 word types\n",
      "2017-05-16 12:32:07,522 : INFO : PROGRESS: at sentence #100000, processed 2291831 words, keeping 33740 word types\n",
      "2017-05-16 12:32:07,571 : INFO : PROGRESS: at sentence #110000, processed 2520309 words, keeping 35115 word types\n",
      "2017-05-16 12:32:07,611 : INFO : PROGRESS: at sentence #120000, processed 2749954 words, keeping 36217 word types\n",
      "2017-05-16 12:32:07,654 : INFO : PROGRESS: at sentence #130000, processed 2977684 words, keeping 37347 word types\n",
      "2017-05-16 12:32:07,696 : INFO : PROGRESS: at sentence #140000, processed 3207263 words, keeping 38585 word types\n",
      "2017-05-16 12:32:07,739 : INFO : PROGRESS: at sentence #150000, processed 3436477 words, keeping 39701 word types\n",
      "2017-05-16 12:32:07,783 : INFO : PROGRESS: at sentence #160000, processed 3666369 words, keeping 40716 word types\n",
      "2017-05-16 12:32:07,826 : INFO : PROGRESS: at sentence #170000, processed 3895405 words, keeping 41735 word types\n",
      "2017-05-16 12:32:07,869 : INFO : PROGRESS: at sentence #180000, processed 4123083 words, keeping 42691 word types\n",
      "2017-05-16 12:32:07,913 : INFO : PROGRESS: at sentence #190000, processed 4350214 words, keeping 43579 word types\n",
      "2017-05-16 12:32:07,959 : INFO : PROGRESS: at sentence #200000, processed 4578613 words, keeping 44513 word types\n",
      "2017-05-16 12:32:08,001 : INFO : collected 45300 word types from a corpus of 4784186 raw words and 208973 sentences\n",
      "2017-05-16 12:32:08,001 : INFO : Loading a fresh vocabulary\n",
      "2017-05-16 12:32:08,053 : INFO : min_count=5 retains 16820 unique words (37% of original 45300, drops 28480)\n",
      "2017-05-16 12:32:08,054 : INFO : min_count=5 leaves 4736602 word corpus (99% of original 4784186, drops 47584)\n",
      "2017-05-16 12:32:08,094 : INFO : deleting the raw counts dictionary of 45300 items\n",
      "2017-05-16 12:32:08,097 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2017-05-16 12:32:08,097 : INFO : downsampling leaves estimated 3527961 word corpus (74.5% of prior 4736602)\n",
      "2017-05-16 12:32:08,098 : INFO : estimated required memory for 16820 words and 300 dimensions: 48778000 bytes\n",
      "2017-05-16 12:32:08,162 : INFO : resetting layer weights\n",
      "2017-05-16 12:32:08,367 : INFO : training model with 4 workers on 16820 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=6\n",
      "2017-05-16 12:32:08,368 : INFO : expecting 208973 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-16 12:32:09,382 : INFO : PROGRESS: at 3.96% examples, 694344 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:10,386 : INFO : PROGRESS: at 8.84% examples, 775941 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:11,390 : INFO : PROGRESS: at 13.85% examples, 810832 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 12:32:12,396 : INFO : PROGRESS: at 19.00% examples, 833163 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:13,405 : INFO : PROGRESS: at 24.63% examples, 863633 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:14,407 : INFO : PROGRESS: at 29.51% examples, 862941 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:15,416 : INFO : PROGRESS: at 33.47% examples, 838741 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:16,421 : INFO : PROGRESS: at 38.32% examples, 840112 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:17,422 : INFO : PROGRESS: at 42.08% examples, 820369 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:18,439 : INFO : PROGRESS: at 45.34% examples, 794512 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 12:32:19,441 : INFO : PROGRESS: at 48.71% examples, 776484 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:20,442 : INFO : PROGRESS: at 52.17% examples, 762717 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:21,460 : INFO : PROGRESS: at 55.59% examples, 749527 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-16 12:32:22,462 : INFO : PROGRESS: at 60.45% examples, 756887 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 12:32:23,467 : INFO : PROGRESS: at 65.38% examples, 764033 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:24,477 : INFO : PROGRESS: at 70.34% examples, 770539 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:25,483 : INFO : PROGRESS: at 75.14% examples, 774768 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:26,495 : INFO : PROGRESS: at 79.95% examples, 778245 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:27,499 : INFO : PROGRESS: at 84.75% examples, 781662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-16 12:32:28,501 : INFO : PROGRESS: at 89.50% examples, 784464 words/s, in_qsize 5, out_qsize 2\n",
      "2017-05-16 12:32:29,502 : INFO : PROGRESS: at 94.30% examples, 787427 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-16 12:32:30,509 : INFO : PROGRESS: at 97.77% examples, 779189 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-16 12:32:31,204 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-16 12:32:31,209 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-16 12:32:31,219 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-16 12:32:31,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-16 12:32:31,247 : INFO : training on 23920930 raw words (17638615 effective words) took 22.9s, 771192 effective words/s\n",
      "2017-05-16 12:32:31,249 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-16 12:32:31,410 : INFO : saving Word2Vec object under ../../Results/Yeast/yeast_be_model, separately None\n",
      "2017-05-16 12:32:31,411 : INFO : not storing attribute syn0norm\n",
      "2017-05-16 12:32:31,412 : INFO : not storing attribute cum_table\n",
      "2017-05-16 12:32:31,947 : INFO : saved ../../Results/Yeast/yeast_be_model\n"
     ]
    }
   ],
   "source": [
    "yeast_w2v_model_strict = pred.make_w2v_model(yeast_strict_real, 'Yeast/yeast_strict', [300, 5, 4, 6, 0.001])\n",
    "yeast_w2v_model_gen = pred.make_w2v_model(yeast_gen_real, 'Yeast/yeast_gen', [300, 5, 4, 6, 0.001])\n",
    "yeast_w2v_model_be = pred.make_w2v_model(yeast_be_real, 'Yeast/yeast_be', [300, 5, 4, 6, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    data_name = '../../Results/Yeast/train_test/yeast_tr_te_split_' + str(seed)\n",
    "    train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "    train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "    test_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "    test_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "\n",
    "    w2v_train_vecs, w2v_test_vecs = pred.word_2_vec_feat_vecs(train_data, test_data, yeast_w2v_model_strict, feature_count=300)\n",
    "\n",
    "    strict_list_SR_dims_param = [w2v_train_vecs, w2v_test_vecs,\n",
    "                                 train_labels, test_labels]\n",
    "\n",
    "    w2v_train_vecs, w2v_test_vecs = pred.word_2_vec_feat_vecs(train_data, test_data, yeast_w2v_model_gen, feature_count=300)\n",
    "\n",
    "    strict_list_GEN_dims_param = [w2v_train_vecs, w2v_test_vecs,\n",
    "                                  train_labels, test_labels]\n",
    "\n",
    "    w2v_train_vecs, w2v_test_vecs = pred.word_2_vec_feat_vecs(train_data, test_data, yeast_w2v_model_be, feature_count=300)\n",
    "\n",
    "    strict_list_BE_dims_param = [w2v_train_vecs, w2v_test_vecs,\n",
    "                                 train_labels, test_labels]\n",
    "\n",
    "    pickle.dump(strict_list_SR_dims_param, open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "    pickle.dump(strict_list_GEN_dims_param, open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "    pickle.dump(strict_list_BE_dims_param, open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train_vecs, train_labels, w2v_model_type, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param=alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_vecs, \n",
    "                              label=train_labels)\n",
    "        cvresult = xgb.cv(xgb_param, \n",
    "                          xgtrain, \n",
    "                          num_boost_round=alg.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds, \n",
    "                          metrics='auc', \n",
    "                          early_stopping_rounds=50)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "    #fit the algorithm on the data\n",
    "    alg.fit(train_vecs, train_labels, eval_metric='auc')\n",
    "    \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train_vecs)\n",
    "    train_predprob = alg.predict_proba(train_vecs)[:,1]\n",
    "    \n",
    "    #Print Model report:\n",
    "    print(w2v_model_type, '\\nModel Report')\n",
    "    print(w2v_model_type, 'Accuracy: %.4g' % metrics.accuracy_score(train_labels, train_predictions))\n",
    "    print(w2v_model_type, 'AUC Score (Train): %f' % metrics.roc_auc_score(train_labels, train_predprob))\n",
    "    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importance')\n",
    "#     plt.ylabel('Feature Importance Score')\n",
    "\n",
    "    error = 1-metrics.accuracy_score(train_labels, train_predictions)\n",
    "    auc = metrics.roc_auc_score(train_labels, train_predprob)\n",
    "    \n",
    "    return error, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6019\n",
      "STRICT AUC Score (Train): 0.628022\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6309\n",
      "GEN AUC Score (Train): 0.672836\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6039\n",
      "BE AUC Score (Train): 0.654637\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate = 0.1, \n",
    "                     n_estimators=1000, \n",
    "                     max_depth=5, \n",
    "                     min_child_weight=1, \n",
    "                     gamma=0, \n",
    "                     subsample=0.8, \n",
    "                     colsample_bytree=0.8, \n",
    "                     objective='binary:logistic', \n",
    "                     nthread=4, \n",
    "                     scale_pos_weight=1, \n",
    "                     seed=24)\n",
    "\n",
    "# strict_error_list = []\n",
    "# strict_auc_list = []\n",
    "# gen_error_list = []\n",
    "# gen_auc_list = []\n",
    "# be_error_list = []\n",
    "# be_auc_list = []\n",
    "\n",
    "seed = 144\n",
    "  \n",
    "strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_dims_param_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_dims_param_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_dims_param_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "print(seed)\n",
    "strict_error, strict_auc = modelfit(xgb1, strict_list[0], strict_list[2], 'STRICT')\n",
    "gen_error, gen_auc = modelfit(xgb1, gen_list[0], gen_list[2], 'GEN')\n",
    "be_error, be_auc = modelfit(xgb1, be_list[0], be_list[2], 'BE')\n",
    "\n",
    "# strict_error_list.append(strict_error)\n",
    "# strict_auc_list.append(strict_auc)\n",
    "# gen_error_list.append(gen_error)\n",
    "# gen_auc_list.append(gen_auc)\n",
    "# be_error_list.append(be_error)\n",
    "# be_auc_list.append(be_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.7953\n",
      "STRICT AUC Score (Train): 0.874488\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.8233\n",
      "GEN AUC Score (Train): 0.905831\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.8428\n",
      "BE AUC Score (Train): 0.918466\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate = 0.1, \n",
    "                     n_estimators=1000, \n",
    "                     max_depth=5, \n",
    "                     min_child_weight=1, \n",
    "                     gamma=0, \n",
    "                     subsample=0.8, \n",
    "                     colsample_bytree=0.8, \n",
    "                     objective='binary:logistic', \n",
    "                     nthread=4, \n",
    "                     scale_pos_weight=1, \n",
    "                     seed=24)\n",
    "\n",
    "# strict_error_list = []\n",
    "# strict_auc_list = []\n",
    "# gen_error_list = []\n",
    "# gen_auc_list = []\n",
    "# be_error_list = []\n",
    "# be_auc_list = []\n",
    "\n",
    "seed = 144\n",
    "  \n",
    "strict_list_old = pickle.load(open('../../Results/Yeast/old_results/yeast_strict_real_pure_144no_BOW_results_list.pkl', 'rb'))\n",
    "gen_list_old = pickle.load(open('../../Results/Yeast/old_results/yeast_strict_real_gen_mod_144no_BOW_results_list.pkl', 'rb'))\n",
    "be_list_old = pickle.load(open('../../Results/Yeast/old_results/yeast_strict_real_be_mod_144no_BOW_results_list.pkl', 'rb'))\n",
    "print(seed)\n",
    "strict_error, strict_auc = modelfit(xgb1, strict_list[0], strict_list[2], 'STRICT')\n",
    "gen_error, gen_auc = modelfit(xgb1, gen_list[0], gen_list[2], 'GEN')\n",
    "be_error, be_auc = modelfit(xgb1, be_list[0], be_list[2], 'BE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.9327\n",
      "STRICT AUC Score (Train): 0.990888\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6287\n",
      "GEN AUC Score (Train): 0.684977\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.654\n",
      "BE AUC Score (Train): 0.724163\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate = 0.1, \n",
    "                     n_estimators=1000, \n",
    "                     max_depth=5, \n",
    "                     min_child_weight=1, \n",
    "                     gamma=0, \n",
    "                     subsample=0.8, \n",
    "                     colsample_bytree=0.8, \n",
    "                     objective='binary:logistic', \n",
    "                     nthread=4, \n",
    "                     scale_pos_weight=1, \n",
    "                     seed=24)\n",
    "\n",
    "# strict_error_list = []\n",
    "# strict_auc_list = []\n",
    "# gen_error_list = []\n",
    "# gen_auc_list = []\n",
    "# be_error_list = []\n",
    "# be_auc_list = []\n",
    "\n",
    "seed = 144\n",
    "  \n",
    "strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "print(seed)\n",
    "strict_error, strict_auc = modelfit(xgb1, strict_list[0], strict_list[2], 'STRICT')\n",
    "gen_error, gen_auc = modelfit(xgb1, gen_list[0], gen_list[2], 'GEN')\n",
    "be_error, be_auc = modelfit(xgb1, be_list[0], be_list[2], 'BE')\n",
    "\n",
    "# strict_error_list.append(strict_error)\n",
    "# strict_auc_list.append(strict_auc)\n",
    "# gen_error_list.append(gen_error)\n",
    "# gen_auc_list.append(gen_auc)\n",
    "# be_error_list.append(be_error)\n",
    "# be_auc_list.append(be_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strict_list[2] == strict_list_old[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Results Error:  0.378943362667 , Auc:  0.66059997443\n",
      "Gen Results Error:  0.371137665262 , Auc:  0.669424603342\n",
      "Be Results Error:  0.381628285206 , Auc:  0.656050922499\n"
     ]
    }
   ],
   "source": [
    "print('Strict Results Error: ', np.mean(strict_error_list), ', Auc: ', np.mean(strict_auc_list))\n",
    "print('Gen Results Error: ', np.mean(gen_error_list), ', Auc: ', np.mean(gen_auc_list))\n",
    "print('Be Results Error: ', np.mean(be_error_list), ', Auc: ', np.mean(be_auc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    'max_depth':[3,4,5,6,7,8,9,10],\n",
    "    'min_child_weight':[1,2,3,4,5,6]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, \n",
    "                                                  n_estimators=140, \n",
    "                                                  max_depth=5, \n",
    "                                                  min_child_weight=1, \n",
    "                                                  gamma=0, \n",
    "                                                  subsample=0.8, \n",
    "                                                  colsample_bytree=0.8, \n",
    "                                                  objective='binary:logistic', \n",
    "                                                  nthread=4, \n",
    "                                                  scale_pos_weight=1, \n",
    "                                                  seed=24), \n",
    "                        param_grid=param_test1, \n",
    "                        scoring='roc_auc', \n",
    "                        n_jobs=4, \n",
    "                        iid=False, \n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ac6ecebb9b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom_seeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstrict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../Results/Yeast/result_list/yeast_strict_list_SR_dims_param_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_results_list.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrict_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstrict_param_test1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \"\"\"\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    571\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m                 for train, test in cv)\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "strict_param_test1 = []\n",
    "for seed in random_seeds:\n",
    "    strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_dims_param_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gsearch1.fit(strict_list[0], strict_list[2])\n",
    "    strict_param_test1.append((seed, (gsearch1.best_params_, gsearch1.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gen_param_test1 = []\n",
    "for seed in random_seeds:\n",
    "    gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_dims_param_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gsearch1.fit(gen_list[0], gen_list[2])\n",
    "    gen_param_test1.append((seed, (gsearch1.best_params_, gsearch1.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "be_param_test1 = []\n",
    "for seed in random_seeds:\n",
    "    be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_dims_param_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gsearch1.fit(be_list[0], be_list[2])\n",
    "    be_param_test1.append((seed, (gsearch1.best_params_, gsearch1.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('\\n', strict_param_test1[i], '\\n', gen_param_test1[i], '\\n', be_param_test1[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
