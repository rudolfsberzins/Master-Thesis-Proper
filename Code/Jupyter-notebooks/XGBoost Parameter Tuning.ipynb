{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Core-scripts/')\n",
    "\n",
    "from parse_and_prepare import ProteinProteinInteractionClassifier as ppi\n",
    "import file_readers as fr\n",
    "import prediction as pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yeast_strict_real = pickle.load(open('../../Results/Yeast/yeast_mentions_strict_real.pkl', 'rb'))\n",
    "yeast_gen_real = pickle.load(open('../../Results/Yeast/yeast_mentions_gen_real.pkl', 'rb'))\n",
    "yeast_be_real = pickle.load(open('../../Results/Yeast/yeast_mentions_be_real.pkl', 'rb'))\n",
    "random_seeds = [144, 235, 905, 2895, 3462, 4225, 5056, 5192, 7751, 7813]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    real_tr_te_name = 'Yeast/train_test/yeast_tr_te_split_' + str(seed)\n",
    "    train_data, b, c, d = pred.manual_train_test_split(yeast_strict_real, real_tr_te_name, random_state=seed ,test_set_prop=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-27 15:31:53,355 : INFO : collecting all words and their counts\n",
      "2017-05-27 15:31:53,356 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-27 15:31:53,401 : INFO : collected 11431 word types from a corpus of 188505 raw words and 7145 sentences\n",
      "2017-05-27 15:31:53,402 : INFO : Loading a fresh vocabulary\n",
      "2017-05-27 15:31:53,419 : INFO : min_count=6 retains 2978 unique words (26% of original 11431, drops 8453)\n",
      "2017-05-27 15:31:53,420 : INFO : min_count=6 leaves 173585 word corpus (92% of original 188505, drops 14920)\n",
      "2017-05-27 15:31:53,430 : INFO : deleting the raw counts dictionary of 11431 items\n",
      "2017-05-27 15:31:53,431 : INFO : sample=0.0001 downsamples 549 most-common words\n",
      "2017-05-27 15:31:53,432 : INFO : downsampling leaves estimated 74593 word corpus (43.0% of prior 173585)\n",
      "2017-05-27 15:31:53,432 : INFO : estimated required memory for 2978 words and 600 dimensions: 23526200 bytes\n",
      "2017-05-27 15:31:53,437 : INFO : constructing a huffman tree from 2978 words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n",
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-27 15:31:53,513 : INFO : built huffman tree with maximum node depth 15\n",
      "2017-05-27 15:31:53,520 : INFO : resetting layer weights\n",
      "2017-05-27 15:31:53,569 : INFO : training model with 4 workers on 2978 vocabulary and 600 features, using sg=1 hs=1 sample=0.0001 negative=5 window=7\n",
      "2017-05-27 15:31:53,569 : INFO : expecting 7145 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-27 15:31:54,868 : INFO : PROGRESS: at 13.77% examples, 39538 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:31:56,040 : INFO : PROGRESS: at 30.70% examples, 46446 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:31:57,088 : INFO : PROGRESS: at 43.38% examples, 46099 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:31:58,095 : INFO : PROGRESS: at 59.33% examples, 48993 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:31:59,177 : INFO : PROGRESS: at 72.02% examples, 47957 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:00,231 : INFO : PROGRESS: at 85.75% examples, 48079 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:00,972 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-27 15:32:01,035 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-27 15:32:01,101 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-27 15:32:01,113 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-27 15:32:01,114 : INFO : training on 942525 raw words (373217 effective words) took 7.5s, 49502 effective words/s\n",
      "2017-05-27 15:32:01,114 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-27 15:32:01,134 : INFO : saving Word2Vec object under ../../Results/Yeast/models/yeast_strict_model, separately None\n",
      "2017-05-27 15:32:01,134 : INFO : not storing attribute syn0norm\n",
      "2017-05-27 15:32:01,135 : INFO : not storing attribute cum_table\n",
      "2017-05-27 15:32:01,305 : INFO : saved ../../Results/Yeast/models/yeast_strict_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-27 15:32:01,839 : INFO : collecting all words and their counts\n",
      "2017-05-27 15:32:01,840 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-27 15:32:01,880 : INFO : PROGRESS: at sentence #10000, processed 241629 words, keeping 11026 word types\n",
      "2017-05-27 15:32:01,921 : INFO : PROGRESS: at sentence #20000, processed 486297 words, keeping 15003 word types\n",
      "2017-05-27 15:32:01,963 : INFO : PROGRESS: at sentence #30000, processed 729082 words, keeping 17548 word types\n",
      "2017-05-27 15:32:02,005 : INFO : PROGRESS: at sentence #40000, processed 971013 words, keeping 19661 word types\n",
      "2017-05-27 15:32:02,037 : INFO : collected 21147 word types from a corpus of 1155938 raw words and 47662 sentences\n",
      "2017-05-27 15:32:02,038 : INFO : Loading a fresh vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-27 15:32:02,064 : INFO : min_count=6 retains 7355 unique words (34% of original 21147, drops 13792)\n",
      "2017-05-27 15:32:02,064 : INFO : min_count=6 leaves 1130505 word corpus (97% of original 1155938, drops 25433)\n",
      "2017-05-27 15:32:02,082 : INFO : deleting the raw counts dictionary of 21147 items\n",
      "2017-05-27 15:32:02,084 : INFO : sample=0.0001 downsamples 521 most-common words\n",
      "2017-05-27 15:32:02,085 : INFO : downsampling leaves estimated 513926 word corpus (45.5% of prior 1130505)\n",
      "2017-05-27 15:32:02,085 : INFO : estimated required memory for 7355 words and 600 dimensions: 58104500 bytes\n",
      "2017-05-27 15:32:02,097 : INFO : constructing a huffman tree from 7355 words\n",
      "2017-05-27 15:32:02,271 : INFO : built huffman tree with maximum node depth 18\n",
      "2017-05-27 15:32:02,286 : INFO : resetting layer weights\n",
      "2017-05-27 15:32:02,399 : INFO : training model with 4 workers on 7355 vocabulary and 600 features, using sg=1 hs=1 sample=0.0001 negative=5 window=7\n",
      "2017-05-27 15:32:02,399 : INFO : expecting 47662 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-27 15:32:03,574 : INFO : PROGRESS: at 1.56% examples, 33982 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:04,665 : INFO : PROGRESS: at 3.64% examples, 41126 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:05,679 : INFO : PROGRESS: at 6.22% examples, 48620 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:06,797 : INFO : PROGRESS: at 7.76% examples, 45321 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:07,928 : INFO : PROGRESS: at 9.83% examples, 45658 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:09,080 : INFO : PROGRESS: at 12.07% examples, 46464 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:10,114 : INFO : PROGRESS: at 13.46% examples, 44850 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:11,132 : INFO : PROGRESS: at 15.71% examples, 46254 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:32:12,151 : INFO : PROGRESS: at 17.60% examples, 46464 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:13,309 : INFO : PROGRESS: at 19.34% examples, 45595 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:14,317 : INFO : PROGRESS: at 21.60% examples, 46595 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:15,424 : INFO : PROGRESS: at 23.52% examples, 46374 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:16,491 : INFO : PROGRESS: at 25.40% examples, 46330 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:17,621 : INFO : PROGRESS: at 26.95% examples, 45504 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:18,649 : INFO : PROGRESS: at 29.19% examples, 46168 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:19,878 : INFO : PROGRESS: at 31.42% examples, 46222 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:20,949 : INFO : PROGRESS: at 33.50% examples, 46431 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:22,088 : INFO : PROGRESS: at 35.40% examples, 46230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:23,098 : INFO : PROGRESS: at 37.64% examples, 46773 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:24,144 : INFO : PROGRESS: at 39.73% examples, 46972 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:25,187 : INFO : PROGRESS: at 41.99% examples, 47363 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:26,300 : INFO : PROGRESS: at 44.08% examples, 47391 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:32:27,436 : INFO : PROGRESS: at 46.30% examples, 47539 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:28,442 : INFO : PROGRESS: at 48.36% examples, 47743 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:29,489 : INFO : PROGRESS: at 50.08% examples, 47527 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:30,587 : INFO : PROGRESS: at 51.64% examples, 47097 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:31,632 : INFO : PROGRESS: at 53.36% examples, 46931 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:32,668 : INFO : PROGRESS: at 55.62% examples, 47230 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:33,669 : INFO : PROGRESS: at 57.51% examples, 47287 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:34,724 : INFO : PROGRESS: at 59.08% examples, 46975 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:35,744 : INFO : PROGRESS: at 61.00% examples, 47003 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:36,914 : INFO : PROGRESS: at 62.73% examples, 46695 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:37,984 : INFO : PROGRESS: at 64.63% examples, 46660 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:39,134 : INFO : PROGRESS: at 66.17% examples, 46287 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-27 15:32:40,426 : INFO : PROGRESS: at 68.23% examples, 46112 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:41,529 : INFO : PROGRESS: at 70.30% examples, 46172 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:42,601 : INFO : PROGRESS: at 72.53% examples, 46376 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:43,722 : INFO : PROGRESS: at 74.61% examples, 46414 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:44,729 : INFO : PROGRESS: at 76.70% examples, 46569 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:45,912 : INFO : PROGRESS: at 78.60% examples, 46422 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:47,096 : INFO : PROGRESS: at 80.69% examples, 46386 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:48,134 : INFO : PROGRESS: at 82.77% examples, 46497 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:49,150 : INFO : PROGRESS: at 84.67% examples, 46532 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:50,249 : INFO : PROGRESS: at 86.39% examples, 46389 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:51,371 : INFO : PROGRESS: at 87.92% examples, 46140 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:52,563 : INFO : PROGRESS: at 89.99% examples, 46107 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:53,663 : INFO : PROGRESS: at 91.89% examples, 46074 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:54,699 : INFO : PROGRESS: at 93.45% examples, 45927 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:55,774 : INFO : PROGRESS: at 95.35% examples, 45918 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:32:56,849 : INFO : PROGRESS: at 97.08% examples, 45826 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:57,858 : INFO : PROGRESS: at 98.82% examples, 45792 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:32:58,394 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-27 15:32:58,575 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-27 15:32:58,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-27 15:32:58,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-27 15:32:58,608 : INFO : training on 5779690 raw words (2569659 effective words) took 56.2s, 45720 effective words/s\n",
      "2017-05-27 15:32:58,609 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-27 15:32:58,651 : INFO : saving Word2Vec object under ../../Results/Yeast/models/yeast_gen_model, separately None\n",
      "2017-05-27 15:32:58,652 : INFO : not storing attribute syn0norm\n",
      "2017-05-27 15:32:58,652 : INFO : not storing attribute cum_table\n",
      "2017-05-27 15:32:58,998 : INFO : saved ../../Results/Yeast/models/yeast_gen_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing datasets sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-27 15:33:01,565 : INFO : collecting all words and their counts\n",
      "2017-05-27 15:33:01,566 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-27 15:33:01,604 : INFO : PROGRESS: at sentence #10000, processed 228892 words, keeping 12369 word types\n",
      "2017-05-27 15:33:01,651 : INFO : PROGRESS: at sentence #20000, processed 457153 words, keeping 17053 word types\n",
      "2017-05-27 15:33:01,702 : INFO : PROGRESS: at sentence #30000, processed 685749 words, keeping 20407 word types\n",
      "2017-05-27 15:33:01,750 : INFO : PROGRESS: at sentence #40000, processed 916505 words, keeping 23089 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-27 15:33:01,799 : INFO : PROGRESS: at sentence #50000, processed 1145300 words, keeping 25362 word types\n",
      "2017-05-27 15:33:01,874 : INFO : PROGRESS: at sentence #60000, processed 1373137 words, keeping 27351 word types\n",
      "2017-05-27 15:33:01,935 : INFO : PROGRESS: at sentence #70000, processed 1602013 words, keeping 29061 word types\n",
      "2017-05-27 15:33:01,986 : INFO : PROGRESS: at sentence #80000, processed 1831766 words, keeping 30664 word types\n",
      "2017-05-27 15:33:02,051 : INFO : PROGRESS: at sentence #90000, processed 2060233 words, keeping 32183 word types\n",
      "2017-05-27 15:33:02,112 : INFO : PROGRESS: at sentence #100000, processed 2289027 words, keeping 33598 word types\n",
      "2017-05-27 15:33:02,165 : INFO : PROGRESS: at sentence #110000, processed 2517818 words, keeping 34978 word types\n",
      "2017-05-27 15:33:02,215 : INFO : PROGRESS: at sentence #120000, processed 2745192 words, keeping 36222 word types\n",
      "2017-05-27 15:33:02,258 : INFO : PROGRESS: at sentence #130000, processed 2975232 words, keeping 37481 word types\n",
      "2017-05-27 15:33:02,300 : INFO : PROGRESS: at sentence #140000, processed 3205687 words, keeping 38538 word types\n",
      "2017-05-27 15:33:02,343 : INFO : PROGRESS: at sentence #150000, processed 3435259 words, keeping 39632 word types\n",
      "2017-05-27 15:33:02,400 : INFO : PROGRESS: at sentence #160000, processed 3664532 words, keeping 40718 word types\n",
      "2017-05-27 15:33:02,461 : INFO : PROGRESS: at sentence #170000, processed 3895697 words, keeping 41742 word types\n",
      "2017-05-27 15:33:02,513 : INFO : PROGRESS: at sentence #180000, processed 4123577 words, keeping 42709 word types\n",
      "2017-05-27 15:33:02,565 : INFO : PROGRESS: at sentence #190000, processed 4352846 words, keeping 43583 word types\n",
      "2017-05-27 15:33:02,631 : INFO : PROGRESS: at sentence #200000, processed 4579911 words, keeping 44533 word types\n",
      "2017-05-27 15:33:02,669 : INFO : collected 45300 word types from a corpus of 4784186 raw words and 208973 sentences\n",
      "2017-05-27 15:33:02,670 : INFO : Loading a fresh vocabulary\n",
      "2017-05-27 15:33:02,716 : INFO : min_count=6 retains 15261 unique words (33% of original 45300, drops 30039)\n",
      "2017-05-27 15:33:02,717 : INFO : min_count=6 leaves 4728807 word corpus (98% of original 4784186, drops 55379)\n",
      "2017-05-27 15:33:02,751 : INFO : deleting the raw counts dictionary of 45300 items\n",
      "2017-05-27 15:33:02,754 : INFO : sample=0.0001 downsamples 525 most-common words\n",
      "2017-05-27 15:33:02,754 : INFO : downsampling leaves estimated 2262232 word corpus (47.8% of prior 4728807)\n",
      "2017-05-27 15:33:02,755 : INFO : estimated required memory for 15261 words and 600 dimensions: 120561900 bytes\n",
      "2017-05-27 15:33:02,775 : INFO : constructing a huffman tree from 15261 words\n",
      "2017-05-27 15:33:03,175 : INFO : built huffman tree with maximum node depth 20\n",
      "2017-05-27 15:33:03,221 : INFO : resetting layer weights\n",
      "2017-05-27 15:33:03,503 : INFO : training model with 4 workers on 15261 vocabulary and 600 features, using sg=1 hs=1 sample=0.0001 negative=5 window=7\n",
      "2017-05-27 15:33:03,504 : INFO : expecting 208973 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-05-27 15:33:04,596 : INFO : PROGRESS: at 0.25% examples, 26171 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:05,631 : INFO : PROGRESS: at 0.63% examples, 33307 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:33:06,747 : INFO : PROGRESS: at 1.00% examples, 34912 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:07,908 : INFO : PROGRESS: at 1.38% examples, 35393 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:08,911 : INFO : PROGRESS: at 1.71% examples, 35787 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:09,974 : INFO : PROGRESS: at 2.13% examples, 37206 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:11,080 : INFO : PROGRESS: at 2.63% examples, 39228 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:33:12,286 : INFO : PROGRESS: at 3.05% examples, 39246 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:33:13,324 : INFO : PROGRESS: at 3.38% examples, 38929 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:14,337 : INFO : PROGRESS: at 3.76% examples, 39245 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:15,505 : INFO : PROGRESS: at 4.22% examples, 39753 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:16,989 : INFO : PROGRESS: at 4.72% examples, 39579 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:18,010 : INFO : PROGRESS: at 5.09% examples, 39730 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:19,017 : INFO : PROGRESS: at 5.51% examples, 40194 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:20,258 : INFO : PROGRESS: at 5.89% examples, 39741 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:21,563 : INFO : PROGRESS: at 6.31% examples, 39484 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:22,633 : INFO : PROGRESS: at 6.76% examples, 40007 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:23,760 : INFO : PROGRESS: at 7.10% examples, 39634 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:24,795 : INFO : PROGRESS: at 7.48% examples, 39712 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:25,856 : INFO : PROGRESS: at 7.89% examples, 39936 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-27 15:33:26,857 : INFO : PROGRESS: at 8.35% examples, 40451 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:27,932 : INFO : PROGRESS: at 8.73% examples, 40414 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:28,975 : INFO : PROGRESS: at 9.19% examples, 40798 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:30,123 : INFO : PROGRESS: at 9.56% examples, 40635 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:31,204 : INFO : PROGRESS: at 9.98% examples, 40759 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:32,301 : INFO : PROGRESS: at 10.31% examples, 40527 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:33,407 : INFO : PROGRESS: at 10.74% examples, 40611 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:34,457 : INFO : PROGRESS: at 11.07% examples, 40448 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:35,533 : INFO : PROGRESS: at 11.49% examples, 40560 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:36,742 : INFO : PROGRESS: at 11.86% examples, 40359 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:37,777 : INFO : PROGRESS: at 12.24% examples, 40382 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:38,908 : INFO : PROGRESS: at 12.65% examples, 40424 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:39,955 : INFO : PROGRESS: at 13.11% examples, 40685 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:40,976 : INFO : PROGRESS: at 13.40% examples, 40466 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:41,991 : INFO : PROGRESS: at 13.82% examples, 40631 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:43,030 : INFO : PROGRESS: at 14.11% examples, 40402 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:44,090 : INFO : PROGRESS: at 14.57% examples, 40624 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:45,154 : INFO : PROGRESS: at 14.94% examples, 40610 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:46,325 : INFO : PROGRESS: at 15.44% examples, 40827 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:47,338 : INFO : PROGRESS: at 15.77% examples, 40745 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:48,339 : INFO : PROGRESS: at 16.18% examples, 40889 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:49,618 : INFO : PROGRESS: at 16.60% examples, 40776 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:50,632 : INFO : PROGRESS: at 17.06% examples, 41003 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:51,637 : INFO : PROGRESS: at 17.32% examples, 40736 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:52,697 : INFO : PROGRESS: at 17.61% examples, 40530 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:53,881 : INFO : PROGRESS: at 18.11% examples, 40706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:54,920 : INFO : PROGRESS: at 18.45% examples, 40613 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:56,000 : INFO : PROGRESS: at 18.91% examples, 40766 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:57,280 : INFO : PROGRESS: at 19.28% examples, 40591 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:33:58,646 : INFO : PROGRESS: at 19.79% examples, 40611 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:33:59,732 : INFO : PROGRESS: at 20.21% examples, 40670 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:34:00,829 : INFO : PROGRESS: at 20.63% examples, 40713 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:01,925 : INFO : PROGRESS: at 21.04% examples, 40760 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:03,058 : INFO : PROGRESS: at 21.46% examples, 40776 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:04,169 : INFO : PROGRESS: at 21.88% examples, 40805 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:05,248 : INFO : PROGRESS: at 22.38% examples, 41003 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:06,249 : INFO : PROGRESS: at 22.88% examples, 41252 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:07,450 : INFO : PROGRESS: at 23.38% examples, 41363 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:08,483 : INFO : PROGRESS: at 23.84% examples, 41511 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:09,585 : INFO : PROGRESS: at 24.22% examples, 41459 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:10,799 : INFO : PROGRESS: at 24.71% examples, 41554 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:12,091 : INFO : PROGRESS: at 25.22% examples, 41594 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:13,101 : INFO : PROGRESS: at 25.55% examples, 41536 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:14,360 : INFO : PROGRESS: at 26.06% examples, 41598 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:15,413 : INFO : PROGRESS: at 26.51% examples, 41710 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:16,437 : INFO : PROGRESS: at 26.89% examples, 41706 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:17,539 : INFO : PROGRESS: at 27.35% examples, 41790 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:18,630 : INFO : PROGRESS: at 27.81% examples, 41876 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:19,704 : INFO : PROGRESS: at 28.26% examples, 41965 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:20,748 : INFO : PROGRESS: at 28.64% examples, 41947 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:21,776 : INFO : PROGRESS: at 28.98% examples, 41879 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:22,797 : INFO : PROGRESS: at 29.39% examples, 41935 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:23,818 : INFO : PROGRESS: at 29.81% examples, 41987 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:24,930 : INFO : PROGRESS: at 30.23% examples, 41995 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:25,945 : INFO : PROGRESS: at 30.73% examples, 42168 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:27,025 : INFO : PROGRESS: at 31.24% examples, 42297 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:28,180 : INFO : PROGRESS: at 31.65% examples, 42275 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:29,228 : INFO : PROGRESS: at 32.03% examples, 42254 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:30,268 : INFO : PROGRESS: at 32.36% examples, 42180 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:31,340 : INFO : PROGRESS: at 32.78% examples, 42203 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:32,360 : INFO : PROGRESS: at 33.19% examples, 42251 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:33,398 : INFO : PROGRESS: at 33.61% examples, 42291 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:34,424 : INFO : PROGRESS: at 34.07% examples, 42386 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:35,506 : INFO : PROGRESS: at 34.57% examples, 42505 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:36,591 : INFO : PROGRESS: at 35.02% examples, 42569 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:37,620 : INFO : PROGRESS: at 35.40% examples, 42555 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:38,668 : INFO : PROGRESS: at 35.85% examples, 42630 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:39,689 : INFO : PROGRESS: at 36.27% examples, 42670 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:40,693 : INFO : PROGRESS: at 36.73% examples, 42763 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:41,720 : INFO : PROGRESS: at 37.10% examples, 42749 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:42,734 : INFO : PROGRESS: at 37.52% examples, 42787 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:43,908 : INFO : PROGRESS: at 37.98% examples, 42807 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:45,030 : INFO : PROGRESS: at 38.44% examples, 42846 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:46,290 : INFO : PROGRESS: at 38.90% examples, 42827 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:47,410 : INFO : PROGRESS: at 39.32% examples, 42823 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:48,465 : INFO : PROGRESS: at 39.79% examples, 42885 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:49,479 : INFO : PROGRESS: at 40.16% examples, 42877 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:50,526 : INFO : PROGRESS: at 40.62% examples, 42941 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:51,544 : INFO : PROGRESS: at 41.08% examples, 43017 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:52,758 : INFO : PROGRESS: at 41.50% examples, 42970 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:53,844 : INFO : PROGRESS: at 42.00% examples, 43058 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:54,937 : INFO : PROGRESS: at 42.51% examples, 43144 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:55,940 : INFO : PROGRESS: at 42.97% examples, 43223 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:34:57,144 : INFO : PROGRESS: at 43.42% examples, 43223 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:58,177 : INFO : PROGRESS: at 43.88% examples, 43288 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:34:59,182 : INFO : PROGRESS: at 44.30% examples, 43320 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:00,323 : INFO : PROGRESS: at 44.75% examples, 43342 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:01,330 : INFO : PROGRESS: at 45.13% examples, 43330 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:02,452 : INFO : PROGRESS: at 45.59% examples, 43359 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:03,641 : INFO : PROGRESS: at 46.10% examples, 43400 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:04,751 : INFO : PROGRESS: at 46.43% examples, 43314 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:05,874 : INFO : PROGRESS: at 46.93% examples, 43377 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:35:07,256 : INFO : PROGRESS: at 47.43% examples, 43352 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:35:08,287 : INFO : PROGRESS: at 47.84% examples, 43370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:09,300 : INFO : PROGRESS: at 48.18% examples, 43321 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:10,485 : INFO : PROGRESS: at 48.68% examples, 43363 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:11,502 : INFO : PROGRESS: at 49.02% examples, 43312 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-27 15:35:12,513 : INFO : PROGRESS: at 49.43% examples, 43339 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:13,559 : INFO : PROGRESS: at 49.89% examples, 43389 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:14,656 : INFO : PROGRESS: at 50.27% examples, 43352 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:15,724 : INFO : PROGRESS: at 50.69% examples, 43360 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:16,752 : INFO : PROGRESS: at 51.15% examples, 43414 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:17,865 : INFO : PROGRESS: at 51.57% examples, 43405 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:18,913 : INFO : PROGRESS: at 52.03% examples, 43450 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:20,093 : INFO : PROGRESS: at 52.44% examples, 43418 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:21,186 : INFO : PROGRESS: at 52.85% examples, 43419 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:22,391 : INFO : PROGRESS: at 53.27% examples, 43384 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:23,463 : INFO : PROGRESS: at 53.61% examples, 43321 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:24,499 : INFO : PROGRESS: at 53.98% examples, 43307 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:25,557 : INFO : PROGRESS: at 54.40% examples, 43316 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:26,692 : INFO : PROGRESS: at 54.89% examples, 43370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:27,922 : INFO : PROGRESS: at 55.40% examples, 43395 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:28,954 : INFO : PROGRESS: at 55.77% examples, 43379 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:35:30,047 : INFO : PROGRESS: at 56.22% examples, 43410 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:31,084 : INFO : PROGRESS: at 56.60% examples, 43391 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:32,095 : INFO : PROGRESS: at 56.93% examples, 43349 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:33,169 : INFO : PROGRESS: at 57.27% examples, 43291 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:34,258 : INFO : PROGRESS: at 57.73% examples, 43324 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:35,263 : INFO : PROGRESS: at 58.06% examples, 43287 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:36,265 : INFO : PROGRESS: at 58.53% examples, 43342 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:37,389 : INFO : PROGRESS: at 58.99% examples, 43364 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:38,426 : INFO : PROGRESS: at 59.44% examples, 43411 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:39,497 : INFO : PROGRESS: at 59.83% examples, 43385 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:40,596 : INFO : PROGRESS: at 60.33% examples, 43442 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:41,785 : INFO : PROGRESS: at 60.75% examples, 43414 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:42,924 : INFO : PROGRESS: at 61.08% examples, 43342 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:43,975 : INFO : PROGRESS: at 61.42% examples, 43294 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:45,133 : INFO : PROGRESS: at 61.83% examples, 43275 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:46,238 : INFO : PROGRESS: at 62.25% examples, 43270 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:47,322 : INFO : PROGRESS: at 62.63% examples, 43242 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:48,323 : INFO : PROGRESS: at 62.96% examples, 43210 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:49,474 : INFO : PROGRESS: at 63.42% examples, 43223 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:50,526 : INFO : PROGRESS: at 63.75% examples, 43179 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:51,536 : INFO : PROGRESS: at 64.08% examples, 43144 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:35:52,755 : INFO : PROGRESS: at 64.46% examples, 43083 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:54,008 : INFO : PROGRESS: at 64.96% examples, 43100 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:35:55,064 : INFO : PROGRESS: at 65.30% examples, 43055 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:56,273 : INFO : PROGRESS: at 65.80% examples, 43081 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:57,348 : INFO : PROGRESS: at 66.26% examples, 43113 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:58,543 : INFO : PROGRESS: at 66.64% examples, 43061 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:35:59,764 : INFO : PROGRESS: at 66.97% examples, 42976 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:00,913 : INFO : PROGRESS: at 67.30% examples, 42912 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:01,916 : INFO : PROGRESS: at 67.68% examples, 42910 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:03,029 : INFO : PROGRESS: at 68.05% examples, 42882 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:04,153 : INFO : PROGRESS: at 68.47% examples, 42876 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:05,257 : INFO : PROGRESS: at 68.85% examples, 42849 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:06,286 : INFO : PROGRESS: at 69.22% examples, 42841 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:07,320 : INFO : PROGRESS: at 69.72% examples, 42908 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:08,357 : INFO : PROGRESS: at 70.10% examples, 42897 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:09,442 : INFO : PROGRESS: at 70.47% examples, 42876 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:10,590 : INFO : PROGRESS: at 70.81% examples, 42815 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:11,680 : INFO : PROGRESS: at 71.15% examples, 42766 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:12,685 : INFO : PROGRESS: at 71.56% examples, 42788 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:13,863 : INFO : PROGRESS: at 71.98% examples, 42772 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:14,869 : INFO : PROGRESS: at 72.44% examples, 42817 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:15,887 : INFO : PROGRESS: at 72.85% examples, 42837 words/s, in_qsize 8, out_qsize 1\n",
      "2017-05-27 15:36:17,036 : INFO : PROGRESS: at 73.27% examples, 42827 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:18,063 : INFO : PROGRESS: at 73.69% examples, 42844 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:19,101 : INFO : PROGRESS: at 74.14% examples, 42883 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:20,110 : INFO : PROGRESS: at 74.60% examples, 42928 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:21,258 : INFO : PROGRESS: at 75.10% examples, 42965 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:22,323 : INFO : PROGRESS: at 75.52% examples, 42974 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:23,436 : INFO : PROGRESS: at 75.93% examples, 42969 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:24,490 : INFO : PROGRESS: at 76.35% examples, 42979 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:25,636 : INFO : PROGRESS: at 76.76% examples, 42967 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:26,865 : INFO : PROGRESS: at 77.23% examples, 42963 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:27,916 : INFO : PROGRESS: at 77.64% examples, 42973 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:29,060 : INFO : PROGRESS: at 78.14% examples, 43011 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:30,344 : INFO : PROGRESS: at 78.65% examples, 43017 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:31,404 : INFO : PROGRESS: at 78.98% examples, 42980 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:32,534 : INFO : PROGRESS: at 79.40% examples, 42974 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:33,576 : INFO : PROGRESS: at 79.87% examples, 43008 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:36:34,608 : INFO : PROGRESS: at 80.33% examples, 43043 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:35,620 : INFO : PROGRESS: at 80.83% examples, 43105 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:36,697 : INFO : PROGRESS: at 81.33% examples, 43153 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:37,783 : INFO : PROGRESS: at 81.83% examples, 43198 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:38,791 : INFO : PROGRESS: at 82.21% examples, 43193 words/s, in_qsize 6, out_qsize 1\n",
      "2017-05-27 15:36:39,973 : INFO : PROGRESS: at 82.63% examples, 43175 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:40,991 : INFO : PROGRESS: at 83.13% examples, 43235 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:42,280 : INFO : PROGRESS: at 83.58% examples, 43219 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:43,342 : INFO : PROGRESS: at 84.00% examples, 43225 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:44,497 : INFO : PROGRESS: at 84.46% examples, 43233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:45,503 : INFO : PROGRESS: at 84.79% examples, 43208 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:46,616 : INFO : PROGRESS: at 85.17% examples, 43182 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:47,654 : INFO : PROGRESS: at 85.46% examples, 43130 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:48,685 : INFO : PROGRESS: at 85.88% examples, 43142 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:49,755 : INFO : PROGRESS: at 86.30% examples, 43146 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:50,988 : INFO : PROGRESS: at 86.68% examples, 43099 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:52,053 : INFO : PROGRESS: at 87.09% examples, 43105 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:53,111 : INFO : PROGRESS: at 87.55% examples, 43133 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:54,118 : INFO : PROGRESS: at 87.96% examples, 43148 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:55,171 : INFO : PROGRESS: at 88.34% examples, 43135 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:56,266 : INFO : PROGRESS: at 88.81% examples, 43155 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:36:57,280 : INFO : PROGRESS: at 89.18% examples, 43150 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:58,289 : INFO : PROGRESS: at 89.60% examples, 43166 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:36:59,518 : INFO : PROGRESS: at 90.05% examples, 43161 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:00,828 : INFO : PROGRESS: at 90.56% examples, 43164 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:01,852 : INFO : PROGRESS: at 90.90% examples, 43136 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:02,866 : INFO : PROGRESS: at 91.27% examples, 43130 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:04,285 : INFO : PROGRESS: at 91.73% examples, 43092 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:05,389 : INFO : PROGRESS: at 92.06% examples, 43050 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:06,406 : INFO : PROGRESS: at 92.39% examples, 43025 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:07,576 : INFO : PROGRESS: at 92.89% examples, 43053 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:08,615 : INFO : PROGRESS: at 93.27% examples, 43043 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:09,712 : INFO : PROGRESS: at 93.64% examples, 43024 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:10,789 : INFO : PROGRESS: at 94.06% examples, 43028 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:11,802 : INFO : PROGRESS: at 94.43% examples, 43024 words/s, in_qsize 7, out_qsize 1\n",
      "2017-05-27 15:37:12,823 : INFO : PROGRESS: at 94.77% examples, 43000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:13,838 : INFO : PROGRESS: at 95.18% examples, 43015 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:14,850 : INFO : PROGRESS: at 95.56% examples, 43012 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:15,969 : INFO : PROGRESS: at 95.97% examples, 43008 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:17,000 : INFO : PROGRESS: at 96.34% examples, 43000 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:18,178 : INFO : PROGRESS: at 96.76% examples, 42986 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:19,259 : INFO : PROGRESS: at 97.26% examples, 43026 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:20,302 : INFO : PROGRESS: at 97.68% examples, 43035 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:21,404 : INFO : PROGRESS: at 98.10% examples, 43035 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:22,655 : INFO : PROGRESS: at 98.48% examples, 42991 words/s, in_qsize 8, out_qsize 0\n",
      "2017-05-27 15:37:23,714 : INFO : PROGRESS: at 98.94% examples, 43016 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:24,779 : INFO : PROGRESS: at 99.31% examples, 43004 words/s, in_qsize 7, out_qsize 0\n",
      "2017-05-27 15:37:25,835 : INFO : PROGRESS: at 99.78% examples, 43028 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-27 15:37:26,183 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-05-27 15:37:26,241 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-27 15:37:26,287 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-27 15:37:26,333 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-27 15:37:26,334 : INFO : training on 23920930 raw words (11312440 effective words) took 262.8s, 43042 effective words/s\n",
      "2017-05-27 15:37:26,335 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-05-27 15:37:26,495 : INFO : saving Word2Vec object under ../../Results/Yeast/models/yeast_be_model, separately None\n",
      "2017-05-27 15:37:26,496 : INFO : not storing attribute syn0norm\n",
      "2017-05-27 15:37:26,496 : INFO : not storing attribute cum_table\n",
      "2017-05-27 15:37:27,280 : INFO : saved ../../Results/Yeast/models/yeast_be_model\n"
     ]
    }
   ],
   "source": [
    "yeast_w2v_model_strict = pred.make_w2v_model(yeast_strict_real, 'Yeast/models/yeast_strict')\n",
    "yeast_w2v_model_gen = pred.make_w2v_model(yeast_gen_real, 'Yeast/models/yeast_gen')\n",
    "yeast_w2v_model_be = pred.make_w2v_model(yeast_be_real, 'Yeast/models/yeast_be')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for seed in random_seeds:\n",
    "    data_name = '../../Results/Yeast/train_test/yeast_tr_te_split_' + str(seed)\n",
    "    train_data = pickle.load(open(data_name + '_train_data.pkl', 'rb'))\n",
    "    train_labels = pickle.load(open(data_name + '_train_labels.pkl', 'rb'))\n",
    "    test_data = pickle.load(open(data_name + '_test_data.pkl', 'rb'))\n",
    "    test_labels = pickle.load(open(data_name + '_test_labels.pkl', 'rb'))\n",
    "\n",
    "    w2v_train_vecs, w2v_test_vecs = pred.word_2_vec_feat_vecs(train_data, test_data, yeast_w2v_model_strict, feature_count=600)\n",
    "\n",
    "    strict_list_SR_dims_param = [w2v_train_vecs, w2v_test_vecs,\n",
    "                                 train_labels, test_labels]\n",
    "\n",
    "    w2v_train_vecs, w2v_test_vecs = pred.word_2_vec_feat_vecs(train_data, test_data, yeast_w2v_model_gen, feature_count=600)\n",
    "\n",
    "    strict_list_GEN_dims_param = [w2v_train_vecs, w2v_test_vecs,\n",
    "                                  train_labels, test_labels]\n",
    "\n",
    "    w2v_train_vecs, w2v_test_vecs = pred.word_2_vec_feat_vecs(train_data, test_data, yeast_w2v_model_be, feature_count=600)\n",
    "\n",
    "    strict_list_BE_dims_param = [w2v_train_vecs, w2v_test_vecs,\n",
    "                                 train_labels, test_labels]\n",
    "\n",
    "    pickle.dump(strict_list_SR_dims_param, open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "    pickle.dump(strict_list_GEN_dims_param, open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'wb'))\n",
    "    pickle.dump(strict_list_BE_dims_param, open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train_vecs, train_labels, w2v_model_type, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param=alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_vecs, \n",
    "                              label=train_labels)\n",
    "        cvresult = xgb.cv(xgb_param, \n",
    "                          xgtrain, \n",
    "                          num_boost_round=alg.get_params()['n_estimators'], \n",
    "                          nfold=cv_folds, \n",
    "                          metrics='auc', \n",
    "                          early_stopping_rounds=50)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "    #fit the algorithm on the data\n",
    "    alg.fit(train_vecs, train_labels, eval_metric='auc')\n",
    "    \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train_vecs)\n",
    "    train_predprob = alg.predict_proba(train_vecs)[:,1]\n",
    "    \n",
    "    #Print Model report:\n",
    "    print(w2v_model_type, '\\nModel Report')\n",
    "    print(w2v_model_type, 'Accuracy: %.4g' % metrics.accuracy_score(train_labels, train_predictions))\n",
    "    print(w2v_model_type, 'AUC Score (Train): %f' % metrics.roc_auc_score(train_labels, train_predprob))\n",
    "    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importance')\n",
    "#     plt.ylabel('Feature Importance Score')\n",
    "\n",
    "    error = 1-metrics.accuracy_score(train_labels, train_predictions)\n",
    "    auc = metrics.roc_auc_score(train_labels, train_predprob)\n",
    "    \n",
    "    return error, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6086\n",
      "STRICT AUC Score (Train): 0.607743\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5843\n",
      "GEN AUC Score (Train): 0.581177\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5965\n",
      "BE AUC Score (Train): 0.598749\n",
      "235\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6004\n",
      "STRICT AUC Score (Train): 0.614761\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6044\n",
      "GEN AUC Score (Train): 0.599407\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6018\n",
      "BE AUC Score (Train): 0.592301\n",
      "905\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6131\n",
      "STRICT AUC Score (Train): 0.627748\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5965\n",
      "GEN AUC Score (Train): 0.603445\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5879\n",
      "BE AUC Score (Train): 0.571791\n",
      "2895\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5841\n",
      "STRICT AUC Score (Train): 0.587349\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5781\n",
      "GEN AUC Score (Train): 0.560142\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5917\n",
      "BE AUC Score (Train): 0.584521\n",
      "3462\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5953\n",
      "STRICT AUC Score (Train): 0.613894\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5783\n",
      "GEN AUC Score (Train): 0.536462\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5866\n",
      "BE AUC Score (Train): 0.578209\n",
      "4225\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5984\n",
      "STRICT AUC Score (Train): 0.611454\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5823\n",
      "GEN AUC Score (Train): 0.547984\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5849\n",
      "BE AUC Score (Train): 0.578985\n",
      "5056\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5963\n",
      "STRICT AUC Score (Train): 0.603952\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5866\n",
      "GEN AUC Score (Train): 0.568760\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6063\n",
      "BE AUC Score (Train): 0.601457\n",
      "5192\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5991\n",
      "STRICT AUC Score (Train): 0.607509\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5831\n",
      "GEN AUC Score (Train): 0.547776\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6036\n",
      "BE AUC Score (Train): 0.600838\n",
      "7751\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6085\n",
      "STRICT AUC Score (Train): 0.628763\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.593\n",
      "GEN AUC Score (Train): 0.587273\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6015\n",
      "BE AUC Score (Train): 0.592454\n",
      "7813\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.593\n",
      "STRICT AUC Score (Train): 0.606322\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5853\n",
      "GEN AUC Score (Train): 0.573034\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.586\n",
      "BE AUC Score (Train): 0.573165\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate = 0.1, \n",
    "                     n_estimators=1000, \n",
    "                     max_depth=5, \n",
    "                     min_child_weight=1, \n",
    "                     gamma=0, \n",
    "                     subsample=0.8, \n",
    "                     colsample_bytree=0.8, \n",
    "                     objective='binary:logistic', \n",
    "                     nthread=4, \n",
    "                     scale_pos_weight=1, \n",
    "                     seed=24)\n",
    "\n",
    "strict_error_list = []\n",
    "strict_auc_list = []\n",
    "gen_error_list = []\n",
    "gen_auc_list = []\n",
    "be_error_list = []\n",
    "be_auc_list = []\n",
    "\n",
    "\n",
    "for seed in random_seeds:  \n",
    "    strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    print(seed)\n",
    "    strict_error, strict_auc = modelfit(xgb1, strict_list[0], strict_list[2], 'STRICT')\n",
    "    gen_error, gen_auc = modelfit(xgb1, gen_list[0], gen_list[2], 'GEN')\n",
    "    be_error, be_auc = modelfit(xgb1, be_list[0], be_list[2], 'BE')\n",
    "    \n",
    "    strict_error_list.append(strict_error)\n",
    "    strict_auc_list.append(strict_auc)\n",
    "    gen_error_list.append(gen_error)\n",
    "    gen_auc_list.append(gen_auc)\n",
    "    be_error_list.append(be_error)\n",
    "    be_auc_list.append(be_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.637\n",
      "STRICT AUC Score (Train): 0.737125\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6552\n",
      "GEN AUC Score (Train): 0.748667\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6712\n",
      "BE AUC Score (Train): 0.785685\n",
      "235\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6624\n",
      "STRICT AUC Score (Train): 0.739747\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6504\n",
      "GEN AUC Score (Train): 0.738340\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6855\n",
      "BE AUC Score (Train): 0.784790\n",
      "905\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6512\n",
      "STRICT AUC Score (Train): 0.746496\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6653\n",
      "GEN AUC Score (Train): 0.765681\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6767\n",
      "BE AUC Score (Train): 0.790130\n",
      "2895\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6491\n",
      "STRICT AUC Score (Train): 0.744200\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6756\n",
      "GEN AUC Score (Train): 0.776996\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.687\n",
      "BE AUC Score (Train): 0.789860\n",
      "3462\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6515\n",
      "STRICT AUC Score (Train): 0.742703\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6644\n",
      "GEN AUC Score (Train): 0.781831\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6758\n",
      "BE AUC Score (Train): 0.787186\n",
      "4225\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6394\n",
      "STRICT AUC Score (Train): 0.736652\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6788\n",
      "GEN AUC Score (Train): 0.775084\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6611\n",
      "BE AUC Score (Train): 0.766342\n",
      "5056\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6279\n",
      "STRICT AUC Score (Train): 0.690399\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6741\n",
      "GEN AUC Score (Train): 0.780861\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6702\n",
      "BE AUC Score (Train): 0.775976\n",
      "5192\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.637\n",
      "STRICT AUC Score (Train): 0.728524\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6564\n",
      "GEN AUC Score (Train): 0.761131\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6617\n",
      "BE AUC Score (Train): 0.768612\n",
      "7751\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6441\n",
      "STRICT AUC Score (Train): 0.738843\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.65\n",
      "GEN AUC Score (Train): 0.757977\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6731\n",
      "BE AUC Score (Train): 0.786600\n",
      "7813\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6284\n",
      "STRICT AUC Score (Train): 0.716465\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6749\n",
      "GEN AUC Score (Train): 0.782777\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6802\n",
      "BE AUC Score (Train): 0.784594\n"
     ]
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(learning_rate = 0.1, \n",
    "                     n_estimators=1000, \n",
    "                     max_depth=5, \n",
    "                     min_child_weight=1, \n",
    "                     gamma=0, \n",
    "                     subsample=0.8, \n",
    "                     colsample_bytree=0.8, \n",
    "                     objective='binary:logistic', \n",
    "                     nthread=4, \n",
    "                     scale_pos_weight=1, \n",
    "                     seed=24)\n",
    "\n",
    "strict_error_list = []\n",
    "strict_auc_list = []\n",
    "gen_error_list = []\n",
    "gen_auc_list = []\n",
    "be_error_list = []\n",
    "be_auc_list = []\n",
    "\n",
    "\n",
    "for seed in random_seeds:  \n",
    "    strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    print(seed)\n",
    "    strict_error, strict_auc = modelfit(xgb1, strict_list[0], strict_list[2], 'STRICT')\n",
    "    gen_error, gen_auc = modelfit(xgb1, gen_list[0], gen_list[2], 'GEN')\n",
    "    be_error, be_auc = modelfit(xgb1, be_list[0], be_list[2], 'BE')\n",
    "    \n",
    "    strict_error_list.append(strict_error)\n",
    "    strict_auc_list.append(strict_auc)\n",
    "    gen_error_list.append(gen_error)\n",
    "    gen_auc_list.append(gen_auc)\n",
    "    be_error_list.append(be_error)\n",
    "    be_auc_list.append(be_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict Results Error:  0.330430479588 , Auc:  0.768074209551\n",
      "Gen Results Error:  0.364579946523 , Auc:  0.707197019221\n",
      "Be Results Error:  0.344158176436 , Auc:  0.74322406605\n"
     ]
    }
   ],
   "source": [
    "print('Strict Results Error: ', np.mean(strict_error_list), ', Auc: ', np.mean(strict_auc_list))\n",
    "print('Gen Results Error: ', np.mean(gen_error_list), ', Auc: ', np.mean(gen_auc_list))\n",
    "print('Be Results Error: ', np.mean(be_error_list), ', Auc: ', np.mean(be_auc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    "    'max_depth':[3,4,5,6,7,8,9,10],\n",
    "    'min_child_weight':[1,2,3,4,5,6]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, \n",
    "                                                  n_estimators=140, \n",
    "                                                  max_depth=5, \n",
    "                                                  min_child_weight=1, \n",
    "                                                  gamma=0, \n",
    "                                                  subsample=0.8, \n",
    "                                                  colsample_bytree=0.8, \n",
    "                                                  objective='binary:logistic', \n",
    "                                                  nthread=4, \n",
    "                                                  scale_pos_weight=1, \n",
    "                                                  seed=24), \n",
    "                        param_grid=param_test1, \n",
    "                        scoring='roc_auc',  \n",
    "                        iid=False, \n",
    "                        cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "strict_param_test1 = []\n",
    "for seed in [144]:\n",
    "    strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gsearch1.fit(strict_list[0], strict_list[2])\n",
    "    strict_param_test1.append((seed, (gsearch1.best_params_, gsearch1.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gen_param_test1 = []\n",
    "for seed in [144]:\n",
    "    gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gsearch1.fit(gen_list[0], gen_list[2])\n",
    "    gen_param_test1.append((seed, (gsearch1.best_params_, gsearch1.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "be_param_test1 = []\n",
    "for seed in [144]:\n",
    "    be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gsearch1.fit(be_list[0], be_list[2])\n",
    "    be_param_test1.append((seed, (gsearch1.best_params_, gsearch1.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(144, ({'max_depth': 3, 'min_child_weight': 1}, 0.5184627231701338))] [(144, ({'max_depth': 3, 'min_child_weight': 5}, 0.5033950980591407))] [(144, ({'max_depth': 6, 'min_child_weight': 6}, 0.4984421837963776))]\n"
     ]
    }
   ],
   "source": [
    "print(strict_param_test1, gen_param_test1, be_param_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.712\n",
      "STRICT AUC Score (Train): 0.869563\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.6677\n",
      "GEN AUC Score (Train): 0.787012\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5891\n",
      "BE AUC Score (Train): 0.640011\n",
      "235\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.581\n",
      "STRICT AUC Score (Train): 0.660918\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.586\n",
      "GEN AUC Score (Train): 0.627057\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5887\n",
      "BE AUC Score (Train): 0.651093\n",
      "905\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5811\n",
      "STRICT AUC Score (Train): 0.656544\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5906\n",
      "GEN AUC Score (Train): 0.629023\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5933\n",
      "BE AUC Score (Train): 0.649648\n",
      "2895\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5872\n",
      "STRICT AUC Score (Train): 0.674281\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5776\n",
      "GEN AUC Score (Train): 0.640244\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5947\n",
      "BE AUC Score (Train): 0.655433\n",
      "3462\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5994\n",
      "STRICT AUC Score (Train): 0.673769\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5836\n",
      "GEN AUC Score (Train): 0.608703\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5922\n",
      "BE AUC Score (Train): 0.645321\n",
      "4225\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5956\n",
      "STRICT AUC Score (Train): 0.672983\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5818\n",
      "GEN AUC Score (Train): 0.634002\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.6013\n",
      "BE AUC Score (Train): 0.660765\n",
      "5056\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5872\n",
      "STRICT AUC Score (Train): 0.668986\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5911\n",
      "GEN AUC Score (Train): 0.614930\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.596\n",
      "BE AUC Score (Train): 0.650820\n",
      "5192\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5955\n",
      "STRICT AUC Score (Train): 0.671295\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5789\n",
      "GEN AUC Score (Train): 0.622833\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5859\n",
      "BE AUC Score (Train): 0.641429\n",
      "7751\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.6027\n",
      "STRICT AUC Score (Train): 0.686008\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5833\n",
      "GEN AUC Score (Train): 0.618640\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5955\n",
      "BE AUC Score (Train): 0.641817\n",
      "7813\n",
      "STRICT \n",
      "Model Report\n",
      "STRICT Accuracy: 0.5836\n",
      "STRICT AUC Score (Train): 0.670016\n",
      "GEN \n",
      "Model Report\n",
      "GEN Accuracy: 0.5932\n",
      "GEN AUC Score (Train): 0.613517\n",
      "BE \n",
      "Model Report\n",
      "BE Accuracy: 0.5963\n",
      "BE AUC Score (Train): 0.662262\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(learning_rate = 0.1, \n",
    "                     n_estimators=1000, \n",
    "                     max_depth=3, \n",
    "                     min_child_weight=3, \n",
    "                     gamma=0, \n",
    "                     subsample=0.8, \n",
    "                     colsample_bytree=0.8, \n",
    "                     objective='binary:logistic', \n",
    "                     nthread=4, \n",
    "                     scale_pos_weight=1, \n",
    "                     seed=24)\n",
    "\n",
    "strict_error_list = []\n",
    "strict_auc_list = []\n",
    "gen_error_list = []\n",
    "gen_auc_list = []\n",
    "be_error_list = []\n",
    "be_auc_list = []\n",
    "\n",
    "\n",
    "for seed in random_seeds:  \n",
    "    strict_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_SR_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    gen_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_GEN_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    be_list = pickle.load(open('../../Results/Yeast/result_list/yeast_strict_list_BE_'+str(seed)+'_results_list.pkl', 'rb'))\n",
    "    print(seed)\n",
    "    strict_error, strict_auc = modelfit(xgb2, strict_list[0], strict_list[2], 'STRICT')\n",
    "    gen_error, gen_auc = modelfit(xgb2, gen_list[0], gen_list[2], 'GEN')\n",
    "    be_error, be_auc = modelfit(xgb2, be_list[0], be_list[2], 'BE')\n",
    "    \n",
    "    strict_error_list.append(strict_error)\n",
    "    strict_auc_list.append(strict_auc)\n",
    "    gen_error_list.append(gen_error)\n",
    "    gen_auc_list.append(gen_auc)\n",
    "    be_error_list.append(be_error)\n",
    "    be_auc_list.append(be_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
